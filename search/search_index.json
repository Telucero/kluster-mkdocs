{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api-reference/reference/","title":"Macro Rendering Error","text":"<p>File: <code>api-reference/reference.md</code></p> <p>UndefinedError: 'batch' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/mkdocs_macros/plugin.py\", line 527, in render\n    return md_template.render(**page_variables)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 599, in top-level template code\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 485, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'batch' is undefined\n</code></pre>"},{"location":"get-started/get-api-key/","title":"Generate your kluster.ai API key","text":"<p>The API key is a unique identifier that authenticates requests associated with your account. You must have at least one API key to access kluster.ai's services.</p> <p>This guide will help you obtain an API key, the first step to leveraging kluster.ai's powerful and cost-effective AI capabilities.</p>"},{"location":"get-started/get-api-key/#create-an-account","title":"Create an account","text":"<p>If you haven't already created an account with kluster.ai, visit the registration page and take the following steps:</p> <ol> <li>Enter your full name</li> <li>Provide a valid email address</li> <li>Create a secure password</li> <li>Click the Sign up button</li> </ol> <p></p>"},{"location":"get-started/get-api-key/#generate-a-new-api-key","title":"Generate a new API key","text":"<p>After you've signed up or logged into the platform through the login page, take the following steps:</p> <ol> <li>Select API Keys on the left-hand side menu</li> <li> <p>In the API Keys section, click the Issue New API Key button</p> <p></p> </li> <li> <p>Enter a descriptive name for your API key in the popup, then click Create Key</p> <p></p> </li> </ol>"},{"location":"get-started/get-api-key/#copy-and-secure-your-api-key","title":"Copy and secure your API key","text":"<ol> <li>Once generated, your API key will be displayed</li> <li> <p>Copy the key and store it in a secure location, such as a password manager</p> <p>Warning</p> <p>For security reasons, you won't be able to view the key again. If lost, you will need to generate a new one.</p> </li> </ol> <p></p> <p>Security tips</p> <ul> <li>Keep it secret - do not share your API key publicly or commit it to version control systems</li> <li>Use environment variables - store your API key in environment variables instead of hardcoding them</li> <li>Regenerate if compromised - if you suspect your API key has been exposed, regenerate it immediately from the API Keys section</li> </ul>"},{"location":"get-started/get-api-key/#managing-your-api-keys","title":"Managing your API keys","text":"<p>The API Key Management section allows you to efficiently manage your kluster.ai API keys. You can create, view, and delete API keys by navigating to the API Keys section. Your API keys will be listed in the API Key Management section.</p> <p>To delete an API key, take the following steps:</p> <ol> <li>Locate the API key you wish to delete in the list</li> <li>Click the trash bin icon (  ) in the Actions column</li> <li>Confirm the deletion when prompted</li> </ol> <p></p> <p>Warning</p> <p>Once deleted, the API key cannot be used again and you must generate a new one if needed.</p>"},{"location":"get-started/get-api-key/#next-steps","title":"Next steps","text":"<p>Now that you have your API key, you can start integrating kluster.ai's LLMs into your applications. Refer to our Getting Started guide for detailed instructions on using the API.</p>"},{"location":"get-started/models/","title":"Models on kluster.ai","text":"<p>kluster.ai offers a wide variety of open-source models for both real-time and batch inferences, with more being constantly added.</p> <p>This page covers all the models the API supports, with the API request limits for each.</p>"},{"location":"get-started/models/#model-names","title":"Model names","text":"<p>Each model supported by kluster.ai has a unique name that must be used when defining the <code>model</code> in the request.</p> Model Model API name DeepSeek R1 <code>deepseek-ai/DeepSeek-R1</code> DeepSeek V3 <code>deepseek-ai/DeepSeek-V3</code> DeepSeek V3 0324 <code>deepseek-ai/DeepSeek-V3-0324</code> Gemma 3 27B <code>google/gemma-3-27b-it</code> Llama 3.1 8B <code>klusterai/Meta-Llama-3.1-8B-Instruct-Turbo</code> Llama 3.1 405B <code>klusterai/Meta-Llama-3.1-405B-Instruct-Turbo</code> Llama 3.3 70B <code>klusterai/Meta-Llama-3.3-70B-Instruct-Turbo</code> Llama 4 Maverick 17B 128E <code>meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8</code> Llama 4 Scout 17B 16E <code>meta-llama/Llama-4-Scout-17B-16E-Instruct</code> Llama 3.3 70B <code>klusterai/Meta-Llama-3.3-70B-Instruct-Turbo</code> Qwen 2.5 7B <code>Qwen/Qwen2.5-VL-7B-Instruct</code>"},{"location":"get-started/models/#model-comparison-table","title":"Model comparison table","text":"Model Mainuse case Real-timeinference support Batchinference support Fine-tuningsupport Imageanalysis Functioncalling DeepSeek R1 Code generationComplex data analysis DeepSeek V3 Natural language generationContextually rich writing DeepSeek V3 0324 Natural language generationContextually rich writing Gemma 3 27B Multilingual applicationsImage analysisComplex reasoning Llama 3.1 8B Low-latency or simple tasksCost-efficient inference Llama 3.1 405B Detailed analysisMaximum accuracy Llama 3.3 70B General-purpose AIBalanced cost-performance Llama 4 Maverick 17B 128E Advanced multimodal reasoningLong-context, high-accuracy tasks Llama 4 Scout 17B 16E Efficient multimodal performanceExtended context, general tasks Qwen 2.5 7B Document analysisImage-based reasoningMultimodal chat"},{"location":"get-started/models/#api-request-limits","title":"API request limits","text":""},{"location":"get-started/openai-compatibility/","title":"Macro Rendering Error","text":"<p>File: <code>get-started/openai-compatibility.md</code></p> <p>UndefinedError: 'libraries' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/mkdocs_macros/plugin.py\", line 527, in render\n    return md_template.render(**page_variables)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 14, in top-level template code\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 485, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'libraries' is undefined\n</code></pre>"},{"location":"get-started/integrations/crewai/","title":"Integrate CrewAI with kluster.ai","text":"<p>CrewAI is a multi-agent platform that organizes specialized AI agents\u2014each with defined roles, tools, and goals\u2014within a structured process to tackle complex tasks efficiently. CrewAI agents streamline workflows and deliver reliable, scalable solutions by coordinating tasks and ensuring smooth collaboration.</p> <p>This guide walks you through integrating kluster.ai with CrewAI to create and run a simple AI agent chatbot that leverages the kluster.ai API.</p>"},{"location":"get-started/integrations/crewai/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following prerequisites:</p> <ul> <li>CrewAI installed - the Installation Guide on the CrewAI website will walk you through installing CrewAI, setting up a virtual Python environment, and creating a new project. Note that CrewAI requires a Python version &gt;=<code>3.10</code> and &lt;<code>3.13</code></li> </ul>"},{"location":"get-started/integrations/crewai/#create-a-project-with-the-cli","title":"Create a project with the CLI","text":"<p>Open your Python virtual environment, and then follow these steps to use the CrewAI CLI to create a new project:</p> <ol> <li>Create a project - following the installation guide, create your first project with the following command: <pre><code>crewai create crew INSERT_PROJECT_NAME\n</code></pre></li> <li>Select model and provider - during setup, the CLI will ask you to choose a provider and a model. Select <code>openai</code> as the provider and then choose any available model. Because you'll configure kluster.ai as a custom model, your initial model choice won't affect the final integration. The CLI will prompt you for an OpenAI API key, but this isn\u2019t required. Simply press enter to skip</li> </ol>"},{"location":"get-started/integrations/crewai/#build-a-simple-ai-agent","title":"Build a simple AI agent","text":"<p>After finishing the CLI setup, you will see a <code>src</code> directory with files <code>crew.py</code> and <code>main.py</code>. This guide won't use these sample files because they include extra features outside the scope. Follow these steps to continue:</p> <ol> <li> <p>Create your first file - create a <code>hello_crew.py</code> file in <code>src/YOUR_PROJECT_NAME</code> to correspond to a simple AI agent chatbot</p> </li> <li> <p>Import modules and select model - open <code>hello_crew.py</code> to add imports and define a custom LLM for kluster.ai by setting the following parameters:</p> <ul> <li>provider - you can specify <code>openai_compatible</code></li> <li> <p>model - choose one of kluster.ai's available models based on your use case. Regardless of which model you choose, prepend its name with <code>openai/</code> to ensure CrewAI, which relies on LiteLLM, processes your requests correctly</p> </li> <li> <p>base_url - use <code>https://api.kluster.ai/v1</code> to send requests to the kluster.ai endpoint</p> </li> <li>api_key - replace <code>INSERT_API_KEY</code> in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide</li> </ul> hello_crew.py<pre><code>\n</code></pre> <p>This example overrides <code>agents_config</code> and <code>tasks_config</code> with empty dictionaries to tell CrewAI to ignore all YAML files and rely solely on your code, keeping this guide as streamlined as possible. </p> </li> <li> <p>Define your agent - set the agent's role, goal, and backstory, and assign the custom LLM (via the kluster.ai API) for generating creative greetings:</p> hello_crew.py<pre><code>\n</code></pre> </li> <li> <p>Give the agent a task - define a task that prompts the agent for a unique, creative greeting using randomness to avoid repetition. Passing this prompt to <code>hello_agent()</code> ensures varied responses. CrewAI requires an <code>expected_output</code> field, defined here as a short greeting:</p> hello_crew.py<pre><code>\n</code></pre> </li> <li> <p>Tie it all together with a <code>@crew</code> method - add the following method to return the assembled Crew object with a single agent and task. This method enables CrewAI to coordinate the agent and task you defined:</p> hello_crew.py<pre><code>\n</code></pre> </li> <li> <p>Set up the entry point for the agent - create a new file named <code>hello_main.py</code>. In <code>hello_main.py</code>, import and initialize the <code>HelloWorldCrew</code> class, call its <code>hello_crew()</code> method, and then <code>kickoff()</code> to launch the task sequence:</p> hello_main.py<pre><code>#!/usr/bin/env python\nfrom hello_crew import HelloWorldCrew\n\n\ndef run():\n    \"\"\"\n    Kick off the HelloWorld crew with no inputs.\n    \"\"\"\n    HelloWorldCrew().hello_crew().kickoff(inputs={})\n\nif __name__ == \"__main__\":\n    run()\n</code></pre> </li> </ol> View complete script hello_crew.py<pre><code>\n</code></pre>"},{"location":"get-started/integrations/crewai/#put-it-all-together","title":"Put it all together","text":"<p>To run your agent, ensure you are in the same directory as your <code>hello_main.py</code> file, then use the following command:</p> <pre><code>python hello_main.py\n</code></pre> <p>Upon running the script, you'll see output that looks like the following:</p> <p>And that's it! You've now successfully configured your AI agent harnessing CrewAI and the power of the kluster.ai API! </p>"},{"location":"get-started/integrations/eliza/","title":"Integrate eliza with kluster.ai","text":"<p>eliza is an open-source framework designed to create and manage AI agents that can handle a variety of tasks, from simple chat interactions to more complex automation.</p> <p>In this guide, you'll learn how to integrate kluster.ai into eliza to leverage its powerful models and quickly set up your AI-driven workflows.</p>"},{"location":"get-started/integrations/eliza/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following kluster prerequisites:</p> <ul> <li>Clone and install the eliza repository - follow the installation instructions on the eliza Quick Start guide</li> </ul> <p>Warning</p> <p>Pay careful attention to the eliza prerequisites, including the minimum supported versions of Node.js and pnpm. You will not be able to successfully follow this guide using npm or yarn.</p> <ul> <li>Stop at the Configure Environment section in the Quick Start guide, as this guide covers those steps</li> </ul>"},{"location":"get-started/integrations/eliza/#configure-your-environment","title":"Configure your environment","text":"<p>After installing eliza, it's simple to utilize kluster.ai with eliza. Only three main changes to the <code>.env</code> file are required. </p> <ol> <li> <p>Create <code>.env</code> file - run the following command to generate a <code>.env</code> file from the eliza repository example: <pre><code>cp .env.example .env\n</code></pre></p> </li> <li> <p>Set variables - update the following variables in the <code>.env</code> file:</p> <ul> <li>OPENAI_API_KEY - replace <code>INSERT_API_KEY</code> in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide</li> <li>OPENAI_API_URL - use <code>https://api.kluster.ai/v1</code> to send requests to the kluster.ai endpoint</li> <li>OPENAI_DEFAULT_MODEL - choose one of kluster.ai's available models based on your use case. You should also set <code>SMALL_OPENAI_MODEL</code>, <code>MEDIUM_OPENAI_MODEL</code>, and <code>LARGE_OPENAI_MODEL</code> to the same value to allow seamless experimentation as different characters use different default models</li> </ul> </li> </ol> <p>The OpenAI configuration section of your <code>.env</code> file should resemble the following:</p> .env<pre><code># OpenAI Configuration\nOPENAI_API_KEY=INSERT_KLUSTER_API_KEY\nOPENAI_API_URL=https://api.kluster.ai/v1\n\n# Community Plugin for OpenAI Configuration\nOPENAI_DEFAULT_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\nSMALL_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\nMEDIUM_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\nLARGE_OPENAI_MODEL=klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\n</code></pre>"},{"location":"get-started/integrations/eliza/#run-and-interact-with-your-first-agent","title":"Run and interact with your first agent","text":"<p>Now that you've configured your environment, you're ready to run your first agent! eliza has several characters you can interact with by prompting or through autonomous tasks like tweeting. This guide relies on the <code>Dobby</code> character for its minimal setup requirements.</p> <ol> <li> <p>Verify character configuration - open the <code>dobby.character.json</code> file inside the <code>characters</code> folder. By default, <code>Dobby</code> uses the <code>openai</code> model, which you've already configured to use the kluster.ai API. The <code>Dobby</code> configuration should start with the following: dobby.character.json<pre><code>{\n  \"name\": \"Dobby\",\n  \"clients\": [],\n  \"modelProvider\": \"openai\" // json truncated for clarity\n}\n</code></pre></p> </li> <li> <p>Run the agent - run the following command from the project root directory to run the <code>Dobby</code> agent: <pre><code>pnpm start --character=\"characters/dobby.character.json\"\n</code></pre></p> </li> <li> <p>Launch the UI - in another terminal window, run the following command to launch the web UI:  <pre><code>pnpm start:client\n</code></pre>   Your terminal output should resemble the following:</p> </li> <li> <p>Open your browser - follow the prompts and open your browser to http://localhost:5173/</p> </li> </ol>"},{"location":"get-started/integrations/eliza/#put-it-all-together","title":"Put it all together","text":"<p>You can now interact with Dobby by selecting on the Send Message button and starting the conversation: </p> <p></p> <p>That's it! You've successfully integrated eliza with the kluster.ai API. You're now ready to harness the power of AI agents with the kluster.ai API!</p>"},{"location":"get-started/integrations/immersive-translate/","title":"Integrate Immersive Translate with kluster.ai","text":"<p>Immersive Translate is an  AI-powered bilingual translation extension that automatically identifies the main text on any web page and provides parallel translations in real-time. This context-driven approach streamlines reading and collaboration across languages with additional features like efficient document translation, hover translation, and support for 10+ translation services.</p> <p>In this guide, you'll learn how to integrate Immersive Translate with the kluster.ai API\u2014from installation through configuration\u2014so you can seamlessly handle multilingual content within your workflows. You will enable Immersive Translate's core capabilities with kluster.ai's powerful models, helping you build more robust and accessible AI-driven applications.</p>"},{"location":"get-started/integrations/immersive-translate/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following:</p> <ul> <li>Installed the Immersive Translate plugin - you can download the Immersive Translate plugin for your respective browser on the Immersive Translate homepage</li> </ul>"},{"location":"get-started/integrations/immersive-translate/#configure-immersive-translate","title":"Configure Immersive Translate","text":"<p>First, open the Immersive Translate extension and click on the Options button in the lower left corner of the extension.</p> <p></p> <p>Then, take the following steps:</p> <ol> <li>Navigate to Translatation Services</li> <li>Press Add OpenAI Compatible Service</li> </ol> <p></p> <p>Take the following steps to configure the kluster.ai API as a custom translation service for Immersive Translate:</p> <ol> <li>Enter a name</li> <li> <p>For the custom API interface address, enter the following:</p> <pre><code>https://api.kluster.ai/v1/chat/completions\n</code></pre> </li> <li> <p>Paste in your kluster.ai API key</p> </li> <li>Check the box to enable custom models </li> <li>Paste in the name of the kluster.ai supported model you'd like to use</li> <li>Specify a value of <code>1</code> for max requests per second to avoid rate limits. Paid kluster.ai API accounts may have higher rate limits</li> <li>Press Verify Service in the upper right corner to validate the input values</li> </ol> <p></p> <p>You must take one more step before using kluster.ai with Immersive Translate. Although kluster.ai has been added as a provider, it is disabled by default. To enable it, take the following steps:</p> <ol> <li>Click on the Translation Services section of settings</li> <li>Toggle the switch to enable kluster.ai as a provider</li> </ol> <p>That's it! The next section will demonstrate using Immersive Translate with the kluster.ai API to perform webpage translations.</p> <p></p>"},{"location":"get-started/integrations/immersive-translate/#translate-content","title":"Translate content","text":"<p>With Immersive Translate, you can easily translate content with just a few clicks. To do so, navigate to the page with the foreign language content. Open the Immersive Translate plugin and take the following steps:</p> <ol> <li>The language of the existing content is auto-detected by the plugin, but it's a good idea to verify it</li> <li>Select the language to translate the content into. This is set by default to your native language </li> <li>Press Translate</li> </ol> <p></p> <p>Then, the content translated by the Immersive Translate plugin will begin to appear on the page. </p> <p></p> <p>And that's it! You've now set up Immersive Translate to use the kluster.ai API and learned how to translate content.</p>"},{"location":"get-started/integrations/langchain/","title":"Integrate LangChain with kluster.ai","text":"<p>LangChain offers a range of features\u2014like memory modules for context tracking, retrieval augmentation to feed external data into prompts, and customizable multi-step \u201cchains\" to break down complex tasks. By leveraging these capabilities with the kluster.ai API, you can build more robust and context-aware solutions that seamlessly handle everything from short-form answers to intricate conversations.</p> <p>This guide demonstrates how to integrate the <code>ChatOpenAI</code> class from the <code>langchain_openai</code> package with the kluster.ai API, then walks through building a multi-turn conversational agent that leverages LangChain's memory for context-aware interactions.</p>"},{"location":"get-started/integrations/langchain/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following:</p> <ul> <li>A python virtual environment - this is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial</li> <li> <p>LangChain packages installed - install the <code>langchain</code> packages:</p> <pre><code>pip install langchain langchain_community langchain_core langchain_openai\n</code></pre> <p>As a shortcut, you can also run:</p> <pre><code>pip install \"langchain[all]\"\n</code></pre> </li> </ul>"},{"location":"get-started/integrations/langchain/#quick-start","title":"Quick Start","text":"<p>It's easy to integrate kluster.ai with LangChain\u2014when configuring the chat model, point your <code>ChatOpenAI</code> instance to the correct base URL and configure the following settings:</p> <ul> <li>Base URL - use <code>https://api.kluster.ai/v1</code> to send requests to the kluster.ai endpoint</li> <li>API key - replace <code>INSERT_API_KEY</code> in the code below with your kluster.ai API key. If you don't have one yet, refer to the Get an API key guide</li> <li>Select your model - choose one of kluster.ai's available models based on your use case</li> </ul> <pre><code>\n</code></pre> <p>That's all you need to start with LangChain and the kluster.ai API! Next, this guide will explore building a multi-turn conversational agent that showcases how memory and context can elevate your chatbot to a more interactive, intelligent experience.</p>"},{"location":"get-started/integrations/langchain/#build-a-multi-turn-conversational-agent","title":"Build a multi-turn conversational agent","text":"<p>This section will explore what LangChain can do beyond a single prompt-and-response interaction. One standout feature of LangChain is its built-in memory, which tracks conversation context across multiple user queries. In the following steps, you'll set up a multi-turn conversational agent that takes advantage of this memory and seamlessly integrates with the kluster.ai API.</p> <ol> <li> <p>Create file - create a new file called <code>langchain-advanced.py</code> using the following command in your terminal: <pre><code>touch langchain-advanced.py\n</code></pre></p> </li> <li> <p>Import LangChain components - inside your new file, import the following components for memory management, prompt handling, and kluster.ai integration: <pre><code>\n</code></pre></p> </li> <li>Create a memory instance - to store and manage the conversation's context, allowing the chatbot to remember previous user messages. <pre><code>\n</code></pre></li> <li>Configure the <code>ChatOpenAI</code> model - point to kluster.ai's endpoint with your API key and chosen model. Remember, you can always change the selected model based on your needs <pre><code>\n</code></pre></li> <li>Define a prompt template - include a system instruction for the assistant, a placeholder for the conversation history, and an input slot for the user's query  <pre><code>\n</code></pre></li> <li>Create the <code>ConversationChain</code> - pass in the LLM, memory, and this prompt template so every new user query is automatically enriched with the stored conversation context and guided by the assistant's role <pre><code>\n</code></pre></li> <li>Prompt the model with the first question - you can prompt the model with any question. The example chosen here is designed to demonstrate context awareness between questions <pre><code>\n</code></pre></li> <li>Pose a follow-up question - ask another question without resupplying the city name and notice how LangChain's memory implicitly handles the context. Return and print the questions and responses to see how the conversation informs each new query to create multi-turn interactions <pre><code>\n</code></pre></li> </ol> View complete script langchain-advanced.py<pre><code>\n</code></pre>"},{"location":"get-started/integrations/langchain/#put-it-all-together","title":"Put it all together","text":"<ol> <li> <p>Use the following command to run your script: <pre><code>python langchain-advanced.py\n</code></pre></p> </li> <li> <p>You should see output that resembles the following:</p> </li> </ol> <p>That's it! You've successfully integrated LangChain with the kluster.ai API, and your configured multi-turn conversational agent is ready to leverage the power of LangChain and the kluster.ai API. For more information about the capabilities of LangChain, be sure to check out the LangChain docs.</p>"},{"location":"get-started/integrations/litellm/","title":"Integrate LiteLLM with kluster.ai","text":"<p>LiteLLM is an open-source Python library that streamlines access to a broad range of Large Language Model (LLM) providers through a standardized interface inspired by the OpenAI format. By providing features like fallback mechanisms, cost tracking, and streaming support, LiteLLM reduces the complexity of working with different models, ensuring a more reliable and cost-effective approach to AI-driven applications.</p> <p>Integrating LiteLLM with the kluster.ai API enables the use of kluster.ai's powerful models alongside LiteLLM's flexible orchestration. This combination makes it simple to switch between models on the fly, handle token usage limits with context window fallback, and monitor usage costs in real-time\u2014leading to robust, scalable, and adaptable AI workflows.</p>"},{"location":"get-started/integrations/litellm/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following:</p> <ul> <li>A python virtual environment - this is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial</li> <li> <p>LiteLLM installed - to install the library, use the following command:</p> <pre><code>pip install litellm\n</code></pre> </li> </ul>"},{"location":"get-started/integrations/litellm/#configure-litellm","title":"Configure LiteLLM","text":"<p>In this section, you'll learn how to integrate kluster.ai with LiteLLM. You'll configure your environment variables, specify a kluster.ai model, and make a simple request using LiteLLM's OpenAI-like interface.</p> <ol> <li>Import LiteLLM and its dependencies - create a new file (e.g., <code>hello-litellm.py</code>) and start by importing the necessary Python modules: <pre><code>\n</code></pre></li> <li>Set your kluster.ai API key and Base URL - replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the Get an API key guide <pre><code>\n</code></pre></li> <li>Define your conversation (system + user messages) - set up your initial system prompt and user message. The system message defines your AI assistant's role, while the user message is the actual question or prompt <pre><code>\n</code></pre></li> <li>Select your kluster.ai model - choose one of kluster.ai's available models that best fits your use case. Prepend the model name with <code>openai/</code> so LiteLLM recognizes it as an OpenAI-like model request <pre><code>\n</code></pre></li> <li>Call the LiteLLM completion function - finally, invoke the completion function to send your request: <pre><code>\n</code></pre></li> </ol> View complete script hello-litellm.py<pre><code>\n</code></pre> <p>Use the following command to run your script:</p> <pre><code>python hello-litellm.py\n</code></pre> <p>That's it! You've successfully integrated LiteLLM with the kluster.ai API. Continue to learn how to experiment with more advanced features of LiteLLM.</p>"},{"location":"get-started/integrations/litellm/#explore-litellm-features","title":"Explore LiteLLM features","text":"<p>In the previous section, you learned how to use LiteLLM with the kluster.ai API by properly configuring the model via an OpenAI-like call and configuring the API key and API base URL. The following sections demonstrate using LiteLLM's streaming response and multi-turn conversation features with the kluster.ai API.</p> <p>The following guide assumes you just finished the configuration exercise in the preceding section. If you haven't already done so, please complete the configuration steps in the Configure LiteLLM section before you continue.</p>"},{"location":"get-started/integrations/litellm/#use-streaming-responses","title":"Use streaming responses","text":"<p>You can enable streaming by simply passing <code>stream=True</code> to the <code>completion()</code> function. Streaming returns a generator instead of a static response, letting you iterate over partial output chunks as they arrive. In the code sample below, each chunk is accessed in a for-in loop, allowing you to extract the textual content (e.g., <code>chunk.choices[0].delta.content)</code> rather than printing all metadata.</p> <p>To configure a streaming response, take the following steps:</p> <ol> <li> <p>Update the <code>messages</code> system prompt and first user message - you can supply a user message or use the sample provided: <pre><code>\n</code></pre></p> </li> <li> <p>Initiate a streaming request to the model - set <code>stream=True</code> in the <code>completion()</code> function to tell LiteLLM to return partial pieces (chunks) of the response as they become available rather than waiting for the entire response to be ready <pre><code>\n</code></pre></p> </li> <li>Isolate the returned text content - returning all of the streamed data will include a lot of excessive noise like token counts, etc. You can isolate the text content from the rest of the streamed response with the following code: <pre><code>\n</code></pre></li> </ol>"},{"location":"get-started/integrations/litellm/#handle-multi-turn-conversation","title":"Handle multi-turn conversation","text":"<p>LiteLLM can facilitate multi-turn conversations by maintaining message history in a sequential chain, enabling the model to consider the context of previous messages. This section demonstrates multi-turn conversation handling by updating the messages list each time we receive a new response from the assistant. This pattern can be repeated for as many turns as you need, continuously appending messages to maintain the conversational flow.</p> <p>Let's take a closer look at each step:</p> <ol> <li>Combine the streamed chunks of the first message - since the message is streamed in chunks, you must re-assemble them into a single message. After collecting partial responses in <code>streamed_text</code>, join them into a single string called <code>complete_first_answer</code>: <pre><code>\n</code></pre></li> <li>Append the assistant's reply - to enhance the context of the conversation. Add <code>complete_first_answer</code> back into messages under the \"assistant\" role as follows: <pre><code>\n</code></pre></li> <li>Craft the second message to the assistant - append a new message object to messages with the user's next question as follows: <pre><code>\n</code></pre></li> <li>Ask the model to respond to the second question - this time, don't enable the streaming feature. Pass the updated messages to <code>completion()</code> with <code>stream=False</code>, prompting LiteLLM to generate a standard (single-shot) response as follows: <pre><code>\n</code></pre></li> <li>Parse and print the second answer - extract <code>response_2.choices[0].message[\"content\"]</code>, store it in <code>second_answer_text</code>, and print to the console for your final output:  <pre><code>\n</code></pre></li> </ol> <p>You can view the full script below. It demonstrates a streamed response versus a regular response and how to handle a multi-turn conversation.  </p> View complete script hello-litellm.py<pre><code>\n</code></pre>"},{"location":"get-started/integrations/litellm/#put-it-all-together","title":"Put it all together","text":"<p>Use the following command to run your script: <pre><code>python hello-litellm.py\n</code></pre></p> <p>You should see output that resembles the following:</p> <p>Both responses appear to trail off abruptly, but that's because we limited the output to <code>300</code> tokens each. Feel free to tweak the parameters and rerun the script at your leisure!</p>"},{"location":"get-started/integrations/msty/","title":"Integrate Msty with kluster.ai","text":"<p>Msty is a user-friendly local AI toolkit that also supports popular online model providers\u2014 all within a sleek, powerful interface. By eliminating tedious setup steps (no Docker or terminal required) and helping you manage attachments, Msty makes large language models more accessible than ever while making every conversation fully informed and flexible.</p> <p>This guide will walk you through integrating kluster.ai with Msty, from installation to hands-on interactions that tap into the kluster.ai API\u2014all in a single, streamlined environment.</p>"},{"location":"get-started/integrations/msty/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following prerequisites:</p> <ul> <li>Msty app installed - The Msty app can be downloaded with one click. You can also find an Installation Guide on the Msty docs site</li> </ul>"},{"location":"get-started/integrations/msty/#quick-start","title":"Quick start","text":"<p>Upon launching the Msty app for the first time, you'll be prompted to configure either a local AI or a remote AI provider. Select Add Remote Model Provider:</p> <p></p> <p>Then, take the following steps to configure Msty to use the kluster.ai API:</p> <ol> <li>For the Provider dropdown, select Open AI Compatible</li> <li>Provide a name, such as <code>kluster</code></li> <li> <p>Provide the kluster.ai API URL for the API endpoint field:</p> <pre><code>https://api.kluster.ai/v1\n</code></pre> </li> <li> <p>Paste your API key and ensure Save key securely in keychain is selected</p> </li> <li>Paste the name of the kluster.ai model you'd like to use. Note that you can specify multiple models</li> <li>Press Add to finalize the addition of kluster.ai API as a provider</li> </ol> <p></p> <p>Great job! You\u2019re now ready to use Msty to query LLMs through the kluster.ai API. For more information on Msty's features, be sure to check out the Msty docs.</p> <p></p>"},{"location":"get-started/integrations/pydantic/","title":"Integrate PydanticAI with kluster.ai","text":"<p>PydanticAI is a typed Python agent framework designed to make building production-grade applications with Generative AI less painful. Pydantic AI leverages Pydantic's robust data validation to ensure your AI interactions are consistent, reliable, and easy to debug. By defining tools (Python functions) with strict type hints and schema validation, you can guide your AI model to call them correctly\u2014reducing confusion or malformed requests.</p> <p>This guide will walk through how to integrate the kluster.ai API with PydanticAI. First, you\u2019ll see how to set up the environment and configure a custom model endpoint for kluster.ai. In the subsequent section, you'll create a tool-based chatbot that can fetch geographic coordinates and retrieve current weather while enforcing schemas and type safety.</p> <p>This approach empowers you to harness the flexibility of large language models without sacrificing strictness: invalid data is caught early, typos in function calls trigger retries or corrections, and every tool action is typed and validated. By the end of this tutorial, you\u2019ll have a working, self-contained weather agent that demonstrates how to keep your AI workflows clean, efficient, and robust when integrating with kluster.ai.</p>"},{"location":"get-started/integrations/pydantic/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following:</p> <ul> <li>A python virtual environment - This is optional but recommended. Ensure that you enter the Python virtual environment before following along with this tutorial</li> <li> <p>PydanticAI installed - to install the library, use the following command:</p> <pre><code>pip install pydantic-ai \n</code></pre> </li> <li> <p>Supporting libraries installed - a few additional supporting libraries are needed for the weather agent tutorial. To install them, use the following command:     <pre><code>pip install httpx devtools logfire\n</code></pre></p> </li> <li> <p>A Tomorrow.io Weather API key - this free API key will allow your weather agent to source accurate real-time weather data</p> </li> <li> <p>A maps.co geocoding API key - this free API key will allow your weather agent to convert a human-readable address into a pair of latitude and longitude coordinates</p> </li> </ul>"},{"location":"get-started/integrations/pydantic/#quick-start","title":"Quick start","text":"<p>In this section, you'll learn how to integrate kluster.ai with PydanticAI. You\u2019ll configure your API key, set your base URL, specify a kluster.ai model, and make a simple request to verify functionality.</p> <ol> <li> <p>Import required libraries - create a new file (e.g., <code>quick-start.py</code>) and import the necessary Python modules:</p> quick-start.py<pre><code>\n</code></pre> </li> <li> <p>Define a custom model to use the kluster.ai API - replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the Get an API key. For the model name, choose one of the kluster.ai models that best fits your use case</p> quick-start.py<pre><code>\n</code></pre> </li> <li> <p>Create a PydanticAI agent - instantiate a PydanticAI agent using the custom model configuration. Then, send a simple prompt to confirm the agent can successfully communicate with the kluster.ai endpoint and print the model's response </p> quick-start.py<pre><code>\n</code></pre> </li> </ol> View complete script quick-start.py<pre><code>\n</code></pre> <p>Use the following command to run your script:</p> <pre><code>python quick-start.py\n</code></pre> <p>That's it! You've successfully integrated PydanticAI with the kluster.ai API. Continue on to learn how to experiment with more advanced features of PydanticAI.</p>"},{"location":"get-started/integrations/pydantic/#build-a-weather-agent-with-pydanticai","title":"Build a weather agent with PydanticAI","text":"<p>In this section, you'll build a weather agent that interprets natural language queries like \"What\u2019s the weather in San Francisco?\" and uses PydanticAI to call both a geo API for latitude/longitude and a weather API for real-time conditions. By defining two tools\u2014one for location lookup and another for weather retrieval\u2014your agent can chain these steps automatically and return a concise, validated response. This approach keeps your AI workflow clean, type-safe, and easy to debug.</p> <ol> <li> <p>Set up dependencies - create a new file (e.g., <code>weather-agent.py</code>), import required packages, and define a <code>Deps</code> data class to store API keys for geocoding and weather. You'll use these dependencies to request latitude/longitude data and real-time weather information</p> <pre><code>\n</code></pre> </li> <li> <p>Define a custom model to use the kluster.ai API - replace INSERT_API_KEY with your actual API key. If you don't have one yet, refer to the Get an API key. For the model name, choose one of the kluster.ai models that best fits your use case</p> <pre><code>\n</code></pre> </li> <li> <p>Define the system prompt - instruct the weather agent on how and when to call the geocoding and weather tools. The agent follows these rules to get valid lat/lng data, fetch the weather, and return a concise response</p> <pre><code>\n</code></pre> </li> <li> <p>Define the geocoding tool - create a tool the agent calls behind the scenes to transform city names to lat/lng using the geocoding API. If the API key is missing or the location is invalid, it defaults to London or raises an error for self-correction</p> <pre><code>\n</code></pre> </li> <li> <p>Define the weather fetching tool - create a tool that fetches weather from Tomorrow.io for a given lat/lng, converting temperatures to Celsius and Fahrenheit. Defaults to a mock response if the API key is missing</p> <pre><code>\n</code></pre> </li> <li> <p>Create a CLI chat - prompt users for a location, send it to the weather agent, and print the final response</p> <pre><code>\n</code></pre> </li> </ol> View complete script weather-agent.py<pre><code>\n</code></pre>"},{"location":"get-started/integrations/pydantic/#put-it-all-together","title":"Put it all together","text":"<p>Use the following command to run your script:</p> <pre><code>python weather-agent.py\n</code></pre> <p>You should see terminal output similar to the following:</p> <p>That's it! You've built a fully functional weather agent using PydanticAI and kluster.ai, showcasing how to integrate type-safe tools and LLMs for real-world data retrieval. Visit the PydanticAI docs site to continue exploring PydanticAI's flexible tool and system prompt features to expand your agent's capabilities and handle more complex use cases with ease.</p>"},{"location":"get-started/integrations/sillytavern/","title":"Integrate SillyTavern with kluster.ai","text":"<p>SillyTavern is a locally installed customizable LLM user interface that focuses on persona-driven LLM interactions\u2014letting you create unique characters or group chats for tasks like code reviews and text editing. It provides custom prompt fields, bookmarks for revisiting specific points in a conversation, and a mobile-friendly design to manage your chat sessions easily.</p> <p>By integrating SillyTavern with the kluster.ai API, you can tap into kluster.ai's high-performance language models as your primary or backup backend. This combination merges SillyTavern's customizable UI and advanced prompt options with kluster.ai's reliable inference, offering a scalable and tailored chat environment for casual users and AI enthusiasts.</p>"},{"location":"get-started/integrations/sillytavern/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following:</p>"},{"location":"get-started/integrations/sillytavern/#configure-sillytavern","title":"Configure SillyTavern","text":"<ol> <li>Launch SillyTavern and open it in your browser at <code>http://127.0.0.1:8000/</code> (default port)</li> <li>Click on the API Connections icon (plug) in the top navigation menu</li> <li>In the API drop-down menu, select Chat Completion</li> <li>In the Chat Completion Source option, choose Custom (OpenAI-compatible)</li> <li> <p>Enter the kluster.ai API endpoint in the Custom Endpoint (Base URL) field:</p> <pre><code>https://api.kluster.ai/v1\n</code></pre> <p>There should be no trailing slash (<code>/</code>) at the end of the URL</p> </li> <li> <p>Paste your kluster.ai API Key into the designated field</p> </li> <li> <p>Enter a Model ID. For this example, you can enter:</p> <pre><code>klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\n</code></pre> </li> <li> <p>Click the Connect button. If you've configured the API correctly, you should see a \ud83d\udfe2 Valid message next to the button</p> </li> <li>Select one of the kluster.ai-supported models from the Available Models drop-down menu</li> </ol> <p></p> <p>That's it! You're now ready to start chatting with your bot powered by kluster.ai.</p>"},{"location":"get-started/integrations/sillytavern/#test-the-connection","title":"Test the connection","text":"<p>Now that you've configured kluster.ai with SillyTavern, you can test the API connection by starting a new conversation.</p> <p>Follow these steps to get started:</p> <ol> <li>Click the menu icon on the bottom-left corner of the page</li> <li>Select Start New Chat to open a new chat with the model</li> <li>Type a message in the Type a message bar at the bottom and send it</li> <li>Verify that the chatbot has returned a response successfully</li> </ol> <p></p> <p>Troubleshooting</p> <p>If you encounter errors, revisit the configuration instructions and double-check your API key and base URL and that you've received a Valid response after connecting the API (see step 8).</p>"},{"location":"get-started/integrations/typingmind/","title":"Integrate TypingMind with kluster.ai","text":"<p>TypingMind is an intuitive frontend chat interface that enhances the UX of LLMs. It offers flexible organization for your conversations (folders, pins, bulk delete), a customizable prompt library, and the ability to build AI agents using your training data. With plugin support for internet access, image generation, and more, TypingMind seamlessly syncs across devices, providing a simplified AI workflow with tailored, high-quality responses\u2014all in one sleek platform.</p> <p>This guide will walk you through integrating kluster.ai with TypingMind, from configuration to hands-on interactions that tap into the kluster.ai API\u2014all in a single, streamlined environment.</p>"},{"location":"get-started/integrations/typingmind/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have the following prerequisites:</p>"},{"location":"get-started/integrations/typingmind/#quick-start","title":"Quick start","text":"<p>Navigate to TypingMind and take the following steps to access the custom model setup:</p> <ol> <li>Click on the model dropdown</li> <li>Click on Custom Models</li> </ol> <p></p> <p>Then, take the following steps to configure TypingMind to use the kluster.ai API:</p> <ol> <li>Provide a name, such as <code>kluster</code></li> <li>For the API Type dropdown, select OpenAI Compatible API</li> <li> <p>Provide the following URL for the Endpoint field:</p> <pre><code>https://api.kluster.ai/v1/chat/completions\n</code></pre> </li> <li> <p>Paste the name of the supported kluster.ai model you'd like to use. Note that you can specify multiple models</p> </li> <li> <p>Press Add Custom Headers and for the Key value, specify <code>Authorization</code>. In the value field on the right, enter <code>Bearer</code> followed by your kluster.ai API key as follows: </p> <pre><code>Bearer INSERT_KLUSTER_API_KEY\n</code></pre> </li> <li> <p>Press Test to ensure the connection is successful</p> </li> <li>Press Add Model to confirm adding the kluster.ai as a custom provider</li> </ol> <p></p>"},{"location":"get-started/integrations/typingmind/#set-default-provider","title":"Set default provider","text":"<p>You've configured the kluster.ai API as a provider, but it hasn't yet been selected as the default one. To change this, take the following steps: </p> <ol> <li>Click on Models on the sidebar</li> <li>Select kluster (or whatever you named your custom model)</li> <li>Press Set Default</li> </ol> <p></p> <p>And that's it! You can now query the LLM successfully using kluster.ai as the default provider. For more information on TypingMind's features, be sure to check out the TypingMind docs. The following section will examine one of TypingMind's features: prebuilt AI agents.</p> <p></p>"},{"location":"get-started/integrations/typingmind/#start-a-chat","title":"Start a chat","text":"<p>TypingMind has a wide variety of prebuilt AI agents that you can use as-is or clone and customize to suit your needs. These AI agents can use the kluster.ai API to perform tasks tailored to your use cases. To get started, take the following steps:</p> <ol> <li>Click on Agents in the sidebar</li> <li>Click on Browse Agents</li> </ol> <p></p> <p>Then select the desired agent you'd like to interact with and press the green icon to install it into your TypingMind workspace. </p> <p></p> <p>Press Chat Now to open up a new chat session with your AI agent:</p> <p></p> <p>Your AI agent is now ready to answer relevant questions and relies on the kluster.ai API to do so:</p> <p></p> <p>You can also clone and customize existing agents or create entirely new ones. For more information on agents on TypingMind, be sure to check out the TypingMind docs.</p>"},{"location":"get-started/start-building/batch/","title":"Macro Rendering Error","text":"<p>File: <code>get-started/start-building/batch.md</code></p> <p>UndefinedError: 'batch' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/mkdocs_macros/plugin.py\", line 527, in render\n    return md_template.render(**page_variables)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 46, in top-level template code\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 485, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'batch' is undefined\n</code></pre>"},{"location":"get-started/start-building/real-time/","title":"Perform real-time inference jobs","text":""},{"location":"get-started/start-building/real-time/#overview","title":"Overview","text":"<p>This guide provides guidance about how to use real-time inference with the kluster.ai API. This type of inference is best suited for use cases requiring instant, synchronous responses for user-facing features like chat interactions, live recommendations, or real-time decision-making.</p> <p>You will learn how to submit a request and retrieve responses, and where to find integration guides for using kluster.ai's API with some of your favorite third-party LLM interfaces. Please make sure you check the API request limits.</p>"},{"location":"get-started/start-building/real-time/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes familiarity with Large Language Model (LLM) development and OpenAI libraries. Before getting started, make sure you have:</p> <ul> <li>A virtual Python environment - (optional) recommended for developers using Python. It helps isolate Python installations in a virtual environment to reduce the risk of environment or package conflicts between your projects</li> <li>Required Python libraries - install the following Python libraries:<ul> <li>OpenAI Python API library - to access the <code>openai</code> module</li> <li><code>getpass</code> - to handle API keys safely</li> </ul> </li> </ul> <p>If you plan to use cURL via the CLI, you can export kluster.ai API key as a variable:</p> <pre><code>export API_KEY=INSERT_API_KEY\n</code></pre>"},{"location":"get-started/start-building/real-time/#supported-models","title":"Supported models","text":"<p>Please visit the Models page to learn more about all the models supported by the kluster.ai batch API.</p> <p>In addition, you can see the complete list of available models programmatically using the list supported models endpoint.</p>"},{"location":"get-started/start-building/real-time/#quickstart-snippets","title":"Quickstart snippets","text":"<p>The following code snippets provide a complete end-to-end real-time inference example for different models supported by kluster.ai. You can copy and paste the snippet into your local environment. </p>"},{"location":"get-started/start-building/real-time/#python","title":"Python","text":"<p>To use these snippets, run the Python script and enter your kluster.ai API key when prompted.</p> DeepSeek R1 DeepSeek V3 DeepSeek V3 0324 Gemma 3 27B LLama 3.1 8B LLama 3.1 405B LLama 3.3 70B Llama 4 Maverick 17B 128E Llama 4 Scout 17B 16E Qwen 2.5 7B"},{"location":"get-started/start-building/real-time/#cli","title":"CLI","text":"<p>Similarly, the following curl commands showcase how to easily send a chat completion request to kluster.ai for the different supported models. This example assumes you've exported your kluster.ai API key as the variable <code>API_KEY</code>.</p> DeepSeek R1 <pre><code>\n</code></pre> DeepSeek V3 <pre><code>\n</code></pre> DeepSeek V3 0324 <pre><code>\n</code></pre> Gemma 3 27B <pre><code>\n</code></pre> LLama 3.1 8B <pre><code>\n</code></pre> LLama 3.1 405B <pre><code>\n</code></pre> LLama 3.3 70B <pre><code>\n</code></pre> Llama 4 Maverick 17B 128E <pre><code>\n</code></pre> Llama 4 Scout 17B 16E <pre><code>\n</code></pre> Qwen 2.5 7B <pre><code>\n</code></pre>"},{"location":"get-started/start-building/real-time/#real-time-inference-flow","title":"Real-time inference flow","text":"<p>This section details the real-time inference process using the kluster.ai API and DeepSeek R1 model, but you can adapt it to any of the supported models.</p>"},{"location":"get-started/start-building/real-time/#submitting-a-request","title":"Submitting a request","text":"<p>The kluster.ai platform offers a simple, OpenAI-compatible interface, making it easy to integrate kluster.ai services seamlessly into your existing system.</p> <p>The following code shows how to do a chat completions request using the OpenAI library.</p> Python <p>If successful, the <code>completion</code> variable contains a full response, which you'll need to analyze to extract the answer you are looking for. In terms of configuration for real-time inferences, there are several parameters that you need to tweak:</p> <ul> <li><code>model</code> string required - name of one of the supported models</li> <li><code>messages</code> array required - a list of chat messages (<code>system</code>, <code>user</code>, or <code>assistant</code> roles, and also <code>image_url</code> for images). In this example, the query is \"What is the ultimate breakfast sandwich?\". </li> </ul> <p>Once these parameters are configured, run your script to send the request.</p>"},{"location":"get-started/start-building/real-time/#fetching-the-response","title":"Fetching the response","text":"<p>If the request is successful, the response is contained in the <code>completion</code> variable from the example above. It should follow the structure below and include relevant data such as the generated output, metadata, and token usage details. </p> Response<pre><code>{\n    \"id\": \"a3af373493654dd195108b207e2faacf\",\n    \"choices\": [\n        {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"logprobs\": null,\n            \"message\": {\n                \"content\": \"The \\\"ultimate\\\" breakfast sandwich is subjective and can vary based on personal preferences, but here\u2019s a classic, crowd-pleasing version that combines savory, sweet, and hearty elements for a satisfying morning meal:\\n\\n### **The Ultimate Breakfast Sandwich**\\n**Ingredients:**\\n- **Bread:** A toasted brioche bun, English muffin, or sourdough slice (your choice for texture and flavor).\\n- **Protein:** Crispy bacon, sausage patty, or ham.\\n- **Egg:** Fried, scrambled, or a fluffy omelet-style egg.\\n- **Cheese:** Sharp cheddar, gooey American, or creamy Swiss.\\n- **Sauce:** Spicy mayo, hollandaise, or a drizzle of maple syrup for sweetness.\\n- **Extras:** Sliced avocado, caramelized onions, saut\u00e9ed mushrooms, or fresh arugula for a gourmet touch.\\n- **Seasoning:** Salt, pepper, and a pinch of red pepper flakes for heat.\\n\\n**Assembly:**\\n1. Toast your bread or bun to golden perfection.\\n2. Cook your protein to your desired crispiness or doneness.\\n3. Prepare your egg\u2014fried with a runny yolk is a classic choice.\\n4. Layer the cheese on the warm egg or protein so it melts slightly.\\n5. Add your extras (avocado, veggies, etc.) for freshness and flavor.\\n6. Spread your sauce on the bread or drizzle it over the filling.\\n7. Stack everything together, season with salt, pepper, or spices, and enjoy!\\n\\n**Optional Upgrades:**\\n- Add a hash brown patty for extra crunch.\\n- Swap regular bacon for thick-cut or maple-glazed bacon.\\n- Use a croissant instead of bread for a buttery, flaky twist.\\n\\nThe ultimate breakfast sandwich is all about balance\u2014crunchy, creamy, savory, and a hint of sweetness. Customize it to your taste and make it your own!\",\n                \"refusal\": null,\n                \"role\": \"assistant\",\n                \"audio\": null,\n                \"function_call\": null,\n                \"tool_calls\": null\n            },\n            \"matched_stop\": 1\n        }\n    ],\n    \"created\": 1742378836,\n    \"model\": \"deepseek-ai/DeepSeek-V3\",\n    \"object\": \"chat.completion\",\n    \"service_tier\": null,\n    \"system_fingerprint\": null,\n    \"usage\": {\n        \"completion_tokens\": 398,\n        \"prompt_tokens\": 10,\n        \"total_tokens\": 408,\n        \"completion_tokens_details\": null,\n        \"prompt_tokens_details\": null\n    }\n}\n</code></pre> <p>The following snippet demonstrates how to extract the data, log it to the console, and save it to a JSON file.</p> Python <pre><code>\n</code></pre> <p>For a detailed breakdown of the chat completion object, see the chat completion API reference section.</p> View the complete script Python <pre><code>\n</code></pre>"},{"location":"get-started/start-building/real-time/#third-party-integrations","title":"Third-party integrations","text":"<p>You can also set up third-party LLM integrations using the kluster.ai API. For step-by-step instructions, check out the following integration guides:</p> <ul> <li>SillyTavern - multi-LLM chat interface</li> <li>LangChain - multi-turn conversational agent</li> <li>eliza - create and manage AI agents</li> <li>CrewAI - specialized agents for complex tasks</li> <li>LiteLLM - streaming response and multi-turn conversation handling</li> </ul>"},{"location":"get-started/start-building/real-time/#summary","title":"Summary","text":"<p>You have now experienced the complete real-time inference job lifecycle using kluster.ai's chat completion API. In this guide, you've learned:</p> <ul> <li>How to submit a real-rime inference request</li> <li>How to configure real-time inference-related API parameters</li> <li>How to interpret the chat completion object API response</li> </ul> <p>The kluster.ai batch API is designed to efficiently and reliably handle your large-scale LLM workloads. If you have questions or suggestions, the support team would love to hear from you.</p>"},{"location":"get-started/start-building/setup/","title":"Macro Rendering Error","text":"<p>File: <code>get-started/start-building/setup.md</code></p> <p>UndefinedError: 'libraries' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/mkdocs_macros/plugin.py\", line 527, in render\n    return md_template.render(**page_variables)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 1301, in render\n    self.environment.handle_exception()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 936, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 20, in top-level template code\n  File \"/opt/hostedtoolcache/Python/3.13.3/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 485, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'libraries' is undefined\n</code></pre>"},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/","title":"Fine-tuning","text":"<p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul> <p>In this notebook, we'll use Python's <code>getpass</code> module to safely input the key. Provide your unique kluster.ai API key (ensure there are no spaces).</p> In\u00a0[\u00a0]: Copied! <pre>from getpass import getpass\n\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass  api_key = getpass(\"Enter your kluster.ai API key: \") In\u00a0[\u00a0]: Copied! <pre>%pip install -q openai\n</pre> %pip install -q openai In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nimport pandas as pd\nfrom openai import OpenAI\nimport time\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport requests\npd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500)\n\n# Import helper functions\nurl = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/main/examples/helpers.py\"\n\n# Fetch the file and save it locally\nresponse = requests.get(url)\nwith open(\"helpers.py\", \"w\") as f:\n    f.write(response.text)\n\n# Import the helper functions\nfrom helpers import create_tasks, save_tasks, create_batch_job, monitor_job_status, get_results\n</pre> import urllib.request import pandas as pd from openai import OpenAI import time import json import matplotlib.pyplot as plt import seaborn as sns import os import requests pd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500)  # Import helper functions url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/main/examples/helpers.py\"  # Fetch the file and save it locally response = requests.get(url) with open(\"helpers.py\", \"w\") as f:     f.write(response.text)  # Import the helper functions from helpers import create_tasks, save_tasks, create_batch_job, monitor_job_status, get_results  In\u00a0[\u00a0]: Copied! <pre># Set up the client\nclient_prod = OpenAI(\n    base_url=\"https://api.kluster.ai/v1\",\n    api_key=api_key,\n)\n</pre> # Set up the client client_prod = OpenAI(     base_url=\"https://api.kluster.ai/v1\",     api_key=api_key, ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/main/data/financial-phrasebank.csv\"\nurllib.request.urlretrieve(url,filename='financial-phrasebank.csv')\n\n# Load and process the dataset based on URL content\ndf = pd.read_csv('financial-phrasebank.csv', encoding = \"ISO-8859-1\",header=None, names=[\"sentiment\", \"text\"])\n\n# For a faster running example, adjust the below variable to select a smaller subset of financial training content. Must be &gt; 100.\ndf = df.iloc[:4000]\ndf.head(3)\n</pre> url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/main/data/financial-phrasebank.csv\" urllib.request.urlretrieve(url,filename='financial-phrasebank.csv')  # Load and process the dataset based on URL content df = pd.read_csv('financial-phrasebank.csv', encoding = \"ISO-8859-1\",header=None, names=[\"sentiment\", \"text\"])  # For a faster running example, adjust the below variable to select a smaller subset of financial training content. Must be &gt; 100. df = df.iloc[:4000] df.head(3) <p>Next, we need to split the data into training and testing datasets (to be used later).</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import train_test_split\n# Split into train and test sets\ntrain_df, test_df = train_test_split(df, test_size=0.9, random_state=42)\n</pre> from sklearn.model_selection import train_test_split # Split into train and test sets train_df, test_df = train_test_split(df, test_size=0.9, random_state=42) In\u00a0[\u00a0]: Copied! <pre>len(train_df)\n</pre> len(train_df) <p>Fine-tuning is the process of adjusting a pre-trained model with new, domain-specific data to enhance performance for a specific task, which typically reduces training time and costs compared to training from scratch. Additionally, it can allow smaller, fine-tuned models to match or even rival the performance of larger, general models that haven\u2019t been fine-tuned.</p> In\u00a0[\u00a0]: Copied! <pre>SYSTEM_PROMPT = '''\n    You are a helpful assistant specializing in determining the sentiment of financial news.\n    Analyze the following text regarding financial information and assign one of the following labels to indicate its sentiment: positive, negative, or neutral.\n    Provide your response as a single word without any punctuation.\n    '''\n\n# Ensure the directory exists\nos.makedirs(\"finetuning/data\", exist_ok=True)\n\n# Generate JSONLines file\nwith open(\"finetuning/data/sentiment.jsonl\", \"w\") as f:\n    for _, row in train_df.iterrows():\n        # Create the message structure\n        messages = {\n            \"messages\": [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": row['text']},\n                {\"role\": \"assistant\", \"content\": row[\"sentiment\"]}\n            ]\n        }\n        # Write to the file as a single JSON object per line\n        f.write(json.dumps(messages) + \"\\n\")\n</pre> SYSTEM_PROMPT = '''     You are a helpful assistant specializing in determining the sentiment of financial news.     Analyze the following text regarding financial information and assign one of the following labels to indicate its sentiment: positive, negative, or neutral.     Provide your response as a single word without any punctuation.     '''  # Ensure the directory exists os.makedirs(\"finetuning/data\", exist_ok=True)  # Generate JSONLines file with open(\"finetuning/data/sentiment.jsonl\", \"w\") as f:     for _, row in train_df.iterrows():         # Create the message structure         messages = {             \"messages\": [                 {\"role\": \"system\", \"content\": SYSTEM_PROMPT},                 {\"role\": \"user\", \"content\": row['text']},                 {\"role\": \"assistant\", \"content\": row[\"sentiment\"]}             ]         }         # Write to the file as a single JSON object per line         f.write(json.dumps(messages) + \"\\n\") In\u00a0[\u00a0]: Copied! <pre>data_dir = 'finetuning/data/sentiment.jsonl'\n\nwith open(data_dir, 'rb') as file:\n    upload_response = client_prod.files.create(\n        file=file,\n        purpose=\"fine-tune\"\n    )\n    file_id = upload_response.id\n    print(f\"File uploaded successfully. File ID: {file_id}\")\n</pre> data_dir = 'finetuning/data/sentiment.jsonl'  with open(data_dir, 'rb') as file:     upload_response = client_prod.files.create(         file=file,         purpose=\"fine-tune\"     )     file_id = upload_response.id     print(f\"File uploaded successfully. File ID: {file_id}\") <p>Next, we'll submit the job to the kluster.ai fine-tuning API. Currently, two base models are supported for fine-tuning:</p> <ul> <li>klusterai/Meta-Llama-3.1-8B-Instruct-Turbo</li> <li>klusterai/Meta-Llama-3.3-70B-Instruct-Turbo</li> </ul> <p>If you specify a different base model, the fine-tuning job will fail. You can also tweak the hyperparameters (such as number of epochs, batch size, and learning rate) to adjust training time and potential performance gains. Remember that increasing the number of epochs will lead to longer training time but may result in higher performance. If you're unsure which hyperparameters to set, you can also comment them out to accept the default values.</p> In\u00a0[\u00a0]: Copied! <pre>job = client_prod.fine_tuning.jobs.create(\n    training_file=file_id,\n    model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    #hyperparameters={\n    #   \"batch_size\": 4,\n    #   \"n_epochs\": 2,\n    #   \"learning_rate_multiplier\": 1\n    #}\n)\nprint(\"\\nFine-tuning job created:\")\nprint(json.dumps(job.model_dump(), indent=2))\n</pre> job = client_prod.fine_tuning.jobs.create(     training_file=file_id,     model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",     #hyperparameters={     #   \"batch_size\": 4,     #   \"n_epochs\": 2,     #   \"learning_rate_multiplier\": 1     #} ) print(\"\\nFine-tuning job created:\") print(json.dumps(job.model_dump(), indent=2)) <pre>\nFine-tuning job created:\n{\n  \"id\": \"67b504e2451f71cc68416fb5\",\n  \"created_at\": 1739916514,\n  \"error\": null,\n  \"fine_tuned_model\": null,\n  \"finished_at\": null,\n  \"hyperparameters\": {\n    \"batch_size\": 1,\n    \"learning_rate_multiplier\": 1.0,\n    \"n_epochs\": 10\n  },\n  \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n  \"object\": \"fine_tuning.job\",\n  \"organization_id\": null,\n  \"result_files\": [],\n  \"seed\": null,\n  \"status\": \"queued\",\n  \"trained_tokens\": null,\n  \"training_file\": \"67b504e1e56b50d27357b6b0\",\n  \"validation_file\": null,\n  \"estimated_finish\": null,\n  \"integrations\": [],\n  \"method\": {\n    \"dpo\": null,\n    \"supervised\": {\n      \"hyperparameters\": null,\n      \"batch_size\": 1,\n      \"learning_rate_multiplier\": 1,\n      \"n_epochs\": 10\n    },\n    \"type\": \"supervised\"\n  }\n}\n</pre> <p>Next, we can retrieve the status of the job through its ID. The following snippet checks the status every 30 seconds.</p> In\u00a0[\u00a0]: Copied! <pre>while True:\n    job_status = client_prod.fine_tuning.jobs.retrieve(job.id)\n    status = job_status.status\n    print(f\"\\nCurrent status: {status}\")\n\n    events = client_prod.fine_tuning.jobs.list_events(job.id)\n    events_list = [e.model_dump() for e in events]\n    events_list.sort(key=lambda x: x['created_at'])\n    print(\"\\nJob events:\")\n    print(json.dumps(events_list, indent=2))\n\n    if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n        break\n\n    time.sleep(30)\n</pre> while True:     job_status = client_prod.fine_tuning.jobs.retrieve(job.id)     status = job_status.status     print(f\"\\nCurrent status: {status}\")      events = client_prod.fine_tuning.jobs.list_events(job.id)     events_list = [e.model_dump() for e in events]     events_list.sort(key=lambda x: x['created_at'])     print(\"\\nJob events:\")     print(json.dumps(events_list, indent=2))      if status in [\"succeeded\", \"failed\", \"cancelled\"]:         break      time.sleep(30) <pre>\nCurrent status: validating_files\n\nJob events:\n[\n  {\n    \"id\": \"67b504e26913e957964c1232\",\n    \"created_at\": 1739916514,\n    \"level\": \"info\",\n    \"message\": \"Validating training file: 67b504e1e56b50d27357b6b0\",\n    \"object\": \"fine_tuning.job.event\",\n    \"data\": {},\n    \"type\": \"message\"\n  },\n  {\n    \"id\": \"67b504e2451f71cc68416fb7\",\n    \"created_at\": 1739916514,\n    \"level\": \"info\",\n    \"message\": \"Created fine-tuning job: 67b504e2451f71cc68416fb5\",\n    \"object\": \"fine_tuning.job.event\",\n    \"data\": {},\n    \"type\": \"message\"\n  }\n]\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>fine_tuned_model = job_status.fine_tuned_model\n</pre> fine_tuned_model = job_status.fine_tuned_model In\u00a0[\u00a0]: Copied! <pre>job_status.fine_tuned_model\n</pre> job_status.fine_tuned_model <p>Congratulations! You've now created a fine-tuned model. The exact name of your fine-tuned model is above.</p> <p>In the next section, you'll submit batch requests to your fine-tuned model. However, you can also submit one-off requests as follows (remember to provide your kluster.ai API key and the name of your fine-tuned model):</p> <pre>curl https://api.kluster.ai/v1/chat/completions \\\n  -H \"Authorization: Bearer INSERT_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"INSERT_FINE_TUNED_MODEL\",\n    \"max_completion_tokens\": 5000,\n    \"temperature\": 0.6,\n    \"top_p\": 1,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant specializing in determining the sentiment of financial news.\\nAnalyze the following text regarding financial information and assign one of the following labels to indicate its sentiment: positive, negative, or neutral.\\nProvide your response as a single word without any punctuation.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Net sales increased to EUR655m in April to June 2010 from EUR438m a year earlier.\"\n      }\n    ]\n  }'\n</pre> <p>In real-world scenarios, you often need to assess your model\u2019s performance on a broad set of inputs rather than just a single prompt. This is where batch inference comes in: by sending multiple prompts in one job, you can quickly gather outputs across diverse examples and see how well your model generalizes.</p> <p>In this section, we\u2019ll run batch requests to the fine-tuned model and baseline models, then compare their outputs. After the jobs finish, we\u2019ll retrieve the responses, measure their accuracy against the ground truth, and highlight where the fine-tuned model excels\u2014or needs further tuning\u2014relative to more generalized models.</p> <p>With LLMs, writing a good prompt, including the system prompt, is essential. Below, you can see an example instruction for the LLM. Feel free to experiment with it and see how it changes the performance!</p> In\u00a0[\u00a0]: Copied! <pre>SYSTEM_PROMPT = '''\n    You are a helpful assistant specializing in determining the sentiment of financial news.\n    Analyze the following text regarding financial information and assign one of the following labels to indicate its sentiment: positive, negative, or neutral.\n    Provide your response as a single word without any punctuation.\n    '''\n</pre> SYSTEM_PROMPT = '''     You are a helpful assistant specializing in determining the sentiment of financial news.     Analyze the following text regarding financial information and assign one of the following labels to indicate its sentiment: positive, negative, or neutral.     Provide your response as a single word without any punctuation.     ''' <p>Now that the prompt is defined, it's time to execute the code and run the classification task for each model. In this step, we loop through the list of models, creating the requests and batch jobs, monitoring progress, and retrieving the results.</p> In\u00a0[\u00a0]: Copied! <pre># Define models\nmodels = {\n        '8B':\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n        '70B':\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\",\n        '405B':\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\",\n        'ft_8B': fine_tuned_model\n        }\n\n# Process each model: create tasks, run jobs, and get results\nfor name, model in models.items():\n    task_list = create_tasks(test_df, task_type='assistant', system_prompt=SYSTEM_PROMPT, model=model, content_column='text')\n    filename = save_tasks(task_list, task_type='assistant')\n    if name != 'ft_8B':\n        job = create_batch_job(filename, client=client_prod)\n        monitor_job_status(client=client_prod, job_id=job.id, task_type=f'{name} model')\n        test_df[f'answer_base_{name}'] = get_results(client=client_prod, job_id=job.id)\n    else:\n        job = create_batch_job(filename, client=client_prod)\n        monitor_job_status(client=client_prod, job_id=job.id, task_type=f'{name} model')\n        test_df[f'answer_{name}'] = get_results(client=client_prod, job_id=job.id)\n</pre> # Define models models = {         '8B':\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",         '70B':\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\",         '405B':\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\",         'ft_8B': fine_tuned_model         }  # Process each model: create tasks, run jobs, and get results for name, model in models.items():     task_list = create_tasks(test_df, task_type='assistant', system_prompt=SYSTEM_PROMPT, model=model, content_column='text')     filename = save_tasks(task_list, task_type='assistant')     if name != 'ft_8B':         job = create_batch_job(filename, client=client_prod)         monitor_job_status(client=client_prod, job_id=job.id, task_type=f'{name} model')         test_df[f'answer_base_{name}'] = get_results(client=client_prod, job_id=job.id)     else:         job = create_batch_job(filename, client=client_prod)         monitor_job_status(client=client_prod, job_id=job.id, task_type=f'{name} model')         test_df[f'answer_{name}'] = get_results(client=client_prod, job_id=job.id) <p>In the chart below, we compare three text samples and evaluate each model\u2019s outputs against the ground truth. While results may vary in different fine-tuning runs, we observe consistent trends: notably, the fine-tuned model's performance closely matches that of the larger (and more expensive) base model. This suggests that fine-tuning can deliver higher accuracy on your specific tasks at a lower cost than relying on higher-parameter models.</p> In\u00a0[\u00a0]: Copied! <pre>test_df.tail(3)\n</pre> test_df.tail(3) In\u00a0[\u00a0]: Copied! <pre># Rename dictionary\nrename_dict = {\n    'answer_base_8B': 'Base 8b model',\n    'answer_base_70B': 'Base 70b model',\n    'answer_base_405B': 'Base 405b model',\n    'answer_ft_8B': 'Fine Tuned 8b model',\n}\n\n# Calculate accuracy for each model with renamed keys\naccuracies = {}\nfor name in rename_dict:\n    accuracy = test_df.apply(lambda row: row[name] in row['sentiment'], axis=1).mean()\n    accuracies[rename_dict[name]] = accuracy\n\n# Horizontal bar chart\nfig, ax = plt.subplots(figsize=(6, 4))\nsns.barplot(\n    y=list(accuracies.keys()),\n    x=list(accuracies.values()),\n    hue=list(accuracies.keys()),  # Add a hue based on the model names\n    palette=\"viridis\",\n    edgecolor='black',\n    ax=ax,\n    legend=False  # Disable the unnecessary legend\n)\n\n# Add labels to bars\nfor i, bar in enumerate(ax.patches):\n    ax.text(bar.get_width() - 0.02,\n            bar.get_y() + bar.get_height() / 2,\n            f\"{list(accuracies.values())[i]:.3f}\",\n            ha='right', va='center', color='white', fontsize=12, fontweight='bold')\n\n# Set plot aesthetics\nax.set_xlim(0, max(accuracies.values()) + 0.05)\nax.set_xlabel('Accuracy', fontsize=12)\nax.set_ylabel('Model', fontsize=12)\nax.set_title('Classification Accuracy by Model', fontsize=14, fontweight='bold')\nsns.despine()\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n</pre> # Rename dictionary rename_dict = {     'answer_base_8B': 'Base 8b model',     'answer_base_70B': 'Base 70b model',     'answer_base_405B': 'Base 405b model',     'answer_ft_8B': 'Fine Tuned 8b model', }  # Calculate accuracy for each model with renamed keys accuracies = {} for name in rename_dict:     accuracy = test_df.apply(lambda row: row[name] in row['sentiment'], axis=1).mean()     accuracies[rename_dict[name]] = accuracy  # Horizontal bar chart fig, ax = plt.subplots(figsize=(6, 4)) sns.barplot(     y=list(accuracies.keys()),     x=list(accuracies.values()),     hue=list(accuracies.keys()),  # Add a hue based on the model names     palette=\"viridis\",     edgecolor='black',     ax=ax,     legend=False  # Disable the unnecessary legend )  # Add labels to bars for i, bar in enumerate(ax.patches):     ax.text(bar.get_width() - 0.02,             bar.get_y() + bar.get_height() / 2,             f\"{list(accuracies.values())[i]:.3f}\",             ha='right', va='center', color='white', fontsize=12, fontweight='bold')  # Set plot aesthetics ax.set_xlim(0, max(accuracies.values()) + 0.05) ax.set_xlabel('Accuracy', fontsize=12) ax.set_ylabel('Model', fontsize=12) ax.set_title('Classification Accuracy by Model', fontsize=14, fontweight='bold') sns.despine()  # Show the plot plt.tight_layout() plt.show()"},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#fine-tuning-models-with-the-klusterai-api","title":"Fine-tuning models with the kluster.ai API\u00b6","text":""},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#introduction","title":"Introduction\u00b6","text":"<p>Fine-tuning is the process of customizing an existing model using new data to improve performance on a specific task. Fine-tuning can offer valuable benefits: it can significantly improve performance for your specific use case and sometimes rival the results of more expensive, general-purpose models.</p> <p>In this guide, you'll learn how to train a sentiment analysis model tailored to your data using kluster.ai. We'll walk through each step of the fine-tuning process\u2014covering dataset setup, environment configuration, and batch inference. By following along, you'll discover how to leverage kluster.ai's powerful platform to create a custom model that boosts accuracy for financial text analysis and beyond.</p>"},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#prerequisites","title":"Prerequisites\u00b6","text":""},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#fetch-a-real-dataset-for-batch-inference","title":"Fetch a real dataset for batch inference\u00b6","text":"<p>This dataset contains a variety of financial news headlines, each labeled as positive, negative, or neutral. In this context, positive indicates a beneficial impact on the company\u2019s stock, negative suggests a detrimental impact, and neutral implies no significant change is expected.</p> <p>In this example, we limit the dataset to the first 4000 rows of the financial phrasebank, resulting in 400 training examples after splitting. For a faster running example, you can select as little as 100 rows of data, as kluster.ai requires a minimum of 10 examples.</p>"},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#split-data-into-traintest-for-fine-tuning","title":"Split data into train/test for fine-tuning\u00b6","text":""},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#fine-tuning-the-model","title":"Fine-tuning the model\u00b6","text":""},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#test-the-fine-tuned-model-with-batch-inference","title":"Test the fine-tuned model with batch inference\u00b6","text":""},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#compare-the-results","title":"Compare the results\u00b6","text":"<p>In this step, we compare the fine-tuned model's classification accuracy against various baseline models. We can determine whether fine-tuning delivers meaningful improvements over larger general-purpose models by calculating and visualizing their performance.</p>"},{"location":"tutorials/klusterai-api/finetuning-sent-analysis/#conclusion","title":"Conclusion\u00b6","text":"<p>As shown in the chart above, the fine-tuned model outperforms the base model with the provided training data and default hyperparameters. Training on 400 financial phrases with these defaults can take multiple hours. Once you complete this tutorial successfully, feel free to explore different hyperparameters, datasets, prompts, and the various models available from kluster.ai. Good luck!</p>"},{"location":"tutorials/klusterai-api/image-analysis/","title":"Image analysis","text":"<p>AI models can be used to perform image analysis tasks, in which you feed the model an image and request it to extract meaningful information.</p> <p>This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to run image analysis on different images using Gemma 3 27B, Qwen 2.5 7B, Llama 4 Maverick 17B 128E and Llama 4 Scout 17B 16E.</p> <p>The example uses four separate images. For each image, we will ask the models to fetch a specific feature and compare how they respond:</p> <ul> <li>A Newton's cradle, we will ask what the device's name is and how many balls are in the image (5)</li> <li>Eggs of different colors, we will ask how many total eggs and per color (10 total, 8 brown and 2 white)</li> <li>Alien-only parking sign, we will ask to interpret the sign (only aliens can park, funny reference)</li> <li>Handwritten note with a typo, we will ask what the text in the image is and to find a typo (\"I LOVE PROGRAMING\", \"I love programing\" missing an \"m\" in both instances)</li> </ul> <p>You can adapt this example by using your own images or requests. With this approach, you can effortlessly process images of any scale, big or small, and obtain image analysis powered by a state-of-the-art language model.</p> <p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul> <p>In this notebook, we'll use Python's <code>getpass</code> module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces).</p> In\u00a0[1]: Copied! <pre>from getpass import getpass\n\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass  api_key = getpass(\"Enter your kluster.ai API key: \") <p>Next, ensure you've installed OpenAI Python library:</p> In\u00a0[2]: Copied! <pre>%pip install -q openai\n</pre> %pip install -q openai <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> <p>With the OpenAI Python library installed, we import the necessary dependencies for the tutorial:</p> In\u00a0[3]: Copied! <pre>from openai import OpenAI\n\nimport pandas as pd\nimport time\nimport json\nimport os\nimport re\nfrom IPython.display import clear_output, display\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport requests\nfrom collections import defaultdict\nfrom io import BytesIO\n</pre> from openai import OpenAI  import pandas as pd import time import json import os import re from IPython.display import clear_output, display import matplotlib.pyplot as plt import matplotlib.image as mpimg import requests from collections import defaultdict from io import BytesIO <p>Then, initialize the <code>client</code> by pointing it to the kluster.ai endpoint and passing your API key.</p> In\u00a0[4]: Copied! <pre># Set up the client\nclient = OpenAI(\n    base_url=\"https://api.kluster.ai/v1\",\n    api_key=api_key,\n)\n</pre> # Set up the client client = OpenAI(     base_url=\"https://api.kluster.ai/v1\",     api_key=api_key, ) <p>There are two main ways to proceed when working with images:</p> <ol> <li>You can provide the raw image file as a URL from the source, for example, GitHub</li> <li>You can provide Base64 encoded. These are typically represented with a blob of text, which starts with <code>data:image/png;base64,ENCODING_DATA_HERE...</code></li> </ol> <p>With both methodologies, image data needs to be provided as an object in the content array with the following format:</p> <pre><code>...\n{\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_DATA}}\n...\n</code></pre> <p>Just replace <code>IMAGE_DATA</code> with either the URL with the raw image file or the Base64 encoded image. This tutorial uses the URL of the images uploaded to GitHub:</p> In\u00a0[5]: Copied! <pre>base_url = (\n    \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/IMAGE_NAME?raw=true\"\n)\n\n# Newton's cradle image\n# Expected answer: Newton's cradle, 5 balls\nimage1 = \"balls-image.jpeg\"\nimage1_url = base_url.replace(\"IMAGE_NAME\", image1)\nprint(image1_url)\n\n\n# Eggs\n# Expected answer: 10 eggs, 8 brown and 2 white\nimage2 = \"eggs-image.jpeg\"\nimage2_url = base_url.replace(\"IMAGE_NAME\", image2)\nprint(image2_url)\n\n# Parking sign\n# Expected answer: Parking only allowed for Aliens (funny)\nimage3 = \"parking-image.jpeg\"\nimage3_url = base_url.replace(\"IMAGE_NAME\", image3)\nprint(image3_url)\n\n# Text\n# Expected answer: I love programming in both all caps and regular\nimage4 = \"text-typo-image.jpeg\"\nimage4_url = base_url.replace(\"IMAGE_NAME\", image4)\nprint(image4_url)\n\n\nimages = [image1, image2, image3, image4]\n</pre> base_url = (     \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/IMAGE_NAME?raw=true\" )  # Newton's cradle image # Expected answer: Newton's cradle, 5 balls image1 = \"balls-image.jpeg\" image1_url = base_url.replace(\"IMAGE_NAME\", image1) print(image1_url)   # Eggs # Expected answer: 10 eggs, 8 brown and 2 white image2 = \"eggs-image.jpeg\" image2_url = base_url.replace(\"IMAGE_NAME\", image2) print(image2_url)  # Parking sign # Expected answer: Parking only allowed for Aliens (funny) image3 = \"parking-image.jpeg\" image3_url = base_url.replace(\"IMAGE_NAME\", image3) print(image3_url)  # Text # Expected answer: I love programming in both all caps and regular image4 = \"text-typo-image.jpeg\" image4_url = base_url.replace(\"IMAGE_NAME\", image4) print(image4_url)   images = [image1, image2, image3, image4] <pre>https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\nhttps://github.com/kluster-ai/klusterai-cookbook/blob/main/images/eggs-image.jpeg?raw=true\nhttps://github.com/kluster-ai/klusterai-cookbook/blob/main/images/parking-image.jpeg?raw=true\nhttps://github.com/kluster-ai/klusterai-cookbook/blob/main/images/text-typo-image.jpeg?raw=true\n</pre> <p>To execute the batch inference job, we'll take the following steps:</p> <ol> <li>Create the batch job file - we'll generate a JSON lines file with the desired requests to be processed by the model</li> <li>Upload the batch job file - once it is ready, we'll upload it to the kluster.ai platform using the API, where it will be processed. We'll receive a unique ID associated with our file</li> <li>Start the batch job - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before</li> <li>Monitor job progress - (optional) track the status of the batch job to ensure it has been successfully completed</li> <li>Retrieve results - once the job has completed execution, we can access and process the resultant data</li> </ol> <p>This notebook is prepared for you to follow along. Run the cells below to watch it all come together.</p> <p>This example uses two models more oriented to image vision/analysis: <code>google/gemma-3-27b-it</code>, <code>Qwen/Qwen2.5-VL-7B-Instruct</code>, <code>meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8</code> and <code>meta-llama/Llama-4-Scout-17B-16E-Instruct</code>. Other models might not support providing images.</p> <p>In addition, please refer to the Supported models section for a list of the models we support.</p> <p>The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its own model. Also, we are using a temperature of <code>0.5</code> but feel free to change it and play around with the different outcomes.</p> In\u00a0[6]: Copied! <pre># System prompt\nSYSTEM_PROMPT = \"\"\"\nYou are a helpful assistant that analyzes image content and provide short concise answers.\n\"\"\"\n# Prompt based on image\nUSER_PROMPTS = {\n    \"balls-image.jpeg\": \"\"\"\n    Tell me the device depicted in the image, and how many balls it has.\n    \"\"\",\n    \"eggs-image.jpeg\": \"\"\"\n    Count how many eggs are in total, and how many brown and white eggs separately.\n    \"\"\",\n    \"parking-image.jpeg\": \"\"\"\n    Tell me what you see in the image, anything interesting?.\n    \"\"\",\n    \"text-typo-image.jpeg\": \"\"\"\n    Tell me the text written in the image, find any typos if any.\n    \"\"\",\n}\n\n# Models\nmodels = {\n    \"Gemma3-27B\": \"google/gemma-3-27b-it\",\n    \"Qwen2.5-7B\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n    \"Llama4-Maverick-17B\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n    \"Llama4-Scout-17B\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n}\n\n# Ensure the directory exists\nos.makedirs(\"image_analysis\", exist_ok=True)\n\n# Create the batch job file with the prompt and content for the model and the image\ndef create_batch_file(model, image):\n    image_url = base_url.replace(\"IMAGE_NAME\", image)\n    request = {\n        \"custom_id\": f\"image-{image}-{model}-analysis\",\n        \"method\": \"POST\",\n        \"url\": \"/v1/chat/completions\",\n        \"body\": {\n            \"model\": models[model],\n            \"temperature\": 1,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": USER_PROMPTS[image]},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                    ],\n                },\n            ],\n        },\n    }\n\n    return request\n\n\n# Save file\ndef save_batch_file(batch_requests, model):\n    filename = f\"image_analysis/batch_job_{model}_request.jsonl\"\n    with open(filename, \"w\") as file:\n        for request in batch_requests:\n            file.write(json.dumps(request) + \"\\n\")\n    return filename\n</pre> # System prompt SYSTEM_PROMPT = \"\"\" You are a helpful assistant that analyzes image content and provide short concise answers. \"\"\" # Prompt based on image USER_PROMPTS = {     \"balls-image.jpeg\": \"\"\"     Tell me the device depicted in the image, and how many balls it has.     \"\"\",     \"eggs-image.jpeg\": \"\"\"     Count how many eggs are in total, and how many brown and white eggs separately.     \"\"\",     \"parking-image.jpeg\": \"\"\"     Tell me what you see in the image, anything interesting?.     \"\"\",     \"text-typo-image.jpeg\": \"\"\"     Tell me the text written in the image, find any typos if any.     \"\"\", }  # Models models = {     \"Gemma3-27B\": \"google/gemma-3-27b-it\",     \"Qwen2.5-7B\": \"Qwen/Qwen2.5-VL-7B-Instruct\",     \"Llama4-Maverick-17B\": \"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",     \"Llama4-Scout-17B\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", }  # Ensure the directory exists os.makedirs(\"image_analysis\", exist_ok=True)  # Create the batch job file with the prompt and content for the model and the image def create_batch_file(model, image):     image_url = base_url.replace(\"IMAGE_NAME\", image)     request = {         \"custom_id\": f\"image-{image}-{model}-analysis\",         \"method\": \"POST\",         \"url\": \"/v1/chat/completions\",         \"body\": {             \"model\": models[model],             \"temperature\": 1,             \"messages\": [                 {\"role\": \"system\", \"content\": SYSTEM_PROMPT},                 {                     \"role\": \"user\",                     \"content\": [                         {\"type\": \"text\", \"text\": USER_PROMPTS[image]},                         {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},                     ],                 },             ],         },     }      return request   # Save file def save_batch_file(batch_requests, model):     filename = f\"image_analysis/batch_job_{model}_request.jsonl\"     with open(filename, \"w\") as file:         for request in batch_requests:             file.write(json.dumps(request) + \"\\n\")     return filename  <p>Let's run the functions we've defined before:</p> In\u00a0[7]: Copied! <pre>filenames = []\n\nfor image in images:\n    batch_requests = []\n    for model in models:\n        batch_request = create_batch_file(model, image)\n        batch_requests.append(batch_request)\n    filename = save_batch_file(batch_requests, image)\n    filenames.append(filename)\n    print(filename)\n</pre> filenames = []  for image in images:     batch_requests = []     for model in models:         batch_request = create_batch_file(model, image)         batch_requests.append(batch_request)     filename = save_batch_file(batch_requests, image)     filenames.append(filename)     print(filename) <pre>image_analysis/batch_job_balls-image.jpeg_request.jsonl\nimage_analysis/batch_job_eggs-image.jpeg_request.jsonl\nimage_analysis/batch_job_parking-image.jpeg_request.jsonl\nimage_analysis/batch_job_text-typo-image.jpeg_request.jsonl\n</pre> <p>Next, we can preview what one of the batch job files looks like:</p> In\u00a0[8]: Copied! <pre>!head -n 1 image_analysis/batch_job_balls-image.jpeg_request.jsonl\n</pre> !head -n 1 image_analysis/batch_job_balls-image.jpeg_request.jsonl <pre>{\"custom_id\": \"image-balls-image.jpeg-Gemma3-27B-analysis\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"google/gemma-3-27b-it\", \"temperature\": 1, \"messages\": [{\"role\": \"system\", \"content\": \"\\nYou are a helpful assistant that analyzes image content and provide short concise answers.\\n\"}, {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"\\n    Tell me the device depicted in the image, and how many balls it has.\\n    \"}, {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://github.com/kluster-ai/klusterai-cookbook/blob/main/images/balls-image.jpeg?raw=true\"}}]}]}}\n</pre> <p>Now that we've prepared our input files, it's time to upload them to the kluster.ai platform. To do so, you can use the <code>files.create</code> endpoint of the client, where the purpose is set to <code>batch</code>. This will return the file ID, which we need to log for the next steps. We will repeat the process for each batch file created.</p> In\u00a0[9]: Copied! <pre>def upload_batch_file(data_dir):\n  print(f\"Creating request for {data_dir}\")\n  \n  with open(data_dir, 'rb') as file:\n    upload_response = client.files.create(\n    file=file,\n    purpose=\"batch\"\n  )\n\n  # Print job ID\n  file_id = upload_response.id\n  print(f\"File uploaded successfully. File ID: {file_id}\")\n\n  return upload_response\n</pre> def upload_batch_file(data_dir):   print(f\"Creating request for {data_dir}\")      with open(data_dir, 'rb') as file:     upload_response = client.files.create(     file=file,     purpose=\"batch\"   )    # Print job ID   file_id = upload_response.id   print(f\"File uploaded successfully. File ID: {file_id}\")    return upload_response In\u00a0[10]: Copied! <pre>batch_files = []\n\n# Loop through all .jsonl files in the data folder\nfor data_dir in filenames:\n    print(f\"Uploading file {data_dir}\")\n    job = upload_batch_file(data_dir)\n    batch_files.append(job)\n</pre> batch_files = []  # Loop through all .jsonl files in the data folder for data_dir in filenames:     print(f\"Uploading file {data_dir}\")     job = upload_batch_file(data_dir)     batch_files.append(job) <pre>Uploading file image_analysis/batch_job_balls-image.jpeg_request.jsonl\nCreating request for image_analysis/batch_job_balls-image.jpeg_request.jsonl\nFile uploaded successfully. File ID: 67f9367aa2152f1de1f953d4\nUploading file image_analysis/batch_job_eggs-image.jpeg_request.jsonl\nCreating request for image_analysis/batch_job_eggs-image.jpeg_request.jsonl\nFile uploaded successfully. File ID: 67f9367aa0600ae1a3a06dce\nUploading file image_analysis/batch_job_parking-image.jpeg_request.jsonl\nCreating request for image_analysis/batch_job_parking-image.jpeg_request.jsonl\nFile uploaded successfully. File ID: 67f9367a3b3246675257bd3e\nUploading file image_analysis/batch_job_text-typo-image.jpeg_request.jsonl\nCreating request for image_analysis/batch_job_text-typo-image.jpeg_request.jsonl\nFile uploaded successfully. File ID: 67f9367aa2152f1de1f953dc\n</pre> <p>All files are now uploaded, and we can proceed with creating the batch jobs.</p> <p>Once all the files have been successfully uploaded, we're ready to start (create) the batch jobs by providing the file ID of each file, which we got in the previous step. To start each job, we use the <code>batches.create</code> method, for which we need to set the endpoint to <code>/v1/chat/completions</code>. This will return each batch job details, with each ID.</p> In\u00a0[11]: Copied! <pre># Create batch job with completions endpoint\ndef create_batch_job(file_id):\n  batch_job = client.batches.create(\n    input_file_id=file_id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\"\n  )\n\n  print(f\"Batch job created with ID {batch_job.id}\")\n  return batch_job\n</pre> # Create batch job with completions endpoint def create_batch_job(file_id):   batch_job = client.batches.create(     input_file_id=file_id,     endpoint=\"/v1/chat/completions\",     completion_window=\"24h\"   )    print(f\"Batch job created with ID {batch_job.id}\")   return batch_job In\u00a0[12]: Copied! <pre>batch_jobs = []\n\n# Loop through all batch files ID and start each job\nfor batch_file in batch_files:\n    print(f\"Creating batch job for file ID {batch_file.id}\")\n    batch_job = create_batch_job(batch_file.id)\n    batch_jobs.append(batch_job)\n</pre> batch_jobs = []  # Loop through all batch files ID and start each job for batch_file in batch_files:     print(f\"Creating batch job for file ID {batch_file.id}\")     batch_job = create_batch_job(batch_file.id)     batch_jobs.append(batch_job) <pre>Creating batch job for file ID 67f9367aa2152f1de1f953d4\nBatch job created with ID 67f9367da0600ae1a3a06e08\nCreating batch job for file ID 67f9367aa0600ae1a3a06dce\nBatch job created with ID 67f9367d4a0a27e1d8b5e5a7\nCreating batch job for file ID 67f9367a3b3246675257bd3e\nBatch job created with ID 67f9367d0dc361d5cd3ecfa6\nCreating batch job for file ID 67f9367aa2152f1de1f953dc\nBatch job created with ID 67f9367ea2152f1de1f95437\n</pre> <p>All requests are queued to be processed.</p> <p>Now that your batch jobs have been created, you can track their progress.</p> <p>To monitor the job's progress, we can use the <code>batches.retrieve</code> method and pass the batch job ID. The response contains a <code>status</code> field that tells whether it is completed or not and the subsequent status of each job separately. We can repeat this process for every batch job ID we got in the previous step.</p> <p>The following snippet checks the status of all batch jobs every 10 seconds until the entire batch is completed.</p> In\u00a0[13]: Copied! <pre>def monitor_batch_jobs(batch_jobs):\n    all_completed = False\n\n    # Loop until all jobs are completed\n    while not all_completed:\n        all_completed = True\n        output_lines = []\n\n        # Loop through all batch jobs\n        for job in batch_jobs:\n            updated_job = client.batches.retrieve(job.id)\n            status = updated_job.status\n\n            # If job is completed\n            if status == \"completed\":\n                output_lines.append(\"Job completed!\")\n            # If job failed, cancelled or expired\n            elif status in [\"failed\", \"cancelled\", \"expired\"]:\n                output_lines.append(f\"Job ended with status: {status}\")\n                break\n            # If job is ongoing\n            else:\n                all_completed = False\n                completed = updated_job.request_counts.completed\n                total = updated_job.request_counts.total\n                output_lines.append(\n                    f\"Job status: {status} - Progress: {completed}/{total}\"\n                )\n\n        # Clear terminal\n        clear_output(wait=True)\n        for line in output_lines:\n            display(line)\n\n        # Check every 10 seconds\n        if not all_completed:\n            time.sleep(10)\n</pre> def monitor_batch_jobs(batch_jobs):     all_completed = False      # Loop until all jobs are completed     while not all_completed:         all_completed = True         output_lines = []          # Loop through all batch jobs         for job in batch_jobs:             updated_job = client.batches.retrieve(job.id)             status = updated_job.status              # If job is completed             if status == \"completed\":                 output_lines.append(\"Job completed!\")             # If job failed, cancelled or expired             elif status in [\"failed\", \"cancelled\", \"expired\"]:                 output_lines.append(f\"Job ended with status: {status}\")                 break             # If job is ongoing             else:                 all_completed = False                 completed = updated_job.request_counts.completed                 total = updated_job.request_counts.total                 output_lines.append(                     f\"Job status: {status} - Progress: {completed}/{total}\"                 )          # Clear terminal         clear_output(wait=True)         for line in output_lines:             display(line)          # Check every 10 seconds         if not all_completed:             time.sleep(10)  In\u00a0[14]: Copied! <pre>monitor_batch_jobs(batch_jobs)\n</pre> monitor_batch_jobs(batch_jobs) <pre>'Job completed!'</pre> <pre>'Job completed!'</pre> <pre>'Job completed!'</pre> <pre>'Job completed!'</pre> <p>With all jobs completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you must retrieve the <code>output_file_id</code> from the batch job and then use the <code>files.content</code> endpoint, providing that specific file ID. We will repeat this for every single batch job id. Note that the job status must be <code>completed</code> to retrieve the results!</p> In\u00a0[15]: Copied! <pre>#Parse results as a JSON object\ndef parse_json_objects(data_string):\n  if isinstance(data_string, bytes):\n    data_string = data_string.decode('utf-8')\n\n  json_strings = data_string.strip().split('\\n')\n  json_objects = []\n\n  for json_str in json_strings:\n    try:\n      json_obj = json.loads(json_str)\n      json_objects.append(json_obj)\n    except json.JSONDecodeError as e:\n      print(f\"Error parsing JSON: {e}\")\n\n  return json_objects\n</pre> #Parse results as a JSON object def parse_json_objects(data_string):   if isinstance(data_string, bytes):     data_string = data_string.decode('utf-8')    json_strings = data_string.strip().split('\\n')   json_objects = []    for json_str in json_strings:     try:       json_obj = json.loads(json_str)       json_objects.append(json_obj)     except json.JSONDecodeError as e:       print(f\"Error parsing JSON: {e}\")    return json_objects In\u00a0[16]: Copied! <pre># Group all responses by image name\nresults_by_image = defaultdict(list)\n\n# Go through all batch jobs, providing the output file ID\nfor batch_job in batch_jobs:\n  job_status = client.batches.retrieve(batch_job.id)\n  result_file_id = job_status.output_file_id\n  result = client.files.content(result_file_id).content\n  results = parse_json_objects(result)\n\n  # For each, print the result\n  for res in results:\n    inference_id = res['custom_id']\n      \n    # Extract image with regex (ending in .jpeg)\n    match = re.search(r'image-(.*?\\.jpeg)', inference_id)\n    image_name = match.group(1) if match else \"unknown.jpeg\"\n    \n    # Extract response      \n    model = res['response']['body']['model']\n    content  = res['response']['body']['choices'][0]['message']['content'].strip()\n\n    results_by_image[image_name].append((model, content))\n\n# Plot results for each image\nfor image_name, model_outputs in results_by_image.items():\n    image_url = base_url.replace(\"IMAGE_NAME\", image_name)\n\n    # Load image\n    response = requests.get(image_url)\n    img = mpimg.imread(BytesIO(response.content), format='jpeg')\n\n    # Build formatted model output string\n    formatted_text = \"\"\n    for model, output in model_outputs:\n        formatted_text += f\"{model}:\\n{output}\\n\\n\"\n\n    # Create side-by-side layout\n    fig, ax = plt.subplots(1, 2, figsize=(16, 8), gridspec_kw={'width_ratios': [1, 1.5]})\n    fig.subplots_adjust(left=0.02, right=0.98, top=0.92, bottom=0.05, wspace=0.2)\n\n    # --- Left: Image ---\n    ax[0].imshow(img)\n    ax[0].axis('off')\n    ax[0].set_anchor('C')  # Vertical center\n\n    # --- Right: Model output ---\n    ax[1].axis('off')\n    ax[1].set_anchor('C')  # Vertical center\n    ax[1].text(0, 0.5, formatted_text, fontsize=14, va='center', wrap=True)\n\n    plt.show()\n</pre> # Group all responses by image name results_by_image = defaultdict(list)  # Go through all batch jobs, providing the output file ID for batch_job in batch_jobs:   job_status = client.batches.retrieve(batch_job.id)   result_file_id = job_status.output_file_id   result = client.files.content(result_file_id).content   results = parse_json_objects(result)    # For each, print the result   for res in results:     inference_id = res['custom_id']            # Extract image with regex (ending in .jpeg)     match = re.search(r'image-(.*?\\.jpeg)', inference_id)     image_name = match.group(1) if match else \"unknown.jpeg\"          # Extract response           model = res['response']['body']['model']     content  = res['response']['body']['choices'][0]['message']['content'].strip()      results_by_image[image_name].append((model, content))  # Plot results for each image for image_name, model_outputs in results_by_image.items():     image_url = base_url.replace(\"IMAGE_NAME\", image_name)      # Load image     response = requests.get(image_url)     img = mpimg.imread(BytesIO(response.content), format='jpeg')      # Build formatted model output string     formatted_text = \"\"     for model, output in model_outputs:         formatted_text += f\"{model}:\\n{output}\\n\\n\"      # Create side-by-side layout     fig, ax = plt.subplots(1, 2, figsize=(16, 8), gridspec_kw={'width_ratios': [1, 1.5]})     fig.subplots_adjust(left=0.02, right=0.98, top=0.92, bottom=0.05, wspace=0.2)      # --- Left: Image ---     ax[0].imshow(img)     ax[0].axis('off')     ax[0].set_anchor('C')  # Vertical center      # --- Right: Model output ---     ax[1].axis('off')     ax[1].set_anchor('C')  # Vertical center     ax[1].text(0, 0.5, formatted_text, fontsize=14, va='center', wrap=True)      plt.show()  <p>This tutorial used the chat completion endpoint to image analysis on multiple images using kluster.ai batch API. This particular example uploaded four specific images:</p> <ul> <li>A Newton's cradle, we asked what the device's name is and how many balls were in the image (5):<ul> <li>All models responded correctly \u2705</li> </ul> </li> <li>Eggs of different colors, we asked how many total eggs and per color (10 total, 8 brown and 2 white):<ul> <li>Gemma 3 27B was able to identify all 10 eggs correctly, counting 8 brown and 2 white \u2705 (although sometimes it provided a 7/3 split between white and brown eggs, specially with a lower temperature)</li> <li>Qwen 2.5 7B was able to identify all 10 eggs correctly, counting 8 brown and 2 white \u2705 (although sometimes it provided a 9/3 split between shite and brown eggs, specially with a lower temperature)</li> <li>Llama 4 Maverick was able to identify all 10 eggs properly, counting 8 brown and 2 white \u2705</li> <li>Llama 4 Scout was able to count a total of 10 eggs \u2705, but wrongly identified 9 brown and 2 white (not adding up to 10)</li> </ul> </li> <li>Aliens only parking sign, we asked to interpret the sign (only aliens can park, funny reference):<ul> <li>All models identified the sign appropriately \u2705</li> </ul> </li> <li>Hand written note with a typo, we asked what the text in the image is and to find a typo if any (\"I LOVE PROGRAMING\", \"I love programing\" missing an \"m\" in both instances):<ul> <li>All models responded correctly \u2705 All models were also tested with a musical score by asking them to identify the sequence of notes. However, none of the models were able to correctly recognize the musical notes, so this test was not included in the final notebook.</li> </ul> </li> </ul> <p>To submit a batch job, we've:</p> <ol> <li>Created the JSONL file, where each file line represented a separate request. We provided the images as URLs from GitHub</li> <li>Submitted the file to the platform</li> <li>Started the batch job, and monitored its progress</li> <li>Once completed, we fetched the results</li> </ol> <p>All of this using the OpenAI Python library and API, no changes needed!</p> <p>Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As the next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!</p>"},{"location":"tutorials/klusterai-api/image-analysis/#image-analysis-with-the-klusterai-api","title":"Image analysis with the kluster.ai API\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#prerequisites","title":"Prerequisites\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#get-the-data","title":"Get the data\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#perform-batch-inference","title":"Perform batch inference\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#create-the-batch-input-file","title":"Create the batch input file\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#upload-inference-file-to-klusterai","title":"Upload inference file to kluster.ai\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#start-the-job","title":"Start the job\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#check-job-progress","title":"Check job progress\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#get-the-results","title":"Get the results\u00b6","text":""},{"location":"tutorials/klusterai-api/image-analysis/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/","title":"Keyword extraction","text":"<p>One simple but powerful use case for AI models is keyword extraction, in which you feed the model a large dataset and ask it to provide a given number of keywords.</p> <p>This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to obtain keywords from a given dataset.</p> <p>The example uses an extract from the AG News dataset. You can adapt this example by using the data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, to obtain keywords, all powered by a state-of-the-art language model.</p> <p>In this notebook, we'll use Python's <code>getpass</code> module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces).</p> In\u00a0[1]: Copied! <pre>from getpass import getpass\n\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass  api_key = getpass(\"Enter your kluster.ai API key: \") <p>Next, ensure you've installed OpenAI Python library:</p> In\u00a0[2]: Copied! <pre>%pip install -q openai\n</pre> %pip install -q openai <p>With the OpenAI Python library installed, we import the necessary dependencies for the tutorial:</p> In\u00a0[3]: Copied! <pre>from openai import OpenAI\n\nimport pandas as pd\nimport time\nimport json\nimport os\nfrom IPython.display import clear_output, display\n</pre> from openai import OpenAI  import pandas as pd import time import json import os from IPython.display import clear_output, display <p>And then, initialize the <code>client</code> by pointing it to the kluster.ai endpoint, and passing your API key.</p> In\u00a0[4]: Copied! <pre># Set up the client\nclient = OpenAI(\n    base_url=\"https://api.kluster.ai/v1\",\n    api_key=api_key,\n)\n</pre> # Set up the client client = OpenAI(     base_url=\"https://api.kluster.ai/v1\",     api_key=api_key, ) <p>Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can discuss the data.</p> <p>This notebook comes with a preloaded sample dataset based on the AG News dataset. It includes sections of news headlines and their leads, all set for processing. No additional setup is needed. Proceed to the next steps to begin working with this data.</p> In\u00a0[6]: Copied! <pre>df = pd.DataFrame({\n    \"text\": [\n        \"Chorus Frog Found Croaking in Virginia - The Southern chorus frog has been found in southeastern Virginia, far outside its previously known range. The animal had never before been reported north of Beaufort County, N.C., about 125 miles to the south.\",\n        \"Expedition to Probe Gulf of Mexico - Scientists will use advanced technology never before deployed beneath the sea as they try to discover new creatures, behaviors and phenomena in a 10-day expedition to the Gulf of Mexico's deepest reaches.\",\n        \"Feds Accused of Exaggerating Fire ImpactP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.\",\n        \"New Method May Predict Quakes Weeks Ahead - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.\",\n        \"Marine Expedition Finds New Species - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings #151; including what appear to be new species of fish and squid #151; could be used to protect marine ecosystems worldwide.\"\n    ]\n})\n</pre> df = pd.DataFrame({     \"text\": [         \"Chorus Frog Found Croaking in Virginia - The Southern chorus frog has been found in southeastern Virginia, far outside its previously known range. The animal had never before been reported north of Beaufort County, N.C., about 125 miles to the south.\",         \"Expedition to Probe Gulf of Mexico - Scientists will use advanced technology never before deployed beneath the sea as they try to discover new creatures, behaviors and phenomena in a 10-day expedition to the Gulf of Mexico's deepest reaches.\",         \"Feds Accused of Exaggerating Fire ImpactP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.\",         \"New Method May Predict Quakes Weeks Ahead - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.\",         \"Marine Expedition Finds New Species - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings #151; including what appear to be new species of fish and squid #151; could be used to protect marine ecosystems worldwide.\"     ] }) <p>To execute the batch inference job, we'll take the following steps:</p> <ol> <li>Create the batch job file - we'll generate a JSON lines file with the desired requests to be processed by the model</li> <li>Upload the batch job file - once it is ready, we'll upload it to the kluster.ai platform using the API, where it will be processed. We'll receive a unique ID associated with our file</li> <li>Start the batch job - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before</li> <li>Monitor job progress - (optional) track the status of the batch job to ensure it has been successfully completed</li> <li>Retrieve results - once the job has completed execution, we can access and process the resultant data</li> </ol> <p>This notebook is prepared for you to follow along. Run the cells below to watch it all come together.</p> <p>This example selects the <code>klusterai/Meta-Llama-3.1-405B-Instruct-Turbo</code> model. If you'd like to use a different model, feel free to change it by modifying the <code>model</code> field. In this notebook, you can also comment Llama 3.1 405B, and uncomment whatever model you want to try out.</p> <p>Please refer to the Supported models section for a list of the models we support.</p> <p>The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its model. Also, we are using a temperature of <code>0.5</code>, but feel free to change it and play around with the different outcomes (but we are only asking to respond with a single word, the genre).</p> In\u00a0[7]: Copied! <pre># Prompt\nSYSTEM_PROMPT = '''\n    Extract up to 5 relevant keywords from the given text. Provide only the keywords between double quotes and separated by commas.\n    '''\n\n# Models\n#model=\"deepseek-ai/DeepSeek-R1\"\n#model=\"deepseek-ai/DeepSeek-V3\"\n#model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"\nmodel=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\"\n#model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\"\n#model=\"Qwen/Qwen2.5-VL-7B-Instruct\"\n\n# Ensure the directory exists\nos.makedirs(\"keyword_extraction/\", exist_ok=True)\n\n# Create the batch job file with the prompt and content\ndef create_batch_file(df):\n    batch_list = []\n    for index, row in df.iterrows():\n        content = row['text']\n\n        request = {\n            \"custom_id\": f\"keyword_extraction-{index}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": model,\n                \"temperature\": 0.5,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": content}\n                ],\n            }\n        }\n        batch_list.append(request)\n    return batch_list\n\n# Save file\ndef save_batch_file(batch_list):\n    filename = f\"keyword_extraction/batch_job_request.jsonl\"\n    with open(filename, 'w') as file:\n        for request in batch_list:\n            file.write(json.dumps(request) + '\\n')\n    return filename\n</pre> # Prompt SYSTEM_PROMPT = '''     Extract up to 5 relevant keywords from the given text. Provide only the keywords between double quotes and separated by commas.     '''  # Models #model=\"deepseek-ai/DeepSeek-R1\" #model=\"deepseek-ai/DeepSeek-V3\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\"  # Ensure the directory exists os.makedirs(\"keyword_extraction/\", exist_ok=True)  # Create the batch job file with the prompt and content def create_batch_file(df):     batch_list = []     for index, row in df.iterrows():         content = row['text']          request = {             \"custom_id\": f\"keyword_extraction-{index}\",             \"method\": \"POST\",             \"url\": \"/v1/chat/completions\",             \"body\": {                 \"model\": model,                 \"temperature\": 0.5,                 \"messages\": [                     {\"role\": \"system\", \"content\": SYSTEM_PROMPT},                     {\"role\": \"user\", \"content\": content}                 ],             }         }         batch_list.append(request)     return batch_list  # Save file def save_batch_file(batch_list):     filename = f\"keyword_extraction/batch_job_request.jsonl\"     with open(filename, 'w') as file:         for request in batch_list:             file.write(json.dumps(request) + '\\n')     return filename <p>Let's run the functions we've defined before:</p> In\u00a0[8]: Copied! <pre>batch_list = create_batch_file(df)\nfilename = save_batch_file(batch_list)\n</pre> batch_list = create_batch_file(df) filename = save_batch_file(batch_list) <p>Next, we can preview what that batch job file looks like:</p> In\u00a0[9]: Copied! <pre>!head -n 1 keyword_extraction/batch_job_request.jsonl\n</pre> !head -n 1 keyword_extraction/batch_job_request.jsonl <pre>{\"custom_id\": \"keyword_extraction-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n    Extract up to 5 relevant keywords from the given text. Provide only the keywords between double quotes and separated by commas.\\n    \"}, {\"role\": \"user\", \"content\": \"Chorus Frog Found Croaking in Virginia - The Southern chorus frog has been found in southeastern Virginia, far outside its previously known range. The animal had never before been reported north of Beaufort County, N.C., about 125 miles to the south.\"}]}}\n</pre> <p>Now that we\u2019ve prepared our input file, it\u2019s time to upload it to the kluster.ai platform. To do so, you can use the <code>files.create</code> endpoint of the client, where the purpose is set to <code>batch</code>. This will return the file ID, which we need to log for the next steps.</p> In\u00a0[10]: Copied! <pre>data_dir = 'keyword_extraction/batch_job_request.jsonl'\n\n# Upload batch job request file\nwith open(data_dir, 'rb') as file:\n    upload_response = client.files.create(\n        file=file,\n        purpose=\"batch\"\n    )\n\n    # Print job ID\n    file_id = upload_response.id\n    print(f\"File uploaded successfully. File ID: {file_id}\")\n</pre> data_dir = 'keyword_extraction/batch_job_request.jsonl'  # Upload batch job request file with open(data_dir, 'rb') as file:     upload_response = client.files.create(         file=file,         purpose=\"batch\"     )      # Print job ID     file_id = upload_response.id     print(f\"File uploaded successfully. File ID: {file_id}\") <pre>File uploaded successfully. File ID: 67e40f05331c771bde3bca69\n</pre> <p>Once the file has been successfully uploaded, we're ready to start (create) the batch job by providing the file ID we got in the previous step. To do so, we use the <code>batches.create</code> method, for which we need to set the endpoint to <code>/v1/chat/completions</code>. This will return the batch job details with the ID.</p> In\u00a0[11]: Copied! <pre># Create batch job with completions endpoint\nbatch_job = client.batches.create(\n    input_file_id=file_id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\"\n)\n\nprint(\"\\nBatch job created:\")\nbatch_dict = batch_job.model_dump()\nprint(json.dumps(batch_dict, indent=2))\n</pre> # Create batch job with completions endpoint batch_job = client.batches.create(     input_file_id=file_id,     endpoint=\"/v1/chat/completions\",     completion_window=\"24h\" )  print(\"\\nBatch job created:\") batch_dict = batch_job.model_dump() print(json.dumps(batch_dict, indent=2)) <pre>\nBatch job created:\n{\n  \"id\": \"67e40f070997f511a77bf70b\",\n  \"completion_window\": \"24h\",\n  \"created_at\": 1742999303,\n  \"endpoint\": \"/v1/chat/completions\",\n  \"input_file_id\": \"67e40f05331c771bde3bca69\",\n  \"object\": \"batch\",\n  \"status\": \"pre_schedule\",\n  \"cancelled_at\": null,\n  \"cancelling_at\": null,\n  \"completed_at\": null,\n  \"error_file_id\": null,\n  \"errors\": [],\n  \"expired_at\": null,\n  \"expires_at\": 1743085703,\n  \"failed_at\": null,\n  \"finalizing_at\": null,\n  \"in_progress_at\": null,\n  \"metadata\": {},\n  \"output_file_id\": null,\n  \"request_counts\": {\n    \"completed\": 0,\n    \"failed\": 0,\n    \"total\": 0\n  }\n}\n</pre> <p>Now that your batch job has been created, you can track its progress.</p> <p>To monitor the job's progress, we can use the <code>batches.retrieve</code> method and pass the batch job ID. The response contains a <code>status</code> field that tells us if it is completed or not and the subsequent status of each job separately.</p> <p>The following snippet checks the status every 10 seconds until the entire batch is completed:</p> In\u00a0[\u00a0]: Copied! <pre>all_completed = False\n\n# Loop to check status every 10 seconds\nwhile not all_completed:\n    all_completed = True\n    output_lines = []\n\n    updated_job = client.batches.retrieve(batch_job.id)\n\n    if updated_job.status != \"completed\":\n        all_completed = False\n        completed = updated_job.request_counts.completed\n        total = updated_job.request_counts.total\n        output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\")\n    else:\n        output_lines.append(f\"Job completed!\")\n\n    # Clear the output and display updated status\n    clear_output(wait=True)\n    for line in output_lines:\n        display(line)\n\n    if not all_completed:\n        time.sleep(10)\n</pre> all_completed = False  # Loop to check status every 10 seconds while not all_completed:     all_completed = True     output_lines = []      updated_job = client.batches.retrieve(batch_job.id)      if updated_job.status != \"completed\":         all_completed = False         completed = updated_job.request_counts.completed         total = updated_job.request_counts.total         output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\")     else:         output_lines.append(f\"Job completed!\")      # Clear the output and display updated status     clear_output(wait=True)     for line in output_lines:         display(line)      if not all_completed:         time.sleep(10) <pre>'Job status: in_progress - Progress: 1/5'</pre> <p>With the job completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you must retrieve the <code>output_file_id</code> from the batch job and then use the <code>files.content</code> endpoint, providing that specific file ID. Note that the job status must be <code>completed</code> to retrieve the results!</p> In\u00a0[\u00a0]: Copied! <pre>#Parse results as a JSON object\ndef parse_json_objects(data_string):\n    if isinstance(data_string, bytes):\n        data_string = data_string.decode('utf-8')\n\n    json_strings = data_string.strip().split('\\n')\n    json_objects = []\n\n    for json_str in json_strings:\n        try:\n            json_obj = json.loads(json_str)\n            json_objects.append(json_obj)\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON: {e}\")\n\n    return json_objects\n\n# Retrieve results with job ID\njob = client.batches.retrieve(batch_job.id)\nresult_file_id = job.output_file_id\nresult = client.files.content(result_file_id).content\n\n# Parse JSON results\nparsed_result = parse_json_objects(result)\n\n# Extract and print only the content of each response\nprint(\"\\nExtracted Responses:\")\nfor item in parsed_result:\n    try:\n        content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n        print(content)\n    except KeyError as e:\n        print(f\"Missing key in response: {e}\")\n</pre> #Parse results as a JSON object def parse_json_objects(data_string):     if isinstance(data_string, bytes):         data_string = data_string.decode('utf-8')      json_strings = data_string.strip().split('\\n')     json_objects = []      for json_str in json_strings:         try:             json_obj = json.loads(json_str)             json_objects.append(json_obj)         except json.JSONDecodeError as e:             print(f\"Error parsing JSON: {e}\")      return json_objects  # Retrieve results with job ID job = client.batches.retrieve(batch_job.id) result_file_id = job.output_file_id result = client.files.content(result_file_id).content  # Parse JSON results parsed_result = parse_json_objects(result)  # Extract and print only the content of each response print(\"\\nExtracted Responses:\") for item in parsed_result:     try:         content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]         print(content)     except KeyError as e:         print(f\"Missing key in response: {e}\") <p>This tutorial used the chat completion endpoint to perform a simple keyword extraction task with batch inference. This particular example extracted keywords from an AG News dataset.</p> <p>To submit a batch job we've:</p> <ol> <li>Created the JSONL file, where each line of the file represented a separate request</li> <li>Submitted the file to the platform</li> <li>Started the batch job, and monitored its progress</li> <li>Once completed, we fetched the results</li> </ol> <p>All of this using the OpenAI Python library and API, no changes needed!</p> <p>Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As the next steps, feel free to create your dataset or expand on this example. Good luck!</p>"},{"location":"tutorials/klusterai-api/keyword-extraction-api/#keyword-extraction-with-klusterai-api","title":"Keyword extraction with kluster.ai API\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul>"},{"location":"tutorials/klusterai-api/keyword-extraction-api/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#get-the-data","title":"Get the data\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#perform-batch-inference","title":"Perform batch inference\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#create-the-batch-file","title":"Create the batch file\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#upload-inference-file-to-klusterai","title":"Upload inference file to kluster.ai\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#start-the-batch-job","title":"Start the batch job\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#check-job-progress","title":"Check job progress\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#get-the-results","title":"Get the results\u00b6","text":""},{"location":"tutorials/klusterai-api/keyword-extraction-api/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/klusterai-api/llm-as-a-judge/","title":"LLM as a judge","text":"<p>In our previous notebook, we explored the idea of selecting the best model to perform a classification task. We did that by calculating the accuracy of each model based on a ground truth label. In real-life applications, though, the ground truth is not always available, and to create one, we might depend on human annotation, which is time-consuming and costly.</p> <p>In this notebook, we will use the <code>Llama-3.1-8B-Instruct-Turbo</code> model to classify the genre of movies from the IMDb Top 1000 dataset based on their descriptions. To evaluate the accuracy of these predictions, we will use the <code>Llama-3.1-405B-Instruct-Turbo</code> model as a judge tasked with determining whether the base model's answers are correct. Since the dataset includes the true genres as ground truth, we can also assess how well the judge model aligns with the actual answers provided in the dataset.</p> In\u00a0[1]: Copied! <pre>from getpass import getpass\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") <pre>Enter your kluster.ai API key:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[2]: Copied! <pre>%pip install -q OpenAI\n</pre> %pip install -q OpenAI <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[3]: Copied! <pre>import os\nimport urllib.request\nimport pandas as pd\nimport numpy as np\nimport random\nimport requests\nfrom openai import OpenAI\nimport time\nimport json\nfrom IPython.display import clear_output, display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\npd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500)\n</pre> import os import urllib.request import pandas as pd import numpy as np import random import requests from openai import OpenAI import time import json from IPython.display import clear_output, display import seaborn as sns import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix, accuracy_score  pd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500) In\u00a0[4]: Copied! <pre># Set up the client\nclient = OpenAI(\n    base_url=\"https://api.kluster.ai/v1\",\n    api_key=api_key,\n)\n</pre> # Set up the client client = OpenAI(     base_url=\"https://api.kluster.ai/v1\",     api_key=api_key, ) In\u00a0[5]: Copied! <pre>def create_tasks(user_contents, system_prompt, task_type, model):\n    tasks = []\n    for index, user_content in enumerate(user_contents):\n        task = {\n            \"custom_id\": f\"{task_type}-{index}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": model,\n                \"temperature\": 0,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_content},\n                ],\n            }\n        }\n        tasks.append(task)\n    return tasks\n\ndef save_tasks(tasks, task_type):\n    filename = f\"batch_tasks_{task_type}.jsonl\"\n    with open(filename, 'w') as file:\n        for task in tasks:\n            file.write(json.dumps(task) + '\\n')\n    return filename\n</pre> def create_tasks(user_contents, system_prompt, task_type, model):     tasks = []     for index, user_content in enumerate(user_contents):         task = {             \"custom_id\": f\"{task_type}-{index}\",             \"method\": \"POST\",             \"url\": \"/v1/chat/completions\",             \"body\": {                 \"model\": model,                 \"temperature\": 0,                 \"messages\": [                     {\"role\": \"system\", \"content\": system_prompt},                     {\"role\": \"user\", \"content\": user_content},                 ],             }         }         tasks.append(task)     return tasks  def save_tasks(tasks, task_type):     filename = f\"batch_tasks_{task_type}.jsonl\"     with open(filename, 'w') as file:         for task in tasks:             file.write(json.dumps(task) + '\\n')     return filename In\u00a0[6]: Copied! <pre>def create_batch_job(file_name):\n    print(f\"Creating batch job for {file_name}\")\n    batch_file = client.files.create(\n        file=open(file_name, \"rb\"),\n        purpose=\"batch\"\n    )\n\n    batch_job = client.batches.create(\n        input_file_id=batch_file.id,\n        endpoint=\"/v1/chat/completions\",\n        completion_window=\"24h\"\n    )\n\n    return batch_job\n</pre> def create_batch_job(file_name):     print(f\"Creating batch job for {file_name}\")     batch_file = client.files.create(         file=open(file_name, \"rb\"),         purpose=\"batch\"     )      batch_job = client.batches.create(         input_file_id=batch_file.id,         endpoint=\"/v1/chat/completions\",         completion_window=\"24h\"     )      return batch_job In\u00a0[7]: Copied! <pre>def parse_json_objects(data_string):\n    if isinstance(data_string, bytes):\n        data_string = data_string.decode('utf-8')\n\n    json_strings = data_string.strip().split('\\n')\n    json_objects = []\n\n    for json_str in json_strings:\n        try:\n            json_obj = json.loads(json_str)\n            json_objects.append(json_obj)\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON: {e}\")\n\n    return json_objects\n\ndef monitor_job_status(client, job_id, task_type):\n    all_completed = False\n\n    while not all_completed:\n        all_completed = True\n        output_lines = []\n\n        updated_job = client.batches.retrieve(job_id)\n\n        if updated_job.status.lower() != \"completed\":\n            all_completed = False\n            completed = updated_job.request_counts.completed\n            total = updated_job.request_counts.total\n            output_lines.append(f\"{task_type.capitalize()} job status: {updated_job.status} - Progress: {completed}/{total}\")\n        else:\n            output_lines.append(f\"{task_type.capitalize()} job completed!\")\n\n        # Clear the output and display updated status\n        clear_output(wait=True)\n        for line in output_lines:\n            display(line)\n\n        if not all_completed:\n            time.sleep(10)\n</pre> def parse_json_objects(data_string):     if isinstance(data_string, bytes):         data_string = data_string.decode('utf-8')      json_strings = data_string.strip().split('\\n')     json_objects = []      for json_str in json_strings:         try:             json_obj = json.loads(json_str)             json_objects.append(json_obj)         except json.JSONDecodeError as e:             print(f\"Error parsing JSON: {e}\")      return json_objects  def monitor_job_status(client, job_id, task_type):     all_completed = False      while not all_completed:         all_completed = True         output_lines = []          updated_job = client.batches.retrieve(job_id)          if updated_job.status.lower() != \"completed\":             all_completed = False             completed = updated_job.request_counts.completed             total = updated_job.request_counts.total             output_lines.append(f\"{task_type.capitalize()} job status: {updated_job.status} - Progress: {completed}/{total}\")         else:             output_lines.append(f\"{task_type.capitalize()} job completed!\")          # Clear the output and display updated status         clear_output(wait=True)         for line in output_lines:             display(line)          if not all_completed:             time.sleep(10) In\u00a0[8]: Copied! <pre>def get_results(client, job_id):\n    batch_job = client.batches.retrieve(job_id)\n    result_file_id = batch_job.output_file_id\n    result = client.files.content(result_file_id).content\n    results = parse_json_objects(result)\n    answers = []\n    \n    for res in results:\n        result = res['response']['body']['choices'][0]['message']['content']\n        answers.append(result)\n    \n    return answers\n</pre> def get_results(client, job_id):     batch_job = client.batches.retrieve(job_id)     result_file_id = batch_job.output_file_id     result = client.files.content(result_file_id).content     results = parse_json_objects(result)     answers = []          for res in results:         result = res['response']['body']['choices'][0]['message']['content']         answers.append(result)          return answers <p>Now that we have covered the core general functions and workflow used for batch inference, in this guide, we\u2019ll be using the IMDb Top 1000 dataset, which contains information about top-rated movies, including their descriptions and genres. Let's download it and see what it looks like.</p> In\u00a0[9]: Copied! <pre># IMDB Top 1000 dataset:\nurl = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\"\nurllib.request.urlretrieve(url,filename='imdb_top_1000.csv')\n\n# Load and process the dataset based on URL content\ndf = pd.read_csv('imdb_top_1000.csv', usecols=['Series_Title', 'Overview', 'Genre']).tail(300)\ndf[['Series_Title','Overview']].head(3)\n</pre> # IMDB Top 1000 dataset: url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\" urllib.request.urlretrieve(url,filename='imdb_top_1000.csv')  # Load and process the dataset based on URL content df = pd.read_csv('imdb_top_1000.csv', usecols=['Series_Title', 'Overview', 'Genre']).tail(300) df[['Series_Title','Overview']].head(3) Out[9]: Series_Title Overview 700 Wait Until Dark A recently blinded woman is terrorized by a trio of thugs while they search for a heroin-stuffed doll they believe is in her apartment. 701 Guess Who's Coming to Dinner A couple's attitudes are challenged when their daughter introduces them to her African-American fianc\u001a. 702 Bonnie and Clyde Bored waitress Bonnie Parker falls in love with an ex-con named Clyde Barrow and together they start a violent crime spree through the country, stealing cars and robbing banks. <p>In this section, we will perform batch inference using the previously defined helper functions and the IMDb dataset. The goal is to classify movie genres based on their descriptions using a Large Language Model (LLM).</p> <p>We define the input prompts for the LLM, which consist of a system prompt outlining the task and user content, which includes a list of movie descriptions from our dataset.</p> In\u00a0[10]: Copied! <pre>prompt_dict = {\n    \"ASSISTANT_PROMPT\" : '''\n        You are a helpful assitant that classifies movie genres based on the movie description. Choose one of the following options: \n        Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.\n        Provide your response as a single word with the matching genre. Don't include punctuation.\n    ''',\n    \"USER_CONTENTS\" : df['Overview'].tolist()\n}\n</pre> prompt_dict = {     \"ASSISTANT_PROMPT\" : '''         You are a helpful assitant that classifies movie genres based on the movie description. Choose one of the following options:          Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.         Provide your response as a single word with the matching genre. Don't include punctuation.     ''',     \"USER_CONTENTS\" : df['Overview'].tolist() } <p>Next, we'll create and save the tasks, submit the batch inference job, and monitor its progress. Once the process is complete, the predictions will be integrated into the dataset.</p> In\u00a0[11]: Copied! <pre>task_list = create_tasks(user_contents=prompt_dict[\"USER_CONTENTS\"], \n                         system_prompt=prompt_dict[\"ASSISTANT_PROMPT\"], \n                         model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", \n                         task_type='assistant')\nfilename = save_tasks(task_list, task_type='assistant')\njob = create_batch_job(filename)\nmonitor_job_status(client=client, job_id=job.id, task_type='assistant')\ndf['predicted_genre'] = get_results(client=client, job_id=job.id)\n</pre> task_list = create_tasks(user_contents=prompt_dict[\"USER_CONTENTS\"],                           system_prompt=prompt_dict[\"ASSISTANT_PROMPT\"],                           model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",                           task_type='assistant') filename = save_tasks(task_list, task_type='assistant') job = create_batch_job(filename) monitor_job_status(client=client, job_id=job.id, task_type='assistant') df['predicted_genre'] = get_results(client=client, job_id=job.id) <pre>'Assistant job completed!'</pre> <p>This section evaluates the performance of the initial LLM predictions. We use another LLM as a judge to assess whether the predicted genres align with the movie descriptions.</p> <p>First, we define the input prompts for the LLM judge. These prompts include the movie description, a list of possible genres, and the genre predicted by the first LLM. The judge LLM evaluates the correctness of the predictions based on specific criteria.</p> In\u00a0[12]: Copied! <pre>prompt_dict = {\n    \"JUDGE_PROMPT\" : '''\n        You will be provided with a movie description, a list of possible genres, and a predicted movie genre made by another LLM. Your task is to evaluate whether the predicted genre is \u2018correct\u2019 or \u2018incorrect\u2019 based on the following steps and requirements.\n        \n        Steps to Follow:\n        1. Carefully read the movie description.\n        2. Determine your own classification of the genre for the movie. Do not rely on the LLM's answer since it may be incorrect. Do not rely on individual words to identify the genre; read the whole description to identify the genre.\n        3. Read the LLM answer (enclosed in double quotes) and evaluate if it is the correct answer by following the Evaluation Criteria mentioned below.\n        4. Provide your evaluation as 'correct' or 'incorrect'.\n        \n        Evaluation Criteria:\n        - Ensure the LLM answer (enclosed in double quotes) is one of the provided genres. If it is not listed, the evaluation should be \u2018incorrect\u2019.\n        - If the LLM answer (enclosed in double quotes) does not align with the movie description, the evaluation should be \u2018incorrect\u2019.\n        - The first letter of the LLM answer (enclosed in double quotes) must be capitalized (e.g., Drama). If it has any other capitalization, the evaluation should be \u2018incorrect\u2019.\n        - All other letters in the LLM answer (enclosed in double quotes) must be lowercase. Otherwise, the evaluation should be \u2018incorrect\u2019.\n        - If the LLM answer consists of multiple words, the evaluation should be \u2018incorrect\u2019.\n        - If the LLM answer includes punctuation, spaces, or additional characters, the evaluation should be \u2018incorrect\u2019.\n        \n        Output Rules:\n        - Provide the evaluation with no additional text, punctuation, or explanation.\n        - The output should be in lowercase.\n        \n        Final Answer Format:\n        evaluation\n        \n        Example:\n        correct\n    ''',\n    \"USER_CONTENTS\" : [f'''Movie Description: {row['Overview']}.\n        Available Genres: Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western\n        LLM answer: \"{row['predicted_genre']}\"\n        ''' for _, row in df.iterrows()\n        ]\n}\n</pre> prompt_dict = {     \"JUDGE_PROMPT\" : '''         You will be provided with a movie description, a list of possible genres, and a predicted movie genre made by another LLM. Your task is to evaluate whether the predicted genre is \u2018correct\u2019 or \u2018incorrect\u2019 based on the following steps and requirements.                  Steps to Follow:         1. Carefully read the movie description.         2. Determine your own classification of the genre for the movie. Do not rely on the LLM's answer since it may be incorrect. Do not rely on individual words to identify the genre; read the whole description to identify the genre.         3. Read the LLM answer (enclosed in double quotes) and evaluate if it is the correct answer by following the Evaluation Criteria mentioned below.         4. Provide your evaluation as 'correct' or 'incorrect'.                  Evaluation Criteria:         - Ensure the LLM answer (enclosed in double quotes) is one of the provided genres. If it is not listed, the evaluation should be \u2018incorrect\u2019.         - If the LLM answer (enclosed in double quotes) does not align with the movie description, the evaluation should be \u2018incorrect\u2019.         - The first letter of the LLM answer (enclosed in double quotes) must be capitalized (e.g., Drama). If it has any other capitalization, the evaluation should be \u2018incorrect\u2019.         - All other letters in the LLM answer (enclosed in double quotes) must be lowercase. Otherwise, the evaluation should be \u2018incorrect\u2019.         - If the LLM answer consists of multiple words, the evaluation should be \u2018incorrect\u2019.         - If the LLM answer includes punctuation, spaces, or additional characters, the evaluation should be \u2018incorrect\u2019.                  Output Rules:         - Provide the evaluation with no additional text, punctuation, or explanation.         - The output should be in lowercase.                  Final Answer Format:         evaluation                  Example:         correct     ''',     \"USER_CONTENTS\" : [f'''Movie Description: {row['Overview']}.         Available Genres: Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western         LLM answer: \"{row['predicted_genre']}\"         ''' for _, row in df.iterrows()         ] } <p>Following the same set of steps as the previous inference, we will create and save the tasks, submit the batch inference job, and monitor its progress. Once the process is complete, the predictions will also be integrated into the dataset.</p> In\u00a0[13]: Copied! <pre>task_list = create_tasks(user_contents=prompt_dict[\"USER_CONTENTS\"], \n                         system_prompt=prompt_dict[\"JUDGE_PROMPT\"], \n                         task_type='judge', \n                         model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\")\nfilename = save_tasks(task_list, task_type='judge')\njob = create_batch_job(filename)\nmonitor_job_status(client=client, job_id=job.id, task_type='judge')\ndf['judge_evaluation'] = get_results(client=client, job_id=job.id)\n</pre> task_list = create_tasks(user_contents=prompt_dict[\"USER_CONTENTS\"],                           system_prompt=prompt_dict[\"JUDGE_PROMPT\"],                           task_type='judge',                           model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\") filename = save_tasks(task_list, task_type='judge') job = create_batch_job(filename) monitor_job_status(client=client, job_id=job.id, task_type='judge') df['judge_evaluation'] = get_results(client=client, job_id=job.id) <pre>'Judge job completed!'</pre> <p>Now, we will calculate the LLM classification accuracy based on what the LLM judge considers correct or incorrect. For this purpose, we will compute the accuracy. If you are unfamiliar with accuracy metrics, please refer to our previous notebook.</p> In\u00a0[14]: Copied! <pre>print('LLM Judge-determined accuracy: ',df['judge_evaluation'].value_counts(normalize=True)['correct'])\n</pre> print('LLM Judge-determined accuracy: ',df['judge_evaluation'].value_counts(normalize=True)['correct']) <pre>LLM Judge-determined accuracy:  0.86\n</pre> <p>According to the LLM judge, the baseline model's accuracy was 82%. This demonstrates how, in situations where we lack ground truth, we can leverage a large-language model to evaluate the responses of another model. By doing so, we can establish a ground truth or an evaluation metric to assess model performance, refine prompts, or understand how well the model performs.</p> <p>This approach is particularly valuable when dealing with large datasets containing thousands of entries, where manual evaluation would be impractical. Automating this process saves significant time and reduces costs by eliminating the need for extensive human annotations. Ultimately, it provides a scalable and efficient way to gain meaningful insights into model performance.</p> <p>According to the LLM judge, the baseline model's accuracy is 82%. But how accurate is this evaluation? In this particular case, the IMDb Top 1000 dataset provides ground truth labels, allowing us to calculate the accuracy of the predicted genres directly. Let's compare and see how close the results are.</p> In\u00a0[15]: Copied! <pre>print('LLM ground truth accuracy: ',df.apply(lambda row: row['predicted_genre'] in row['Genre'].split(', '), axis=1).mean())\n</pre> print('LLM ground truth accuracy: ',df.apply(lambda row: row['predicted_genre'] in row['Genre'].split(', '), axis=1).mean()) <pre>LLM ground truth accuracy:  0.7833333333333333\n</pre> <p>Although the ground truth accuracy is not exactly identical to the evaluation provided by the LLM judge, in situations where we lack ground truth, using an LLM as an evaluator offers a valuable way to assess how well our baseline model is performing.</p>"},{"location":"tutorials/klusterai-api/llm-as-a-judge/#evaluating-llm-performance-without-ground-truth-using-an-llm-judge","title":"Evaluating LLM performance without ground truth using an LLM judge\u00b6","text":""},{"location":"tutorials/klusterai-api/llm-as-a-judge/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul>"},{"location":"tutorials/klusterai-api/llm-as-a-judge/#setup","title":"Setup\u00b6","text":"<p>In this notebook, we'll use Python's <code>getpass</code> module to safely input the key. Provide your unique kluster.ai API key (ensure there are no spaces).</p>"},{"location":"tutorials/klusterai-api/llm-as-a-judge/#build-our-evaluation-pipeline","title":"Build our evaluation pipeline\u00b6","text":"<p>In this section, we'll create several utility functions that will help us:</p> <ol> <li>Prepare our data for batch processing</li> <li>Send requests to the kluster.ai API</li> <li>Monitor the progress of our evaluation</li> <li>Collect and analyze results</li> </ol> <p>These functions will make our evaluation process more efficient and organized. Let's go through each one and understand its purpose.</p> <ol> <li><code>create_tasks()</code> - formats our data for the API</li> <li><code>save_tasks()</code> - prepares batch files for processing</li> <li><code>monitor_job_status()</code> - tracks evaluation progress</li> <li><code>get_results()</code> - collects and processes model outputs</li> </ol>"},{"location":"tutorials/klusterai-api/llm-as-a-judge/#create-and-manage-batch-files","title":"Create and manage batch files\u00b6","text":"<p>A batch file in our context is a collection of requests that we'll send to our models for evaluation. Think of it as a organized list of tasks we want our models to complete.</p> <p>We'll take the following steps to create batch files:</p> <ol> <li>Creating tasks - we'll convert each movie description into a format LLMs can process</li> <li>Organizing data -we'll add necessary metadata and instructions for each task</li> <li>Saving files - we'll store these tasks in a structured format (JSONL) for processing</li> </ol> <p>Let's break down the key components of our batch file creation:</p> <ul> <li><code>custom_id</code> - helps us track individual requests</li> <li><code>system_prompt</code> - provides instructions to the model</li> <li><code>content</code> - the actual text we want to classify</li> </ul> <p>This structured approach allows us to efficiently process multiple requests in parallel.</p>"},{"location":"tutorials/klusterai-api/llm-as-a-judge/#upload-files-to-klusterai","title":"Upload files to kluster.ai\u00b6","text":"<p>Now that we've prepared our batch files, we'll upload them to the kluster.ai platform for batch inference. This step is crucial for:</p> <ol> <li>Getting our data to the models</li> <li>Setting up the processing queue</li> <li>Preparing for inference</li> </ol> <p>Once the upload is complete, the following actions will take place:</p> <ol> <li>The platform queues our requests</li> <li>Models process them efficiently</li> <li>Results are made available for collection</li> </ol>"},{"location":"tutorials/klusterai-api/llm-as-a-judge/#check-job-progress","title":"Check job progress\u00b6","text":"<p>This function provides real-time monitoring of batch job progress:</p> <ul> <li>Continuously checks job status via the kluster.ai API</li> <li>Displays current completion count (completed/total requests)</li> <li>Updates status every 10 seconds until job is finished</li> <li>Automatically clears previous output for clean progress tracking</li> </ul>"},{"location":"tutorials/klusterai-api/llm-as-a-judge/#collect-and-process-results","title":"Collect and process results\u00b6","text":"<p>The <code>get_results()</code> function below does the following:</p> <ol> <li>Retrieves the completed batch job results</li> <li>Extracts the model's response content from each result</li> <li>Returns a list of all model responses</li> </ol>"},{"location":"tutorials/klusterai-api/llm-as-a-judge/#data-acquisition","title":"Data acquisition\u00b6","text":""},{"location":"tutorials/klusterai-api/llm-as-a-judge/#performing-batch-inference","title":"Performing batch inference\u00b6","text":""},{"location":"tutorials/klusterai-api/llm-as-a-judge/#llm-as-a-judge","title":"LLM as a judge\u00b6","text":""},{"location":"tutorials/klusterai-api/llm-as-a-judge/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/klusterai-api/llm-as-a-judge/#optional-validation-against-ground-truth","title":"(Optional) Validation against ground truth\u00b6","text":""},{"location":"tutorials/klusterai-api/model-comparison/","title":"Evaluating LLMs with labeled data","text":"<p>In this hands-on tutorial, you'll learn how to systematically evaluate Language Models (LLMs) using the kluster.ai batch API. We'll walk through a practical example of comparing different models for a real-world task.</p> <p>Choosing the right LLM for your specific use case is crucial but can be challenging. While larger models might offer better performance, they often come with higher costs. kluster.ai provides high-performing models at competitive prices, making advanced AI more accessible.</p> <p>Together, we'll create a systematic evaluation pipeline that:</p> <ol> <li>Loads and processes a public dataset (which you can later replace with your own)</li> <li>Tests three state-of-the-art Llama models on a text classification task</li> <li>Compares their accuracy using annotated data</li> <li>Helps you make an informed decision based on both performance and cost</li> </ol> <p>Let's get started with understanding how we'll measure model performance.</p> In\u00a0[1]: Copied! <pre>from getpass import getpass\n# Enter you personal kluster.ai API key (make sure in advance it has no blank spaces)\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass # Enter you personal kluster.ai API key (make sure in advance it has no blank spaces) api_key = getpass(\"Enter your kluster.ai API key: \") <pre>Enter your kluster.ai API key:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[2]: Copied! <pre>%pip install -q OpenAI\n</pre> %pip install -q OpenAI <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[3]: Copied! <pre>import urllib.request\nimport pandas as pd\nimport numpy as np\nfrom openai import OpenAI\nimport time\nimport json\nfrom IPython.display import clear_output, display\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500)\n</pre> import urllib.request import pandas as pd import numpy as np from openai import OpenAI import time import json from IPython.display import clear_output, display import matplotlib.pyplot as plt  pd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500) In\u00a0[4]: Copied! <pre># Set up the client\nclient = OpenAI(\n    base_url=\"https://api.kluster.ai/v1\",\n    api_key=api_key,\n)\n</pre> # Set up the client client = OpenAI(     base_url=\"https://api.kluster.ai/v1\",     api_key=api_key, ) In\u00a0[5]: Copied! <pre>def create_tasks(df, task_type, system_prompt, model):\n    tasks = []\n    for index, row in df.iterrows():\n        content = row['Overview']\n        \n        task = {\n            \"custom_id\": f\"{task_type}-{index}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": model,\n                \"temperature\": 0,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": content},\n                ],\n            }\n        }\n        tasks.append(task)\n    return tasks\n\ndef save_tasks(tasks, task_type):\n    filename = f\"batch_tasks_{task_type}.jsonl\"\n    with open(filename, 'w') as file:\n        for task in tasks:\n            file.write(json.dumps(task) + '\\n')\n    return filename\n</pre> def create_tasks(df, task_type, system_prompt, model):     tasks = []     for index, row in df.iterrows():         content = row['Overview']                  task = {             \"custom_id\": f\"{task_type}-{index}\",             \"method\": \"POST\",             \"url\": \"/v1/chat/completions\",             \"body\": {                 \"model\": model,                 \"temperature\": 0,                 \"messages\": [                     {\"role\": \"system\", \"content\": system_prompt},                     {\"role\": \"user\", \"content\": content},                 ],             }         }         tasks.append(task)     return tasks  def save_tasks(tasks, task_type):     filename = f\"batch_tasks_{task_type}.jsonl\"     with open(filename, 'w') as file:         for task in tasks:             file.write(json.dumps(task) + '\\n')     return filename In\u00a0[6]: Copied! <pre>def create_batch_job(file_name):\n    print(f\"Creating batch job for {file_name}\")\n    batch_file = client.files.create(\n        file=open(file_name, \"rb\"),\n        purpose=\"batch\"\n    )\n\n    batch_job = client.batches.create(\n        input_file_id=batch_file.id,\n        endpoint=\"/v1/chat/completions\",\n        completion_window=\"24h\"\n    )\n\n    return batch_job\n</pre> def create_batch_job(file_name):     print(f\"Creating batch job for {file_name}\")     batch_file = client.files.create(         file=open(file_name, \"rb\"),         purpose=\"batch\"     )      batch_job = client.batches.create(         input_file_id=batch_file.id,         endpoint=\"/v1/chat/completions\",         completion_window=\"24h\"     )      return batch_job <p>This function provides real-time monitoring of batch job progress:</p> <ul> <li>Continuously checks job status via the kluster.ai API</li> <li>Displays current completion count (completed/total requests)</li> <li>Updates status every 10 seconds until job is finished</li> <li>Automatically clears previous output for clean progress tracking</li> </ul> In\u00a0[7]: Copied! <pre>def parse_json_objects(data_string):\n    if isinstance(data_string, bytes):\n        data_string = data_string.decode('utf-8')\n\n    json_strings = data_string.strip().split('\\n')\n    json_objects = []\n\n    for json_str in json_strings:\n        try:\n            json_obj = json.loads(json_str)\n            json_objects.append(json_obj)\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON: {e}\")\n\n    return json_objects\n\ndef monitor_job_status(client, job_id, task_type):\n    all_completed = False\n\n    while not all_completed:\n        all_completed = True\n        output_lines = []\n\n        updated_job = client.batches.retrieve(job_id)\n\n        if updated_job.status.lower() != \"completed\":\n            all_completed = False\n            completed = updated_job.request_counts.completed\n            total = updated_job.request_counts.total\n            output_lines.append(f\"{task_type.capitalize()} job status: {updated_job.status} - Progress: {completed}/{total}\")\n        else:\n            output_lines.append(f\"{task_type.capitalize()} job completed!\")\n\n        # Clear the output and display updated status\n        clear_output(wait=True)\n        for line in output_lines:\n            display(line)\n\n        if not all_completed:\n            time.sleep(10)\n</pre> def parse_json_objects(data_string):     if isinstance(data_string, bytes):         data_string = data_string.decode('utf-8')      json_strings = data_string.strip().split('\\n')     json_objects = []      for json_str in json_strings:         try:             json_obj = json.loads(json_str)             json_objects.append(json_obj)         except json.JSONDecodeError as e:             print(f\"Error parsing JSON: {e}\")      return json_objects  def monitor_job_status(client, job_id, task_type):     all_completed = False      while not all_completed:         all_completed = True         output_lines = []          updated_job = client.batches.retrieve(job_id)          if updated_job.status.lower() != \"completed\":             all_completed = False             completed = updated_job.request_counts.completed             total = updated_job.request_counts.total             output_lines.append(f\"{task_type.capitalize()} job status: {updated_job.status} - Progress: {completed}/{total}\")         else:             output_lines.append(f\"{task_type.capitalize()} job completed!\")          # Clear the output and display updated status         clear_output(wait=True)         for line in output_lines:             display(line)          if not all_completed:             time.sleep(10) In\u00a0[8]: Copied! <pre>def get_results(client, job_id):\n    batch_job = client.batches.retrieve(job_id)\n    result_file_id = batch_job.output_file_id\n    result = client.files.content(result_file_id).content\n    results = parse_json_objects(result)\n    answers = []\n    \n    for res in results:\n        result = res['response']['body']['choices'][0]['message']['content']\n        answers.append(result)\n    \n    return answers\n</pre> def get_results(client, job_id):     batch_job = client.batches.retrieve(job_id)     result_file_id = batch_job.output_file_id     result = client.files.content(result_file_id).content     results = parse_json_objects(result)     answers = []          for res in results:         result = res['response']['body']['choices'][0]['message']['content']         answers.append(result)          return answers <p>Now that we have covered the core general functions and workflow used for batch inference, in this guide, we'll use the IMDb Top 1000 dataset. This dataset contains information about top-rated movies, including their descriptions and genres. Let's download it and see what it looks like.</p> In\u00a0[9]: Copied! <pre># IMDB Top 1000 dataset:\nurl = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\"\nurllib.request.urlretrieve(url,filename='imdb_top_1000.csv')\n\n# Load and process the dataset based on URL content\ndf = pd.read_csv('imdb_top_1000.csv', usecols=['Series_Title', 'Overview', 'Genre'])\ndf.head(3)\n</pre> # IMDB Top 1000 dataset: url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\" urllib.request.urlretrieve(url,filename='imdb_top_1000.csv')  # Load and process the dataset based on URL content df = pd.read_csv('imdb_top_1000.csv', usecols=['Series_Title', 'Overview', 'Genre']) df.head(3) Out[9]: Series_Title Genre Overview 0 The Shawshank Redemption Drama Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency. 1 The Godfather Crime, Drama An organized crime dynasty's aging patriarch transfers control of his clandestine empire to his reluctant son. 2 The Dark Knight Action, Crime, Drama When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice. <p>As you may notice, each movie already has an annotated label, and in some cases, there may be more than one label for each movie. We will ask the LLM to identify just one genre for this notebook. We will consider the prediction correct if the predicted genre matches at least one of the genres listed in the dataset\u2019s genre column (our ground truth). Using ground truth annotated data, we can calculate the accuracy and measure how well the LLM performed.</p> <p>With LLMs, it is really important to write a good prompt, including the system prompt. Below, you can see our example instructions for the LLM. You should experiment with this and see how it changes performance!</p> In\u00a0[10]: Copied! <pre>SYSTEM_PROMPT = '''\n    You are a helpful assitant that classifies movie genres based on the movie description. Choose one of the following options: \n    Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.\n    Provide your response as a single word with the matching genre. Don't include punctuation.\n    '''\n</pre> SYSTEM_PROMPT = '''     You are a helpful assitant that classifies movie genres based on the movie description. Choose one of the following options:      Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.     Provide your response as a single word with the matching genre. Don't include punctuation.     ''' <p>Now that the prompt is defined, it\u2019s time to execute the code and run the classification task for each model. In this step, we loop through the list of models, creating the requests and batch jobs, monitoring progress, and retrieving the results.</p> In\u00a0[11]: Copied! <pre># Define models\nmodels = {\n        '8B':\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n        '70B':\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\",\n        '405B':\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\",\n        }\n\n# Process each model: create tasks, run jobs, and get results\nfor name, model in models.items():\n    task_list = create_tasks(df, task_type='assistant', system_prompt=SYSTEM_PROMPT, model=model)\n    filename = save_tasks(task_list, task_type='assistant')\n    job = create_batch_job(filename)\n    monitor_job_status(client=client, job_id=job.id, task_type=f'{name} model')\n    df[f'{name}_genre'] = get_results(client=client, job_id=job.id)\n</pre> # Define models models = {         '8B':\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",         '70B':\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\",         '405B':\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\",         }  # Process each model: create tasks, run jobs, and get results for name, model in models.items():     task_list = create_tasks(df, task_type='assistant', system_prompt=SYSTEM_PROMPT, model=model)     filename = save_tasks(task_list, task_type='assistant')     job = create_batch_job(filename)     monitor_job_status(client=client, job_id=job.id, task_type=f'{name} model')     df[f'{name}_genre'] = get_results(client=client, job_id=job.id) <pre>'405b model job completed!'</pre> In\u00a0[12]: Copied! <pre># Calculate accuracy for each model\naccuracies = {}\nfor name, _ in models.items():\n    accuracy = df.apply(lambda row: row[f'{name}_genre'] in row['Genre'].split(', '), axis=1).mean()\n    accuracies[name] = accuracy\n\n# Create the bar plot\nfig, ax = plt.subplots()\nbars = ax.bar(accuracies.keys(), accuracies.values(), edgecolor='black')\nax.bar_label(bars, label_type='center', color='white', fmt=\"%.3f\")\nax.set_ylim(0, max(accuracies.values())+ 0.01)\nax.set_xlabel('Model')\nax.set_ylabel('Accuracy')\n\nax.set_title('Classification accuracy by model')\nplt.show()\n</pre> # Calculate accuracy for each model accuracies = {} for name, _ in models.items():     accuracy = df.apply(lambda row: row[f'{name}_genre'] in row['Genre'].split(', '), axis=1).mean()     accuracies[name] = accuracy  # Create the bar plot fig, ax = plt.subplots() bars = ax.bar(accuracies.keys(), accuracies.values(), edgecolor='black') ax.bar_label(bars, label_type='center', color='white', fmt=\"%.3f\") ax.set_ylim(0, max(accuracies.values())+ 0.01) ax.set_xlabel('Model') ax.set_ylabel('Accuracy')  ax.set_title('Classification accuracy by model') plt.show()"},{"location":"tutorials/klusterai-api/model-comparison/#evaluating-llms-with-labeled-data","title":"Evaluating LLMs with labeled data\u00b6","text":""},{"location":"tutorials/klusterai-api/model-comparison/#understanding-accuracy-in-model-evaluation","title":"Understanding accuracy in model evaluation\u00b6","text":"<p>Before comparing models, let's understand our main evaluation metric: accuracy. In machine learning, accuracy is one of the most intuitive performance metrics.</p> <p>Accuracy is calculated by taking the number of correct predictions and dividing it by the total number of predictions. For example, if a model correctly classifies 85 out of 100 movie genres, its accuracy would be 85%.</p> <p>$$ \\text{Accuracy} = \\frac{\\text{Number of Correct Classifications}}{\\text{Total Number of Classifications}} $$</p> <p>We're choosing accuracy for this tutorial because:</p> <ol> <li>It's easy to understand and interpret</li> <li>It directly answers the question: \"How often is our model correct?\"</li> </ol> <p>In the next section, we'll see how to implement this metric in our evaluation pipeline.</p>"},{"location":"tutorials/klusterai-api/model-comparison/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul>"},{"location":"tutorials/klusterai-api/model-comparison/#setup","title":"Setup\u00b6","text":"<p>In this notebook, we'll use Python's <code>getpass</code> module to safely input the key. Provide your unique kluster.ai API key (ensure there are no spaces).</p>"},{"location":"tutorials/klusterai-api/model-comparison/#build-our-evaluation-pipeline","title":"Build our evaluation pipeline\u00b6","text":"<p>In this section, we'll create several utility functions that will help us:</p> <ol> <li>Prepare our data for batch processing</li> <li>Send requests to the kluster.ai API</li> <li>Monitor the progress of our evaluation</li> <li>Collect and analyze results</li> </ol> <p>These functions will make our evaluation process more efficient and organized. Let's go through each one and understand its purpose:</p> <ol> <li><code>create_tasks()</code> - formats our data for the API</li> <li><code>save_tasks()</code> - prepares batch files for processing</li> <li><code>monitor_job_status()</code> - tracks evaluation progress</li> <li><code>get_results()</code> - collects and processes model outputs</li> </ol>"},{"location":"tutorials/klusterai-api/model-comparison/#create-and-manage-batch-files","title":"Create and manage batch files\u00b6","text":"<p>A batch file in our context is a collection of requests that we'll send to our models for evaluation. Think of it as a organized list of tasks we want our models to complete.</p> <p>We'll take the following steps to create batch files:</p> <ol> <li>Creating tasks - we'll convert each movie description into a format LLMs can process</li> <li>Organizing data -we'll add necessary metadata and instructions for each task</li> <li>Saving files - we'll store these tasks in a structured format (JSONL) for processing</li> </ol> <p>Let's break down the key components of our batch file creation:</p> <ul> <li><code>custom_id</code> - helps us track individual requests</li> <li><code>system_prompt</code> - provides instructions to the model</li> <li><code>content</code> - the actual text we want to classify</li> </ul> <p>This structured approach allows us to efficiently process multiple requests in parallel.</p>"},{"location":"tutorials/klusterai-api/model-comparison/#upload-files-to-klusterai","title":"Upload files to kluster.ai\u00b6","text":"<p>Now that we've prepared our batch files, we'll upload them to the kluster.ai platform for batch inference. This step is crucial for:</p> <ol> <li>Getting our data to the models</li> <li>Setting up the processing queue</li> <li>Preparing for inference</li> </ol> <p>Once the upload is complete, the following actions will take place:</p> <ol> <li>The platform queues our requests</li> <li>Models process them efficiently</li> <li>Results are made available for collection</li> </ol>"},{"location":"tutorials/klusterai-api/model-comparison/#check-job-progress","title":"Check job progress\u00b6","text":""},{"location":"tutorials/klusterai-api/model-comparison/#collect-and-process-results","title":"Collect and process results\u00b6","text":"<p>The <code>get_results()</code> function below does the following:</p> <ol> <li>Retrieves the completed batch job results</li> <li>Extracts the model's response content from each result</li> <li>Returns a list of all model responses</li> </ol>"},{"location":"tutorials/klusterai-api/model-comparison/#prepare-a-real-dataset-for-batch-inference","title":"Prepare a real dataset for batch inference\u00b6","text":""},{"location":"tutorials/klusterai-api/model-comparison/#perform-batch-inference","title":"Perform batch inference\u00b6","text":""},{"location":"tutorials/klusterai-api/model-comparison/#analyze-the-results","title":"Analyze the results\u00b6","text":"<p>Now that we've evaluated our models let's analyze their performance. The graph below shows the accuracy scores for each model we tested. Here's what we can observe:</p> <ol> <li>Performance comparison<ul> <li>The 70B and 405B models achieved similar accuracy levels</li> <li>Both outperformed the 8B model significantly</li> </ul> </li> <li>Cost-benefit analysis<ul> <li>Given the similar performance of the 70B and 405B models</li> <li>Considering the lower cost of the 70B model</li> <li>The 70B model emerges as the most cost-effective choice</li> </ul> </li> </ol> <p>We recommend using the 70B model for this specific task based on our evaluation. It offers strong performance comparable to the larger model, better cost efficiency, and a good balance of accuracy and resource usage.</p> <p>This demonstrates how systematic evaluation can help make data-driven decisions in model selection.</p>"},{"location":"tutorials/klusterai-api/model-comparison/#conclusion","title":"Conclusion\u00b6","text":"<p>In this tutorial, we've covered the following key concepts:</p> <ul> <li>Model evaluation process - how to systematically compare LLM performance, using accuracy as a key metric and implementing batch inference for efficient evaluation</li> <li>Cost-performance balance - larger models aren\u2019t always significantly better; the importance of considering cost-effectiveness and making data-driven model selections</li> <li>Practical implementations - using the kluster.ai batch API effectively, processing large datasets efficiently, and making informed decisions based on results</li> </ul> <p>With this knowledge, you are now equipped to:</p> <ul> <li>Apply to your use case - adapt this approach to your specific needs, use your own labeled datasets, and customize evaluation metrics as needed</li> <li>Optimize further - experiment with different prompts, try other model configurations and explore additional evaluation metrics</li> <li>Scale your solution - implement in production environments, monitor performance over time, and adjust based on real-world feedback</li> </ul> <p>Remember: The goal is finding the right balance between your application's performance and cost.</p>"},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/","title":"Multiple batch predictions","text":"<p>In other notebooks, we used AI models to perform simple tasks like text classification, sentiment analysis and keyword extraction.</p> <p>This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to combine different tasks into a single batch file. Note that each task in the JSONL file can have its own model, system prompt, and particular request.</p> <p>You can adapt this example by using your data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, and obtain categorized results powered by a state-of-the-art language model.</p> <p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul> <p>In this notebook, we'll use Python's <code>getpass</code> module to safely input the key. After execution, please provide your unique kluster.ai API key (ensure no spaces).</p> In\u00a0[1]: Copied! <pre>from getpass import getpass\n\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass  api_key = getpass(\"Enter your kluster.ai API key: \") <pre>Enter your kluster.ai API key:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p>Next, ensure you've installed OpenAI Python library:</p> In\u00a0[\u00a0]: Copied! <pre>%pip install -q openai\n</pre> %pip install -q openai <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> <p>With the OpenAI Python library installed, we import the necessary dependencies for the tutorial:</p> In\u00a0[3]: Copied! <pre>from openai import OpenAI\n\nimport pandas as pd\nimport time\nimport json\nimport os\nimport urllib.request\nimport requests\nfrom IPython.display import clear_output, display\n\npd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500)\n</pre> from openai import OpenAI  import pandas as pd import time import json import os import urllib.request import requests from IPython.display import clear_output, display  pd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500) <p>And then, initialize the <code>client</code> by pointing it to the kluster.ai endpoint, and passing your API key.</p> In\u00a0[4]: Copied! <pre># Set up the client\nclient = OpenAI(\n    base_url=\"https://api.kluster.ai/v1\",\n    api_key=api_key,\n)\n</pre> # Set up the client client = OpenAI(     base_url=\"https://api.kluster.ai/v1\",     api_key=api_key, ) <p>Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can discuss the data.</p> <p>This notebook includes three sample datasets: Amazon musical instruments reviews, Top 1000 IMDb Movies, and AG News sample.</p> <p>The following code fetches the data and the last 5 data points of a single data sample. Feel free to change this or bring your own dataset.</p> In\u00a0[5]: Copied! <pre># Datasets\n#1. Amazon musical instruments reviews sample dataset\n#url = \"https://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz\"\n#2. IMDB top 1000 sample dataset\n#url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\" \n#3. AG News sample dataset\nurl = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/ag_news.csv\"\n</pre> # Datasets #1. Amazon musical instruments reviews sample dataset #url = \"https://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz\" #2. IMDB top 1000 sample dataset #url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\"  #3. AG News sample dataset url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/ag_news.csv\" In\u00a0[6]: Copied! <pre>def fetch_dataset(url, file_path=None):\n  \n  # Set the default file path based on the URL if none is provided\n  if not file_path:\n    file_path = os.path.join(\"data\", os.path.basename(url))\n\n  # Create the directory if it does not exist\n  os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n  # Download the file if it doesn't already exist\n  if not os.path.exists(file_path):\n    urllib.request.urlretrieve(url, file_path)\n    print(f\"Dataset downloaded and saved as {file_path}\")\n  else:\n    print(f\"Using cached file at {file_path}\")\n\n  # Load and process the dataset based on URL content\n  if \"imdb_top_1000.csv\" in url:\n    df = pd.read_csv(file_path)\n    df['text'] = df['Series_Title'].astype(str) + \": \" + df['Overview'].astype(str)\n    df = df[['text']]\n  elif \"ag_news\" in url:\n    df = pd.read_csv(file_path, header=None, names=[\"label\", \"title\", \"description\"])\n    df['text'] = df['title'].astype(str) + \": \" + df['description'].astype(str)\n    df = df[['text']]\n  elif \"reviews_Musical_Instruments_5.json.gz\" in url:\n    df = pd.read_json(file_path, compression='gzip', lines=True)\n    df.rename(columns={'reviewText': 'text'}, inplace=True)\n    df = df[['text']]\n  else:\n    raise ValueError(\"URL does not match any known dataset format.\")\n\n  return df[['text']].tail(3).reset_index(drop=True) # Return last 3 entries resetting the index\n\n# Fetch dataset\ndf = fetch_dataset(url=url, file_path=None)\ndf.head()\n</pre> def fetch_dataset(url, file_path=None):      # Set the default file path based on the URL if none is provided   if not file_path:     file_path = os.path.join(\"data\", os.path.basename(url))    # Create the directory if it does not exist   os.makedirs(os.path.dirname(file_path), exist_ok=True)    # Download the file if it doesn't already exist   if not os.path.exists(file_path):     urllib.request.urlretrieve(url, file_path)     print(f\"Dataset downloaded and saved as {file_path}\")   else:     print(f\"Using cached file at {file_path}\")    # Load and process the dataset based on URL content   if \"imdb_top_1000.csv\" in url:     df = pd.read_csv(file_path)     df['text'] = df['Series_Title'].astype(str) + \": \" + df['Overview'].astype(str)     df = df[['text']]   elif \"ag_news\" in url:     df = pd.read_csv(file_path, header=None, names=[\"label\", \"title\", \"description\"])     df['text'] = df['title'].astype(str) + \": \" + df['description'].astype(str)     df = df[['text']]   elif \"reviews_Musical_Instruments_5.json.gz\" in url:     df = pd.read_json(file_path, compression='gzip', lines=True)     df.rename(columns={'reviewText': 'text'}, inplace=True)     df = df[['text']]   else:     raise ValueError(\"URL does not match any known dataset format.\")    return df[['text']].tail(3).reset_index(drop=True) # Return last 3 entries resetting the index  # Fetch dataset df = fetch_dataset(url=url, file_path=None) df.head() <pre>Dataset downloaded and saved as data/ag_news.csv\n</pre> Out[6]: text 0 Feds Accused of Exaggerating Fire Impact (AP): AP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.: nan 1 New Method May Predict Quakes Weeks Ahead (AP): AP - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.: nan 2 Marine Expedition Finds New Species (AP): AP - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings  #151; including what appear to be new species of fish and squid  #151; could be used to protect marine ecosystems worldwide.: nan <p>Now that we've fetched and saved the dataset let's move to the batch inference flow.</p> <p>For this particular tutorial, we predefined five requests for the model to execute based on common customer use cases:</p> <ul> <li>Sentiment analysis - reviewing text to determine whether there is positive, neutral, or negative notation to the statement</li> <li>Translation - translate the text to any other language, in this example, Spanish</li> <li>Summarization - express the text in a concise form</li> <li>Topic classification - classify the text between a given set of categories</li> <li>Keyword extraction - provide a number of keywords</li> </ul> <p>Requests are defined as a system prompt. This example runs through different types of requests, so they are defined as JSON objects. For each use case, we also defined the structure of the response we expect from the model.</p> <p>If you\u2019re happy with these requests and structure, you can simply run the code as-is. However, if you\u2019d like to customize them, please modify the prompts (or add new ones) to make personal requests.</p> In\u00a0[7]: Copied! <pre>SYSTEM_PROMPTS = {\n  'sentiment': '''\n  Analyze the sentiment of the given text. Provide only a JSON object with the following structure:\n  {\n    \"sentiment\": string, // \"positive\", \"negative\", or \"neutral\"\n    \"confidence\": float, // A value between 0 and 1 indicating your confidence in the sentiment analysis\n  }\n  ''',\n\n  'translation': '''\n  Translate the given text from English to Spanish, paraphrase, rewrite or perform cultural adaptations for the text to make sense in Spanish. Provide only a JSON object with the following structure:\n  {\n    \"translation\": string, // The Spanish translation\n    \"notes\": string // Any notes about the translation, such as cultural adaptations or challenging phrases (max 500 words). Write this mainly in English.\n  }\n  ''',\n\n  'summary': '''\n  Summarize the main points of the given text. Provide only a JSON object with the following structure:\n  {\n    \"summary\": string, // A concise summary of the text (max 100 words)\n  }\n  ''',\n\n  'topic_classification': '''\n  Classify the main topic of the given text based on the following categories: \"politics\", \"sports\", \"technology\", \"science\", \"business\", \"entertainment\", \"health\", \"other\". Provide only a JSON object with the following structure:\n  {\n    \"category\": string, // The primary category of the provided text\n    \"confidence\": float, // A value between 0 and 1 indicating confidence in the classification\n  }\n  ''',\n\n  'keyword_extraction': '''\n  Extract relevant keywords from the given text. Provide only a JSON object with the following structure:\n  {\n    \"keywords\": string[], // An array of up to 5 keywords that best represent the text content\n    \"context\": string // Briefly explain how each keyword is relevant to the text (max 200 words)\n  }\n  '''\n}\n</pre> SYSTEM_PROMPTS = {   'sentiment': '''   Analyze the sentiment of the given text. Provide only a JSON object with the following structure:   {     \"sentiment\": string, // \"positive\", \"negative\", or \"neutral\"     \"confidence\": float, // A value between 0 and 1 indicating your confidence in the sentiment analysis   }   ''',    'translation': '''   Translate the given text from English to Spanish, paraphrase, rewrite or perform cultural adaptations for the text to make sense in Spanish. Provide only a JSON object with the following structure:   {     \"translation\": string, // The Spanish translation     \"notes\": string // Any notes about the translation, such as cultural adaptations or challenging phrases (max 500 words). Write this mainly in English.   }   ''',    'summary': '''   Summarize the main points of the given text. Provide only a JSON object with the following structure:   {     \"summary\": string, // A concise summary of the text (max 100 words)   }   ''',    'topic_classification': '''   Classify the main topic of the given text based on the following categories: \"politics\", \"sports\", \"technology\", \"science\", \"business\", \"entertainment\", \"health\", \"other\". Provide only a JSON object with the following structure:   {     \"category\": string, // The primary category of the provided text     \"confidence\": float, // A value between 0 and 1 indicating confidence in the classification   }   ''',    'keyword_extraction': '''   Extract relevant keywords from the given text. Provide only a JSON object with the following structure:   {     \"keywords\": string[], // An array of up to 5 keywords that best represent the text content     \"context\": string // Briefly explain how each keyword is relevant to the text (max 200 words)   }   ''' } <p>This example uses the <code>deepseek-ai/DeepSeek-V3</code> model. If you'd like to use a different model, feel free to change it by modifying the <code>model</code> field. In this notebook, you can also comment DeepSeek V3, and uncomment whatever model you want to try out.</p> <p>Please refer to the Supported models section for a list of the models we support.</p> <p>The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its own model. Also, we are using a temperature of <code>0.5</code> but feel free to change it and play around with the different outcomes.</p> In\u00a0[8]: Copied! <pre># Models\n# model=\"deepseek-ai/DeepSeek-R1\"\nmodel = \"deepseek-ai/DeepSeek-V3\"\n# model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"\n# model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\"\n# model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\"\n# model=\"Qwen/Qwen2.5-VL-7B-Instruct\"\n\n\ndef create_batch_file(df, inference_type, system_prompt):\n    batch_list = []\n    for index, row in df.iterrows():\n        content = row[\"text\"]\n\n        # Build the request for a given model, prompt, and data\n        request = {\n            \"custom_id\": f\"{inference_type}-{index}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": model,\n                \"temperature\": 0.5,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": content},\n                ],\n            },\n        }\n        batch_list.append(request)\n    return batch_list\n\n# Save file as JSON lines\ndef save_batch_file(batch_list, inference_type):\n    filename = f\"data/batch_request_{inference_type}.jsonl\"\n    with open(filename, \"w\") as file:\n        for request in batch_list:\n            file.write(json.dumps(request) + \"\\n\")\n    return filename\n</pre> # Models # model=\"deepseek-ai/DeepSeek-R1\" model = \"deepseek-ai/DeepSeek-V3\" # model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" # model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\" # model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" # model=\"Qwen/Qwen2.5-VL-7B-Instruct\"   def create_batch_file(df, inference_type, system_prompt):     batch_list = []     for index, row in df.iterrows():         content = row[\"text\"]          # Build the request for a given model, prompt, and data         request = {             \"custom_id\": f\"{inference_type}-{index}\",             \"method\": \"POST\",             \"url\": \"/v1/chat/completions\",             \"body\": {                 \"model\": model,                 \"temperature\": 0.5,                 \"messages\": [                     {\"role\": \"system\", \"content\": system_prompt},                     {\"role\": \"user\", \"content\": content},                 ],             },         }         batch_list.append(request)     return batch_list  # Save file as JSON lines def save_batch_file(batch_list, inference_type):     filename = f\"data/batch_request_{inference_type}.jsonl\"     with open(filename, \"w\") as file:         for request in batch_list:             file.write(json.dumps(request) + \"\\n\")     return filename  In\u00a0[9]: Copied! <pre>batch_requests = []\nfilenames = []\n\n# Loop through all the different prompts\nfor inference_type, system_prompt in SYSTEM_PROMPTS.items():\n    batch_list = create_batch_file(df, inference_type, system_prompt)\n    filename = save_batch_file(batch_list, inference_type)\n    batch_requests.append((inference_type, filename))\n    filenames.append(filename)\n    print(filename)\n</pre> batch_requests = [] filenames = []  # Loop through all the different prompts for inference_type, system_prompt in SYSTEM_PROMPTS.items():     batch_list = create_batch_file(df, inference_type, system_prompt)     filename = save_batch_file(batch_list, inference_type)     batch_requests.append((inference_type, filename))     filenames.append(filename)     print(filename) <pre>data/batch_request_sentiment.jsonl\ndata/batch_request_translation.jsonl\ndata/batch_request_summary.jsonl\ndata/batch_request_topic_classification.jsonl\ndata/batch_request_keyword_extraction.jsonl\n</pre> <p>Next, we can preview what a single batch job looks like:</p> In\u00a0[10]: Copied! <pre>!head -n 5 data/batch_request_sentiment.jsonl\n</pre> !head -n 5 data/batch_request_sentiment.jsonl <pre>{\"custom_id\": \"sentiment-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n  Analyze the sentiment of the given text. Provide only a JSON object with the following structure:\\n  {\\n    \\\"sentiment\\\": string, // \\\"positive\\\", \\\"negative\\\", or \\\"neutral\\\"\\n    \\\"confidence\\\": float, // A value between 0 and 1 indicating your confidence in the sentiment analysis\\n  }\\n  \"}, {\"role\": \"user\", \"content\": \"Feds Accused of Exaggerating Fire Impact (AP): AP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.: nan\"}]}}\n{\"custom_id\": \"sentiment-1\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n  Analyze the sentiment of the given text. Provide only a JSON object with the following structure:\\n  {\\n    \\\"sentiment\\\": string, // \\\"positive\\\", \\\"negative\\\", or \\\"neutral\\\"\\n    \\\"confidence\\\": float, // A value between 0 and 1 indicating your confidence in the sentiment analysis\\n  }\\n  \"}, {\"role\": \"user\", \"content\": \"New Method May Predict Quakes Weeks Ahead (AP): AP - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.: nan\"}]}}\n{\"custom_id\": \"sentiment-2\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n  Analyze the sentiment of the given text. Provide only a JSON object with the following structure:\\n  {\\n    \\\"sentiment\\\": string, // \\\"positive\\\", \\\"negative\\\", or \\\"neutral\\\"\\n    \\\"confidence\\\": float, // A value between 0 and 1 indicating your confidence in the sentiment analysis\\n  }\\n  \"}, {\"role\": \"user\", \"content\": \"Marine Expedition Finds New Species (AP): AP - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings  #151; including what appear to be new species of fish and squid  #151; could be used to protect marine ecosystems worldwide.: nan\"}]}}\n</pre> <p>Now that we've prepared our input files, it's time to upload it to the kluster.ai platform. To do so, you can use the <code>files.create</code> endpoint of the client, where the purpose is set to <code>batch</code>. This will return the file ID, which we need to log for the next steps. We will repeat the process for each batch file created.</p> In\u00a0[11]: Copied! <pre>def upload_batch_file(data_dir):\n  print(f\"Creating request for {data_dir}\")\n  \n  with open(data_dir, 'rb') as file:\n    upload_response = client.files.create(\n    file=file,\n    purpose=\"batch\"\n  )\n\n  # Print job ID\n  file_id = upload_response.id\n  print(f\"File uploaded successfully. File ID: {file_id}\")\n\n  return upload_response\n</pre> def upload_batch_file(data_dir):   print(f\"Creating request for {data_dir}\")      with open(data_dir, 'rb') as file:     upload_response = client.files.create(     file=file,     purpose=\"batch\"   )    # Print job ID   file_id = upload_response.id   print(f\"File uploaded successfully. File ID: {file_id}\")    return upload_response In\u00a0[12]: Copied! <pre>batch_files = []\n\n# Loop through all .jsonl files in the data folder\nfor data_dir in filenames:\n    print(f\"Uploading file {data_dir}\")\n    job = upload_batch_file(data_dir)\n    batch_files.append(job)\n</pre> batch_files = []  # Loop through all .jsonl files in the data folder for data_dir in filenames:     print(f\"Uploading file {data_dir}\")     job = upload_batch_file(data_dir)     batch_files.append(job) <pre>Uploading file data/batch_request_sentiment.jsonl\nCreating request for data/batch_request_sentiment.jsonl\nFile uploaded successfully. File ID: 67e677a2c04383db4bd5141d\nUploading file data/batch_request_translation.jsonl\nCreating request for data/batch_request_translation.jsonl\nFile uploaded successfully. File ID: 67e677a3711c9502a75a01ad\nUploading file data/batch_request_summary.jsonl\nCreating request for data/batch_request_summary.jsonl\nFile uploaded successfully. File ID: 67e677a330398a707d4a884d\nUploading file data/batch_request_topic_classification.jsonl\nCreating request for data/batch_request_topic_classification.jsonl\nFile uploaded successfully. File ID: 67e677a3711c9502a75a01b3\nUploading file data/batch_request_keyword_extraction.jsonl\nCreating request for data/batch_request_keyword_extraction.jsonl\nFile uploaded successfully. File ID: 67e677a41f2fb6ea20485e37\n</pre> <p>All files are now uploaded, and we can proceed with creating the batch jobs.</p> <p>Once all the files have been successfully uploaded, we're ready to start (create) the batch jobs by providing the file ID of each file, which we got in the previous step. To start each job, we use the <code>batches.create</code> method, for which we need to set the endpoint to <code>/v1/chat/completions</code>. This will return each batch job details, with each ID.</p> In\u00a0[13]: Copied! <pre># Create batch job with completions endpoint\ndef create_batch_job(file_id):\n  batch_job = client.batches.create(\n    input_file_id=file_id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\"\n  )\n\n  print(f\"Batch job created with ID {batch_job.id}\")\n  return batch_job\n</pre> # Create batch job with completions endpoint def create_batch_job(file_id):   batch_job = client.batches.create(     input_file_id=file_id,     endpoint=\"/v1/chat/completions\",     completion_window=\"24h\"   )    print(f\"Batch job created with ID {batch_job.id}\")   return batch_job In\u00a0[14]: Copied! <pre>batch_jobs = []\n\n# Loop through all batch files ID and start each job\nfor batch_file in batch_files:\n    print(f\"Creating batch job for file ID {batch_file.id}\")\n    batch_job = create_batch_job(batch_file.id)\n    batch_jobs.append(batch_job)\n</pre> batch_jobs = []  # Loop through all batch files ID and start each job for batch_file in batch_files:     print(f\"Creating batch job for file ID {batch_file.id}\")     batch_job = create_batch_job(batch_file.id)     batch_jobs.append(batch_job) <pre>Creating batch job for file ID 67e677a2c04383db4bd5141d\nBatch job created with ID 67e677a88d3b27ee9af94ce3\nCreating batch job for file ID 67e677a3711c9502a75a01ad\nBatch job created with ID 67e677b230398a707d4a893d\nCreating batch job for file ID 67e677a330398a707d4a884d\nBatch job created with ID 67e677bc1f2fb6ea20486001\nCreating batch job for file ID 67e677a3711c9502a75a01b3\nBatch job created with ID 67e677c730398a707d4a8a77\nCreating batch job for file ID 67e677a41f2fb6ea20485e37\nBatch job created with ID 67e677d2711c9502a75a042e\n</pre> <p>All requests are currently being processed.</p> <p>Now that your batch jobs have been created, you can track their progress.</p> <p>To monitor the job's progress, we can use the <code>batches.retrieve</code> method and pass the batch job ID. The response contains a <code>status</code> field that tells us if it is completed or not and the subsequent status of each job separately. We can repeat this process for every batch job ID we got in the previous step.</p> <p>The following snippet checks the status of all batch jobs every 10 seconds until the entire batch is completed.</p> In\u00a0[15]: Copied! <pre>def monitor_batch_jobs(batch_jobs):\n    all_completed = False\n\n    # Loop until all jobs are completed\n    while not all_completed:\n        all_completed = True\n        output_lines = []\n\n        # Loop through all batch jobs\n        for job in batch_jobs:\n            updated_job = client.batches.retrieve(job.id)\n            status = updated_job.status\n\n            # If job is completed\n            if status == \"completed\":\n                output_lines.append(\"Job completed!\")\n            # If job failed, cancelled or expired\n            elif status in [\"failed\", \"cancelled\", \"expired\"]:\n                output_lines.append(f\"Job ended with status: {status}\")\n                break\n            # If job is ongoing\n            else:\n                all_completed = False\n                completed = updated_job.request_counts.completed\n                total = updated_job.request_counts.total\n                output_lines.append(\n                    f\"Job status: {status} - Progress: {completed}/{total}\"\n                )\n\n        # Clear terminal\n        clear_output(wait=True)\n        for line in output_lines:\n            display(line)\n\n        # Check every 10 seconds\n        if not all_completed:\n            time.sleep(10)\n</pre> def monitor_batch_jobs(batch_jobs):     all_completed = False      # Loop until all jobs are completed     while not all_completed:         all_completed = True         output_lines = []          # Loop through all batch jobs         for job in batch_jobs:             updated_job = client.batches.retrieve(job.id)             status = updated_job.status              # If job is completed             if status == \"completed\":                 output_lines.append(\"Job completed!\")             # If job failed, cancelled or expired             elif status in [\"failed\", \"cancelled\", \"expired\"]:                 output_lines.append(f\"Job ended with status: {status}\")                 break             # If job is ongoing             else:                 all_completed = False                 completed = updated_job.request_counts.completed                 total = updated_job.request_counts.total                 output_lines.append(                     f\"Job status: {status} - Progress: {completed}/{total}\"                 )          # Clear terminal         clear_output(wait=True)         for line in output_lines:             display(line)          # Check every 10 seconds         if not all_completed:             time.sleep(10)  In\u00a0[16]: Copied! <pre>monitor_batch_jobs(batch_jobs)\n</pre> monitor_batch_jobs(batch_jobs) <pre>'Job completed!'</pre> <pre>'Job completed!'</pre> <pre>'Job completed!'</pre> <pre>'Job completed!'</pre> <pre>'Job completed!'</pre> <p>With all jobs completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you need to retrieve the <code>output_file_id</code> from the batch job, and then use the <code>files.content</code> endpoint, providing that specific file ID. We will repeat this for every single batch job id. Note that the job status must be <code>completed</code> for you to retrieve the results!</p> In\u00a0[17]: Copied! <pre>#Parse results as a JSON object\ndef parse_json_objects(data_string):\n  if isinstance(data_string, bytes):\n    data_string = data_string.decode('utf-8')\n\n  json_strings = data_string.strip().split('\\n')\n  json_objects = []\n\n  for json_str in json_strings:\n    try:\n      json_obj = json.loads(json_str)\n      json_objects.append(json_obj)\n    except json.JSONDecodeError as e:\n      print(f\"Error parsing JSON: {e}\")\n\n  return json_objects\n</pre> #Parse results as a JSON object def parse_json_objects(data_string):   if isinstance(data_string, bytes):     data_string = data_string.decode('utf-8')    json_strings = data_string.strip().split('\\n')   json_objects = []    for json_str in json_strings:     try:       json_obj = json.loads(json_str)       json_objects.append(json_obj)     except json.JSONDecodeError as e:       print(f\"Error parsing JSON: {e}\")    return json_objects In\u00a0[18]: Copied! <pre># Go through all batch jobs, providing the output file ID\nfor batch_job in batch_jobs:\n  job_status = client.batches.retrieve(batch_job.id)\n  result_file_id = job_status.output_file_id\n  result = client.files.content(result_file_id).content\n  results = parse_json_objects(result)\n\n    # For each, print the result\n  for res in results:\n    inference_id = res['custom_id']\n    index = inference_id.split('-')[-1]\n    result = res['response']['body']['choices'][0]['message']['content']\n    text = df.iloc[int(index)]['text']\n    print(f'\\n -------------------------- \\n')\n    print(f\"Inference ID: {inference_id}. \\n\\nTEXT: {text}\\n\\nRESULT: {result}\")\n</pre> # Go through all batch jobs, providing the output file ID for batch_job in batch_jobs:   job_status = client.batches.retrieve(batch_job.id)   result_file_id = job_status.output_file_id   result = client.files.content(result_file_id).content   results = parse_json_objects(result)      # For each, print the result   for res in results:     inference_id = res['custom_id']     index = inference_id.split('-')[-1]     result = res['response']['body']['choices'][0]['message']['content']     text = df.iloc[int(index)]['text']     print(f'\\n -------------------------- \\n')     print(f\"Inference ID: {inference_id}. \\n\\nTEXT: {text}\\n\\nRESULT: {result}\") <pre>\n -------------------------- \n\nInference ID: sentiment-0. \n\nTEXT: Feds Accused of Exaggerating Fire Impact (AP): AP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.: nan\n\nRESULT: ```json\n{\n  \"sentiment\": \"negative\",\n  \"confidence\": 0.75\n}\n```\n\n -------------------------- \n\nInference ID: sentiment-1. \n\nTEXT: New Method May Predict Quakes Weeks Ahead (AP): AP - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.: nan\n\nRESULT: ```json\n{\n  \"sentiment\": \"neutral\",\n  \"confidence\": 0.95\n}\n```\n\n -------------------------- \n\nInference ID: sentiment-2. \n\nTEXT: Marine Expedition Finds New Species (AP): AP - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings  #151; including what appear to be new species of fish and squid  #151; could be used to protect marine ecosystems worldwide.: nan\n\nRESULT: ```json\n{\n    \"sentiment\": \"positive\",\n    \"confidence\": 0.85\n}\n```\n\n -------------------------- \n\nInference ID: translation-0. \n\nTEXT: Feds Accused of Exaggerating Fire Impact (AP): AP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.: nan\n\nRESULT: {\n  \"translation\": \"Acusan a las autoridades federales de exagerar el impacto de los incendios (AP): AP - El Servicio Forestal exager\u00f3 el efecto de los incendios forestales en los b\u00fahos manchados de California para justificar un aumento planificado de la tala en la Sierra Nevada, seg\u00fan un experto de larga trayectoria en la agencia que trabaj\u00f3 en el plan.\",\n  \"notes\": \"The translation maintains the original structure and meaning of the text. The term 'Feds' was translated as 'autoridades federales' to convey the same informal yet official tone. 'Fire impact' was translated as 'impacto de los incendios' to ensure clarity. The phrase 'California spotted owls' was translated as 'b\u00fahos manchados de California' to accurately reflect the species name. The term 'logging' was translated as 'tala,' which is the standard term used in Spanish for this context. The phrase 'longtime agency expert' was translated as 'experto de larga trayectoria en la agencia' to emphasize the person's extensive experience within the organization.\"\n}\n\n -------------------------- \n\nInference ID: translation-1. \n\nTEXT: New Method May Predict Quakes Weeks Ahead (AP): AP - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.: nan\n\nRESULT: {\n  \"translation\": \"Nuevo M\u00e9todo Podr\u00eda Predecir Terremotos con Semanas de Anticipaci\u00f3n (AP): AP - Ge\u00f3logos suecos podr\u00edan haber encontrado una manera de predecir terremotos semanas antes de que ocurran, mediante el monitoreo de la cantidad de metales como zinc y cobre en el agua subterr\u00e1nea cerca de los sitios de terremotos, dijeron cient\u00edficos el mi\u00e9rcoles.\",\n  \"notes\": \"The translation maintains the original meaning and structure of the text. The phrase 'subsoil water' was translated as 'agua subterr\u00e1nea,' which is the most common term used in Spanish for this concept. The cultural context remains the same as the topic is scientific and universally understood. No significant cultural adaptations were necessary.\"\n}\n\n -------------------------- \n\nInference ID: translation-2. \n\nTEXT: Marine Expedition Finds New Species (AP): AP - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings  #151; including what appear to be new species of fish and squid  #151; could be used to protect marine ecosystems worldwide.: nan\n\nRESULT: {\n  \"translation\": \"Expedici\u00f3n marina descubre nuevas especies (AP): AP - Cient\u00edficos noruegos que exploraron las aguas profundas del Oc\u00e9ano Atl\u00e1ntico anunciaron el jueves que sus hallazgos \u2014incluyendo lo que parecen ser nuevas especies de peces y calamares\u2014 podr\u00edan ser utilizados para proteger los ecosistemas marinos a nivel mundial.\",\n  \"notes\": \"The translation maintains the original structure and intent of the text. The use of em dashes (\u2014) is preserved to indicate a break in thought, which is also common in Spanish. The phrase 'could be used to protect marine ecosystems worldwide' is translated directly, as the concept of protecting ecosystems is universally understood and relevant in Spanish-speaking contexts. No significant cultural adaptations were necessary, as the topic of scientific discovery and environmental protection is globally applicable.\"\n}\n\n -------------------------- \n\nInference ID: summary-0. \n\nTEXT: Feds Accused of Exaggerating Fire Impact (AP): AP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.: nan\n\nRESULT: {\n  \"summary\": \"A Forest Service expert claims the agency overstated wildfires' impact on California spotted owls to justify increased logging in the Sierra Nevada.\"\n}\n\n -------------------------- \n\nInference ID: summary-1. \n\nTEXT: New Method May Predict Quakes Weeks Ahead (AP): AP - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.: nan\n\nRESULT: {\n  \"summary\": \"Swedish geologists have potentially developed a method to predict earthquakes weeks in advance by monitoring zinc and copper levels in subsoil water near earthquake-prone areas.\"\n}\n\n -------------------------- \n\nInference ID: summary-2. \n\nTEXT: Marine Expedition Finds New Species (AP): AP - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings  #151; including what appear to be new species of fish and squid  #151; could be used to protect marine ecosystems worldwide.: nan\n\nRESULT: {\n  \"summary\": \"Norwegian scientists discovered potential new species of fish and squid during a deep-sea expedition in the Atlantic Ocean. Their findings aim to aid in the protection of global marine ecosystems.\"\n}\n\n -------------------------- \n\nInference ID: topic_classification-0. \n\nTEXT: Feds Accused of Exaggerating Fire Impact (AP): AP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.: nan\n\nRESULT: ```json\n{\n  \"category\": \"politics\",\n  \"confidence\": 0.8\n}\n```\n\n -------------------------- \n\nInference ID: topic_classification-1. \n\nTEXT: New Method May Predict Quakes Weeks Ahead (AP): AP - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.: nan\n\nRESULT: ```json\n{\n  \"category\": \"science\",\n  \"confidence\": 0.95\n}\n```\n\n -------------------------- \n\nInference ID: topic_classification-2. \n\nTEXT: Marine Expedition Finds New Species (AP): AP - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings  #151; including what appear to be new species of fish and squid  #151; could be used to protect marine ecosystems worldwide.: nan\n\nRESULT: ```json\n{\n  \"category\": \"science\",\n  \"confidence\": 0.95\n}\n```\n\n -------------------------- \n\nInference ID: keyword_extraction-0. \n\nTEXT: Feds Accused of Exaggerating Fire Impact (AP): AP - The Forest Service exaggerated the effect of wildfires on California spotted owls in justifying a planned increase in logging in the Sierra Nevada, according to a longtime agency expert who worked on the plan.: nan\n\nRESULT: ```json\n{\n  \"keywords\": [\"Forest Service\", \"wildfires\", \"California spotted owls\", \"logging\", \"Sierra Nevada\"],\n  \"context\": \"The keywords highlight the core elements of the text. 'Forest Service' refers to the agency accused of exaggerating wildfire impacts. 'Wildfires' are the natural disaster in question, impacting 'California spotted owls', a species affected by the fires. 'Logging' is the planned activity justified by the exaggerated claims, and 'Sierra Nevada' is the geographic region where these events are taking place.\"\n}\n```\n\n -------------------------- \n\nInference ID: keyword_extraction-1. \n\nTEXT: New Method May Predict Quakes Weeks Ahead (AP): AP - Swedish geologists may have found a way to predict earthquakes weeks before they happen by monitoring the amount of metals like zinc and copper in subsoil water near earthquake sites, scientists said Wednesday.: nan\n\nRESULT: ```json\n{\n  \"keywords\": [\"earthquake prediction\", \"geologists\", \"zinc\", \"copper\", \"subsoil water\"],\n  \"context\": \"The text discusses a new method developed by Swedish geologists to predict earthquakes weeks in advance. The method involves monitoring the levels of metals such as zinc and copper in subsoil water near earthquake sites. 'Earthquake prediction' is the main focus, while 'geologists' refers to the scientists involved. 'Zinc' and 'copper' are the metals being monitored, and 'subsoil water' is the medium where these metals are measured.\"\n}\n```\n\n -------------------------- \n\nInference ID: keyword_extraction-2. \n\nTEXT: Marine Expedition Finds New Species (AP): AP - Norwegian scientists who explored the deep waters of the Atlantic Ocean said Thursday their findings  #151; including what appear to be new species of fish and squid  #151; could be used to protect marine ecosystems worldwide.: nan\n\nRESULT: {\n  \"keywords\": [\"Marine Expedition\", \"New Species\", \"Atlantic Ocean\", \"Norwegian Scientists\", \"Marine Ecosystems\"],\n  \"context\": \"The keywords highlight the core aspects of the text. 'Marine Expedition' refers to the scientific exploration conducted in the deep waters. 'New Species' emphasizes the discovery of previously unknown fish and squid. 'Atlantic Ocean' specifies the location of the expedition. 'Norwegian Scientists' identifies the group responsible for the research. 'Marine Ecosystems' underscores the broader goal of using the findings to protect ocean habitats globally.\"\n}\n</pre> <p>This tutorial used the chat completion endpoint to perform many tasks via kluster.ai batch API. This particular example performed five different tasks for each element of the dataset: sentiment analysis, translation (to Spanish), summarization, topic classification and keyword extraction.</p> <p>To submit a batch job we've:</p> <ol> <li>Created the JSONL file, where each line of the file represented a separate request (for each task and element of dataset)</li> <li>Submitted the file to the platform</li> <li>Started the batch job, and monitored its progress</li> <li>Once completed, we fetched the results</li> </ol> <p>All of this using the OpenAI Python library and API, no changes needed!</p> <p>Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!</p>"},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#multiple-inference-requests-with-klusterai","title":"Multiple inference requests with kluster.ai\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#prerequisites","title":"Prerequisites\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#get-the-data","title":"Get the data\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#define-the-requests","title":"Define the requests\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#create-the-batch-job-file","title":"Create the batch job file\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#upload-batch-job-files-to-klusterai","title":"Upload batch job files to kluster.ai\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#start-the-batch-job","title":"Start the batch job\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#check-job-progress","title":"Check job progress\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#get-the-results","title":"Get the results\u00b6","text":""},{"location":"tutorials/klusterai-api/multiple-tasks-batch-api/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/","title":"Sentiment analysis","text":"<p>Sentiment analysis is the process of reviewing text to determine whether there is positive, neutral, or negative notation to the statement. LLMs can be extremely powerful, processing a lot of data quickly, helping understand the overall sentiment of a large dataset.</p> <p>This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to run a sentiment analysis on sample data.</p> <p>The example uses an extract from the Amazon musical instrument reviews dataset to determine the sentiment of each review.</p> <p>You can adapt this example by using your data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, and obtain categorized results powered by a state-of-the-art language model.</p> <p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul> <p>In this notebook, we'll use Python's <code>getpass</code> module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces).</p> In\u00a0[1]: Copied! <pre>from getpass import getpass\n\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass  api_key = getpass(\"Enter your kluster.ai API key: \") <pre>Enter your kluster.ai API key:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p>Next, ensure you've installed OpenAI Python library:</p> In\u00a0[\u00a0]: Copied! <pre>%pip install -q openai\n</pre> %pip install -q openai <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> <p>With the OpenAI Python library installed, we import the necessary dependencies for the tutorial:</p> In\u00a0[3]: Copied! <pre>from openai import OpenAI\n\nimport pandas as pd\nimport time\nimport json\nimport os\nfrom IPython.display import clear_output, display\n</pre> from openai import OpenAI  import pandas as pd import time import json import os from IPython.display import clear_output, display <p>And then, initialize the <code>client</code> by pointing it to the kluster.ai endpoint, and passing your API key.</p> In\u00a0[4]: Copied! <pre># Set up the client\nclient = OpenAI(\n    base_url=\"https://api.kluster.ai/v1\",\n    api_key=api_key,\n)\n</pre> # Set up the client client = OpenAI(     base_url=\"https://api.kluster.ai/v1\",     api_key=api_key, ) <p>We've preloaded a sample dataset sourced from Amazon's reviews of musical instruments. This dataset contains customer feedback on various music-related products, ready for you to analyze. No further setup is required\u2014just jump into the next steps to start working with the data.</p> <p>Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can talk about the data.</p> <p>This notebook includes a preloaded sample dataset sourced from Amazon's reviews of musical instruments. It contains customer feedback on various music-related products. No additional setup is needed. Proceed to the next steps to begin working with this data.</p> In\u00a0[5]: Copied! <pre>df = pd.DataFrame({\n    \"text\": [\n        \"It hums, crackles, and I think I'm having problems with my equipment. As soon as I use any of my other cords then the problem is gone. Hosa makes some other products that have good value. But based on my experience I don't recommend this one.\",\n        \"I bought this to use with my keyboard. I wasn't really aware that there were other options for keyboard pedals. It doesn't work as smoothly as the pedals do on an acoustic piano, which is what I'd always used. Doesn't have the same feel either. Nowhere close.In my opinion, a sustain pedal like the M-Audio SP-2 Sustain Pedal with Piano Style Action or other similar pedal is a much better choice. The price difference is only a few dollars and the feel and action are so much better.\",\n        \"This cable disproves the notion that you get what you pay for. It's quality outweighs its price. Let's face it, a cable is a cable is a cable. But the quality of these cables can vary greatly. I replaced a lighter cable with this one and I was surprised at the difference in the quality of the sound from my amp. I have an Ibanez ART series guitar into an Ibanez 15 watt amp set up in my home. With nothing changed but the cable, there was a significant difference in quality and volume. So much so that I checked with my guitar teacher who said he was not surprised. The quality appears good. The ends are heavy duty and the little bit of hum I had due to the proximity of everything was attenuated to the point where it was inconsequential. I've seen more expensive cables and this one is (so far) great.Hosa GTR210 Guitar Cable 10 Ft\",\n        \"Bought this to hook up a Beta 58 to a Panasonic G2 DSLR and a Kodak Zi8 for interviews. Works the way it's supposed to. 90 degree TRS is a nice touch. Good price.\",\n        \"96\tJust received this cord and it seems to work as expected. What can you say about an adapter cord? It is well made, good construction and sound from my DSLR with my mic is superb.\"\n    ]\n})\n</pre> df = pd.DataFrame({     \"text\": [         \"It hums, crackles, and I think I'm having problems with my equipment. As soon as I use any of my other cords then the problem is gone. Hosa makes some other products that have good value. But based on my experience I don't recommend this one.\",         \"I bought this to use with my keyboard. I wasn't really aware that there were other options for keyboard pedals. It doesn't work as smoothly as the pedals do on an acoustic piano, which is what I'd always used. Doesn't have the same feel either. Nowhere close.In my opinion, a sustain pedal like the M-Audio SP-2 Sustain Pedal with Piano Style Action or other similar pedal is a much better choice. The price difference is only a few dollars and the feel and action are so much better.\",         \"This cable disproves the notion that you get what you pay for. It's quality outweighs its price. Let's face it, a cable is a cable is a cable. But the quality of these cables can vary greatly. I replaced a lighter cable with this one and I was surprised at the difference in the quality of the sound from my amp. I have an Ibanez ART series guitar into an Ibanez 15 watt amp set up in my home. With nothing changed but the cable, there was a significant difference in quality and volume. So much so that I checked with my guitar teacher who said he was not surprised. The quality appears good. The ends are heavy duty and the little bit of hum I had due to the proximity of everything was attenuated to the point where it was inconsequential. I've seen more expensive cables and this one is (so far) great.Hosa GTR210 Guitar Cable 10 Ft\",         \"Bought this to hook up a Beta 58 to a Panasonic G2 DSLR and a Kodak Zi8 for interviews. Works the way it's supposed to. 90 degree TRS is a nice touch. Good price.\",         \"96\tJust received this cord and it seems to work as expected. What can you say about an adapter cord? It is well made, good construction and sound from my DSLR with my mic is superb.\"     ] }) <p>To execute the batch inference job, we'll take the following steps:</p> <ol> <li>Create the batch job file - we'll generate a JSON lines file with the desired requests to be processed by the model</li> <li>Upload the batch job file - once it is ready, we'll upload it to the kluster.ai platform using the API, where it will be processed. We'll receive a unique ID associated with our file</li> <li>Start the batch job - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before</li> <li>Monitor job progress - (optional) track the status of the batch job to ensure it has been successfully completed</li> <li>Retrieve results - once the job has completed execution, we can access and process the resultant data</li> </ol> <p>This notebook is prepared for you to follow along. Run the cells below to watch it all come together.</p> <p>This example selects the <code>klusterai/Meta-Llama-3.3-70B-Instruct-Turbo</code> model. If you'd like to use a different model, feel free to change it by modifying the <code>model</code> field. In this notebook, you can also comment Llama 3.3 70B, and uncomment whatever model you want to try out.</p> <p>Please refer to the Supported models section for a list of the models we support.</p> <p>The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its own model. Also, we are using a temperature of <code>0.5</code> but feel free to change it and play around with the different outcomes.</p> In\u00a0[6]: Copied! <pre># Prompt\nSYSTEM_PROMPT = '''\n    Analyze the sentiment of this text and respond with one word: positive, negative, or neutral.\n    '''\n\n# Models\n#model=\"deepseek-ai/DeepSeek-R1\"\n#model=\"deepseek-ai/DeepSeek-V3\"\n#model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"\n#model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\"\nmodel=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\"\n#model=\"Qwen/Qwen2.5-VL-7B-Instruct\"\n\n# Ensure the directory exists\nos.makedirs(\"sentiment_analysis\", exist_ok=True)\n\n# Create the batch job file with the prompt and content\ndef create_batch_file(df):\n    batch_list = []\n    for index, row in df.iterrows():\n        content = row['text']\n\n        request = {\n            \"custom_id\": f\"sentiment-analysis-{index}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": model,\n                \"temperature\": 0.5,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": content}\n                ],\n            }\n        }\n        batch_list.append(request)\n    return batch_list\n\n# Save file\ndef save_batch_file(batch_list):\n    filename = f\"sentiment_analysis/batch_job_request.jsonl\"\n    with open(filename, 'w') as file:\n        for request in batch_list:\n            file.write(json.dumps(request) + '\\n')\n    return filename\n</pre> # Prompt SYSTEM_PROMPT = '''     Analyze the sentiment of this text and respond with one word: positive, negative, or neutral.     '''  # Models #model=\"deepseek-ai/DeepSeek-R1\" #model=\"deepseek-ai/DeepSeek-V3\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\" model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\"  # Ensure the directory exists os.makedirs(\"sentiment_analysis\", exist_ok=True)  # Create the batch job file with the prompt and content def create_batch_file(df):     batch_list = []     for index, row in df.iterrows():         content = row['text']          request = {             \"custom_id\": f\"sentiment-analysis-{index}\",             \"method\": \"POST\",             \"url\": \"/v1/chat/completions\",             \"body\": {                 \"model\": model,                 \"temperature\": 0.5,                 \"messages\": [                     {\"role\": \"system\", \"content\": SYSTEM_PROMPT},                     {\"role\": \"user\", \"content\": content}                 ],             }         }         batch_list.append(request)     return batch_list  # Save file def save_batch_file(batch_list):     filename = f\"sentiment_analysis/batch_job_request.jsonl\"     with open(filename, 'w') as file:         for request in batch_list:             file.write(json.dumps(request) + '\\n')     return filename <p>Let's run the functions we've defined before:</p> In\u00a0[7]: Copied! <pre>batch_list = create_batch_file(df)\ndata_dir = save_batch_file(batch_list)\nprint(data_dir)\n</pre> batch_list = create_batch_file(df) data_dir = save_batch_file(batch_list) print(data_dir) <pre>sentiment_analysis/batch_job_request.jsonl\n</pre> <p>Next, we can preview what that batch job file looks like:</p> In\u00a0[8]: Copied! <pre>!head -n 1 sentiment_analysis/batch_job_request.jsonl\n</pre> !head -n 1 sentiment_analysis/batch_job_request.jsonl <pre>{\"custom_id\": \"sentiment-analysis-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n    Analyze the sentiment of this text and respond with one word: positive, negative, or neutral.\\n    \"}, {\"role\": \"user\", \"content\": \"It hums, crackles, and I think I'm having problems with my equipment. As soon as I use any of my other cords then the problem is gone. Hosa makes some other products that have good value. But based on my experience I don't recommend this one.\"}]}}\n</pre> <p>Now that we've prepared our input file, it's time to upload it to the kluster.ai platform. To do so, you can use the <code>files.create</code> endpoint of the client, where the purpose is set to <code>batch</code>. This will return the file ID, which we need to log for the next steps.</p> In\u00a0[9]: Copied! <pre># Upload batch job request file\nwith open(data_dir, 'rb') as file:\n    upload_response = client.files.create(\n        file=file,\n        purpose=\"batch\"\n    )\n\n    # Print job ID\n    file_id = upload_response.id\n    print(f\"File uploaded successfully. File ID: {file_id}\")\n</pre> # Upload batch job request file with open(data_dir, 'rb') as file:     upload_response = client.files.create(         file=file,         purpose=\"batch\"     )      # Print job ID     file_id = upload_response.id     print(f\"File uploaded successfully. File ID: {file_id}\")  <pre>File uploaded successfully. File ID: 67e57e7933090e20560503db\n</pre> <p>Once the file has been successfully uploaded, we're ready to start (create) the batch job by providing the file ID we got in the previous step. To do so, we use the <code>batches.create</code> method, for which we need to set the endpoint to <code>/v1/chat/completions</code>. This will return the batch job details, with the ID.</p> In\u00a0[10]: Copied! <pre># Create batch job with completions endpoint\nbatch_job = client.batches.create(\n    input_file_id=file_id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\"\n)\n\nprint(\"\\nBatch job created:\")\nbatch_dict = batch_job.model_dump()\nprint(json.dumps(batch_dict, indent=2))\n</pre> # Create batch job with completions endpoint batch_job = client.batches.create(     input_file_id=file_id,     endpoint=\"/v1/chat/completions\",     completion_window=\"24h\" )  print(\"\\nBatch job created:\") batch_dict = batch_job.model_dump() print(json.dumps(batch_dict, indent=2)) <pre>\nBatch job created:\n{\n  \"id\": \"67e57e7d33090e20560504e2\",\n  \"completion_window\": \"24h\",\n  \"created_at\": 1743093373,\n  \"endpoint\": \"/v1/chat/completions\",\n  \"input_file_id\": \"67e57e7933090e20560503db\",\n  \"object\": \"batch\",\n  \"status\": \"pre_schedule\",\n  \"cancelled_at\": null,\n  \"cancelling_at\": null,\n  \"completed_at\": null,\n  \"error_file_id\": null,\n  \"errors\": [],\n  \"expired_at\": null,\n  \"expires_at\": 1743179773,\n  \"failed_at\": null,\n  \"finalizing_at\": null,\n  \"in_progress_at\": null,\n  \"metadata\": {},\n  \"output_file_id\": null,\n  \"request_counts\": {\n    \"completed\": 0,\n    \"failed\": 0,\n    \"total\": 0\n  }\n}\n</pre> <p>All requests are currently being processed.</p> <p>Now that your batch job has been created, you can track its progress.</p> <p>To monitor the job's progress, you can use the <code>batches.retrieve</code> method and pass the batch job ID. The response contains an <code>status</code> field that tells us if it is completed or not, and the subsequent status of each job separately.</p> <p>The following snippet checks the status every 10 seconds until the entire batch is completed:</p> In\u00a0[11]: Copied! <pre>all_completed = False\n\n# Loop to check status every 10 seconds\nwhile not all_completed:\n    all_completed = True\n    output_lines = []\n\n    updated_job = client.batches.retrieve(batch_job.id)\n\n    if updated_job.status != \"completed\":\n        all_completed = False\n        completed = updated_job.request_counts.completed\n        total = updated_job.request_counts.total\n        output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\")\n    else:\n        output_lines.append(f\"Job completed!\")\n\n    # Clear the output and display updated status\n    clear_output(wait=True)\n    for line in output_lines:\n        display(line)\n\n    if not all_completed:\n        time.sleep(10)\n</pre> all_completed = False  # Loop to check status every 10 seconds while not all_completed:     all_completed = True     output_lines = []      updated_job = client.batches.retrieve(batch_job.id)      if updated_job.status != \"completed\":         all_completed = False         completed = updated_job.request_counts.completed         total = updated_job.request_counts.total         output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\")     else:         output_lines.append(f\"Job completed!\")      # Clear the output and display updated status     clear_output(wait=True)     for line in output_lines:         display(line)      if not all_completed:         time.sleep(10) <pre>'Job completed!'</pre> <p>With the job completed, we'll retrieve the results and review the responses generated for each request. We then parse these results. To fetch them from the platform, retrieve the <code>output_file_id</code> from the batch job, then use the <code>files.content</code> endpoint with that file ID. Note that the job status must be <code>completed</code> before you can retrieve the results!</p> In\u00a0[12]: Copied! <pre>#Parse results as a JSON object\ndef parse_json_objects(data_string):\n    if isinstance(data_string, bytes):\n        data_string = data_string.decode('utf-8')\n\n    json_strings = data_string.strip().split('\\n')\n    json_objects = []\n\n    for json_str in json_strings:\n        try:\n            json_obj = json.loads(json_str)\n            json_objects.append(json_obj)\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON: {e}\")\n\n    return json_objects\n\n# Retrieve results with job ID\njob = client.batches.retrieve(batch_job.id)\nresult_file_id = job.output_file_id\nresult = client.files.content(result_file_id).content\n\n# Parse JSON results\nparsed_result = parse_json_objects(result)\n\n# Extract and print only the content of each response\nprint(\"\\nExtracted Responses:\")\nfor item in parsed_result:\n    try:\n        content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n        print(content)\n    except KeyError as e:\n        print(f\"Missing key in response: {e}\")\n</pre> #Parse results as a JSON object def parse_json_objects(data_string):     if isinstance(data_string, bytes):         data_string = data_string.decode('utf-8')      json_strings = data_string.strip().split('\\n')     json_objects = []      for json_str in json_strings:         try:             json_obj = json.loads(json_str)             json_objects.append(json_obj)         except json.JSONDecodeError as e:             print(f\"Error parsing JSON: {e}\")      return json_objects  # Retrieve results with job ID job = client.batches.retrieve(batch_job.id) result_file_id = job.output_file_id result = client.files.content(result_file_id).content  # Parse JSON results parsed_result = parse_json_objects(result)  # Extract and print only the content of each response print(\"\\nExtracted Responses:\") for item in parsed_result:     try:         content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]         print(content)     except KeyError as e:         print(f\"Missing key in response: {e}\") <pre>\nExtracted Responses:\nNegative.\nNegative.\nPositive.\nPositive.\nPositive.\n</pre> <p>This tutorial used the chat completion endpoint to perform a simple sentiment analysis task with batch inference. This particular example classified a series of reviews to understand if they had a positive, neutral or negative note.</p> <p>To submit a batch job, we've:</p> <ol> <li>Created the JSONL file, where each line of the file represented a separate request</li> <li>Submitted the file to the platform</li> <li>Started the batch job, and monitored its progress</li> <li>Once completed, we fetched the results</li> </ol> <p>All of this using the OpenAI Python library and API, no changes needed!</p> <p>Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!</p>"},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#sentiment-analysis-with-klusterai-api","title":"Sentiment analysis with kluster.ai API\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#prerequisites","title":"Prerequisites\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#get-the-data","title":"Get the data\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#perform-batch-inference","title":"Perform batch inference\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#create-the-batch-input-file","title":"Create the batch input file\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#upload-inference-file-to-klusterai","title":"Upload inference file to kluster.ai\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#start-the-job","title":"Start the job\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#check-job-progress","title":"Check job progress\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#get-the-results","title":"Get the results\u00b6","text":""},{"location":"tutorials/klusterai-api/sentiment-analysis-api/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-curator/","title":"Using Curator","text":"<p>This notebook goes through the same example as in our previous Text classification notebook, but this time, we'll be using Bespoke Curator instead of the OpenAI Python library</p> <p>To recap, the notebook uses kluster.ai batch API to classify a data set based on a predefined set of categories.</p> <p>The example uses an extract from the IMDB top 1000 movies dataset and categorizes them into \"Action,\" \"Adventure,\" \"Comedy,\" \"Crime,\" \"Documentary,\" \"Drama,\" \"Fantasy,\" \"Horror,\" \"Romance,\" or \"Sci-Fi.\"</p> <p>You can adapt this example by using your data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, and obtain categorized results powered by a state-of-the-art language model.</p> <p>In this notebook, we'll use Python's <code>getpass</code> module to input the key safely. After execution, please provide your unique kluster.ai API key (ensure no spaces).</p> In\u00a0[3]: Copied! <pre>from getpass import getpass\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass api_key = getpass(\"Enter your kluster.ai API key: \") <pre>Enter your kluster.ai API key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p>Next, ensure you've the Bespoke Curator Python library:</p> In\u00a0[1]: Copied! <pre>pip install -q bespokelabs-curator\n</pre> pip install -q bespokelabs-curator <pre>WARNING: google-cloud-aiplatform 1.71.1 does not provide the extra 'all'\n</pre> <p>Now that we've the library, we can initialize the LLM object for batch. Note that Curator supports kluster.ai natively, so you just need to provide the model to use, API key, and completion window.</p> <p>This example uses <code>klusterai/Meta-Llama-3.1-8B-Instruct-Turbo</code>, but feel free to comment it and uncomment any other model you want to try out.</p> <p>Please refer to the Supported models section for a list of the models we support.</p> In\u00a0[4]: Copied! <pre>from bespokelabs import curator\n\n# Models\n#model=\"deepseek-ai/DeepSeek-R1\"\n#model=\"deepseek-ai/DeepSeek-V3\"\nmodel=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"\n#model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\"\n#model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\"\n#model=\"Qwen/Qwen2.5-VL-7B-Instruct\"\n\nllm = curator.LLM(\n    model_name=model,\n    batch=True,\n    backend=\"klusterai\",\n    backend_params={\"api_key\": api_key, \"completion_window\": \"24h\"})\n</pre> from bespokelabs import curator  # Models #model=\"deepseek-ai/DeepSeek-R1\" #model=\"deepseek-ai/DeepSeek-V3\" model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\"  llm = curator.LLM(     model_name=model,     batch=True,     backend=\"klusterai\",     backend_params={\"api_key\": api_key, \"completion_window\": \"24h\"}) <pre>DEBUG:curator.bespokelabs.curator.log:Adjusting file descriptor limit from 1048576 to 1048576 (hard limit: 1048576)\n</pre> <p>With the Curator LLM object ready, let's define the data and prompt.</p> <p>This notebook includes a preloaded sample dataset derived from the Top 1000 IMDb Movies dataset. It contains movie descriptions ready for classification. No additional setup is needed. Proceed to the next steps to begin working with this data.</p> <p>For this particular scenario, the prompt consists of the request to the model and the data (movie) to be classified. Because this is a batch job, each separate request must contain both.</p> In\u00a0[5]: Copied! <pre>movies = [\"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\",\n        \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\",\n        \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\",\n        \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\",\n        \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\"]\n</pre> movies = [\"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\",         \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\",         \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\",         \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\",         \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\"] In\u00a0[\u00a0]: Copied! <pre>prompts = [f\"Classify the main genre of the given movie description based on the following genres (Respond with only the genre): \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.\\n{movie}\" for movie in movies]\n\n# Log the prompt\nfor prompt in prompts:\n    print(prompt)\n</pre> prompts = [f\"Classify the main genre of the given movie description based on the following genres (Respond with only the genre): \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.\\n{movie}\" for movie in movies]  # Log the prompt for prompt in prompts:     print(prompt)  <pre>Classify the main genre of the given movie description based on the following genres (Respond with only the genre): \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.\nBreakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\nClassify the main genre of the given movie description based on the following genres (Respond with only the genre): \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.\nGiant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\nClassify the main genre of the given movie description based on the following genres (Respond with only the genre): \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.\nFrom Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\nClassify the main genre of the given movie description based on the following genres (Respond with only the genre): \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.\nLifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\nClassify the main genre of the given movie description based on the following genres (Respond with only the genre): \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.\nThe 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\n</pre> <p>Now that everything is set, we can execute the inference job. With Curator it is extremely simple, we just need to pass the prompts to the LLM object, and log the response.</p> In\u00a0[7]: Copied! <pre>responses = llm(prompts)\n</pre> responses = llm(prompts) <pre>Generating train split: 0 examples [00:00, ? examples/s]</pre> <pre>DEBUG:curator.bespokelabs.curator.log:Curator Cache Fingerprint String: 36e766fc298f4350_5ac272e3bc92b32e_klusterai/Meta-Llama-3.1-8B-Instruct-Turbo_text_True\nDEBUG:curator.bespokelabs.curator.log:Curator Cache Fingerprint: 3acb0d5efbda6beb\n</pre> <pre>[03/24/25 09:51:35] INFO     Running OpenAIBatchRequestProcessor completions with     base_request_processor.py:130\n                             model: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo                                     \n</pre> <pre>INFO:curator.bespokelabs.curator.log:Running OpenAIBatchRequestProcessor completions with model: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\n</pre> <pre>                    INFO     Preparing request file(s) in                             base_request_processor.py:230\n                             /root/.cache/curator/3acb0d5efbda6beb                                                 \n</pre> <pre>INFO:curator.bespokelabs.curator.log:Preparing request file(s) in /root/.cache/curator/3acb0d5efbda6beb\n</pre> <pre>                    INFO     Wrote 5 requests to                                      base_request_processor.py:312\n                             /root/.cache/curator/3acb0d5efbda6beb/requests_0.jsonl.                               \n</pre> <pre>INFO:curator.bespokelabs.curator.log:Wrote 5 requests to /root/.cache/curator/3acb0d5efbda6beb/requests_0.jsonl.\nDEBUG:curator.bespokelabs.curator.log:Batch file content size: 0.00 MB (3,367 bytes)\n</pre> <pre>Output()</pre> <pre>DEBUG:curator.bespokelabs.curator.log:skipping uploaded file status check, provider does not support file checks.\nDEBUG:curator.bespokelabs.curator.log:File uploaded with id 67e12b272d9e5ba243fe9ba1\nDEBUG:curator.bespokelabs.curator.log:Batch submitted with id 67e12b286afe1d706e726f73\nDEBUG:curator.bespokelabs.curator.log:Marked /root/.cache/curator/3acb0d5efbda6beb/requests_0.jsonl as submitted with batch 67e12b286afe1d706e726f73\nDEBUG:curator.bespokelabs.curator.log:Updated submitted batch 67e12b286afe1d706e726f73 with new request counts\nDEBUG:curator.bespokelabs.curator.log:Batch 67e12b286afe1d706e726f73 status: in_progress requests: 0/0/5 succeeded/failed/total\nDEBUG:curator.bespokelabs.curator.log:Batches returned: 0/1 Requests completed: 0/5\nDEBUG:curator.bespokelabs.curator.log:Sleeping for 60 seconds...\nDEBUG:curator.bespokelabs.curator.log:Updated submitted batch 67e12b286afe1d706e726f73 with new request counts\nDEBUG:curator.bespokelabs.curator.log:Batch 67e12b286afe1d706e726f73 status: completed requests: 5/0/5 succeeded/failed/total\nDEBUG:curator.bespokelabs.curator.log:Batch 67e12b286afe1d706e726f73 finished with status: completed\nDEBUG:curator.bespokelabs.curator.log:Marked batch 67e12b286afe1d706e726f73 as finished\nDEBUG:curator.bespokelabs.curator.log:Batch 67e12b286afe1d706e726f73 completed and downloaded\n</pre> <pre>/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab \n(https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n</pre> <pre>tokenizer.json:   0%|          | 0.00/9.08M [00:00&lt;?, ?B/s]</pre> <pre>DEBUG:curator.bespokelabs.curator.log:Batch 67e12b286afe1d706e726f73 written to /root/.cache/curator/3acb0d5efbda6beb/responses_0.jsonl\nDEBUG:curator.bespokelabs.curator.log:Marked batch 67e12b286afe1d706e726f73 as downloaded\nDEBUG:curator.bespokelabs.curator.log:Batches returned: 1/1 Requests completed: 5/5\n</pre> <pre></pre> <pre>\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% \u2022 Time Elapsed 0:01:04 \u2022 Time Remaining 0:00:00\n</pre> <pre>Curator Viewer: Disabled                                                            \nSet HOSTED_CURATOR_VIEWER=1 to view your data live at https://curator.bespokelabs.ai\nBatches: Total: 1 \u2022 Submitted: 0\u22ef \u2022 Downloaded: 1\u2713                                  \nRequests: Total: 5 \u2022 Submitted: 0\u22ef \u2022 Succeeded: 5\u2713 \u2022 Failed: 0\u2717                     \nTokens: Avg Input: 133 \u2022 Avg Output: 7                                              \nCost: Current: $0.000 \u2022 Projected: $0.000 \u2022 Rate: $0.000/request                    \nModel: Name: klusterai/Meta-Llama-3.1-8B-Instruct-Turbo                             \nModel Pricing: Per 1M tokens: Input: $0.050 \u2022 Output: $0.050                        \n</pre> <pre>                         Final Curator Statistics                          \n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Section/Metric             \u2502 Value                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Model                      \u2502                                            \u2502\n\u2502 Model                      \u2502 klusterai/Meta-Llama-3.1-8B-Instruct-Turbo \u2502\n\u2502 Batches                    \u2502                                            \u2502\n\u2502 Total Batches              \u2502 1                                          \u2502\n\u2502 Submitted                  \u2502 0                                          \u2502\n\u2502 Downloaded                 \u2502 1                                          \u2502\n\u2502 Requests                   \u2502                                            \u2502\n\u2502 Total Requests             \u2502 5                                          \u2502\n\u2502 Successful                 \u2502 5                                          \u2502\n\u2502 Failed                     \u2502 0                                          \u2502\n\u2502 Tokens                     \u2502                                            \u2502\n\u2502 Total Tokens Used          \u2502 0                                          \u2502\n\u2502 Total Input Tokens         \u2502 664                                        \u2502\n\u2502 Total Output Tokens        \u2502 35                                         \u2502\n\u2502 Average Tokens per Request \u2502 0                                          \u2502\n\u2502 Average Input Tokens       \u2502 132                                        \u2502\n\u2502 Average Output Tokens      \u2502 7                                          \u2502\n\u2502 Costs                      \u2502                                            \u2502\n\u2502 Total Cost                 \u2502 $0.000                                     \u2502\n\u2502 Average Cost per Request   \u2502 $0.000                                     \u2502\n\u2502 Input Cost per 1M Tokens   \u2502 $0.050                                     \u2502\n\u2502 Output Cost per 1M Tokens  \u2502 $0.050                                     \u2502\n\u2502 Performance                \u2502                                            \u2502\n\u2502 Total Time                 \u2502 64.37s                                     \u2502\n\u2502 Average Time per Request   \u2502 12.87s                                     \u2502\n\u2502 Requests per Minute        \u2502 4.7                                        \u2502\n\u2502 Input Tokens per Minute    \u2502 618.9                                      \u2502\n\u2502 Output Tokens per Minute   \u2502 32.6                                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</pre> <pre>[03/24/25 09:52:39] INFO     Read 5 responses.                                        base_request_processor.py:437\n</pre> <pre>INFO:curator.bespokelabs.curator.log:Read 5 responses.\n</pre> <pre>                    INFO     Finalizing writer                                        base_request_processor.py:446\n</pre> <pre>INFO:curator.bespokelabs.curator.log:Finalizing writer\n</pre> <p>Lastly, let's print the response.</p> In\u00a0[8]: Copied! <pre>responses['response']\n</pre> responses['response'] Out[8]: <pre>['Romance',\n 'Drama',\n 'Drama',\n 'Drama',\n 'Thriller is a possible genre but choosing an option from the above categories, it would be \"Drama\"']</pre> <p>This tutorial used the chat completion endpoint and Bespoke Curator to perform a simple text classification task with batch inference. This particular example clasified a series of movies based on their description.</p> <p>Using Curator, submitting a batch job is extremely simple. It handles all the steps of creating the file, uploading it, submitting the batch job, monitoring the job, and retrieving results. Moreover, kluster.ai is natively supported, making things even easier!</p> <p>Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!</p>"},{"location":"tutorials/klusterai-api/text-classification/text-classification-curator/#text-classification-with-klusterai-api-and-bespoke-curator","title":"Text classification with kluster.ai API and Bespoke Curator\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-curator/#prerequisites","title":"Prerequisites\u00b6","text":"<p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul>"},{"location":"tutorials/klusterai-api/text-classification/text-classification-curator/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-curator/#get-the-data","title":"Get the data\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-curator/#perform-batch-inference-with-curator","title":"Perform batch inference with Curator\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-curator/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/","title":"Using OpenAI API","text":"<p>Text classification is assigning a class/label to a given text, and it is a common go-to example to demonstrate how helpful an AI model can be.</p> <p>This tutorial runs through a notebook where you'll learn how to use the kluster.ai batch API to classify a dataset based on a predefined set of categories.</p> <p>The example uses an extract from the IMDB top 1000 movies dataset and categorizes them into \"Action,\" \"Adventure,\" \"Comedy,\" \"Crime,\" \"Documentary,\" \"Drama,\" \"Fantasy,\" \"Horror,\" \"Romance,\" or \"Sci-Fi.\"</p> <p>You can adapt this example by using your data and categories relevant to your use case. With this approach, you can effortlessly process datasets of any scale, big or small, and obtain categorized results powered by a state-of-the-art language model.</p> <p>Before getting started, ensure you have the following:</p> <ul> <li>A kluster.ai account - sign up on the kluster.ai platform if you don't have one</li> <li>A kluster.ai API key - after signing in, go to the API Keys section and create a new key. For detailed instructions, check out the Get an API key guide</li> </ul> <p>In this notebook, we'll use Python's <code>getpass</code> module to safely input the key. After execution, please provide your unique kluster.ai API key (ensure no spaces).</p> In\u00a0[1]: Copied! <pre>from getpass import getpass\n\napi_key = getpass(\"Enter your kluster.ai API key: \")\n</pre> from getpass import getpass  api_key = getpass(\"Enter your kluster.ai API key: \") <pre>Enter your kluster.ai API key:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p>Next, ensure you've installed OpenAI Python library:</p> In\u00a0[\u00a0]: Copied! <pre>%pip install -q openai\n</pre> %pip install -q openai <pre>Note: you may need to restart the kernel to use updated packages.\n</pre> <p>With the OpenAI Python library installed, we import the necessary dependencies for the tutorial:</p> In\u00a0[3]: Copied! <pre>from openai import OpenAI\n\nimport pandas as pd\nimport time\nimport json\nimport os\nfrom IPython.display import clear_output, display\n</pre> from openai import OpenAI  import pandas as pd import time import json import os from IPython.display import clear_output, display <p>And then, initialize the <code>client</code> by pointing it to the kluster.ai endpoint, and passing your API key.</p> In\u00a0[4]: Copied! <pre># Set up the client\nclient = OpenAI(\n    base_url=\"https://api.kluster.ai/v1\",\n    api_key=api_key,\n)\n</pre> # Set up the client client = OpenAI(     base_url=\"https://api.kluster.ai/v1\",     api_key=api_key, ) <p>Now that you've initialized an OpenAI-compatible client pointing to kluster.ai, we can talk about the data.</p> <p>This notebook includes a preloaded sample dataset derived from the Top 1000 IMDb Movies dataset. It contains movie descriptions ready for classification. No additional setup is needed. Simply proceed to the next steps to begin working with this data.</p> In\u00a0[5]: Copied! <pre>df = pd.DataFrame({\n    \"text\": [\n        \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\",\n        \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\",\n        \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\",\n        \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\",\n        \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\"\n    ]\n})\n</pre> df = pd.DataFrame({     \"text\": [         \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\",         \"Giant: Sprawling epic covering the life of a Texas cattle rancher and his family and associates.\",         \"From Here to Eternity: In Hawaii in 1941, a private is cruelly punished for not boxing on his unit's team, while his captain's wife and second-in-command are falling in love.\",         \"Lifeboat: Several survivors of a torpedoed merchant ship in World War II find themselves in the same lifeboat with one of the crew members of the U-boat that sank their ship.\",         \"The 39 Steps: A man in London tries to help a counter-espionage Agent. But when the Agent is killed, and the man stands accused, he must go on the run to save himself and stop a spy ring which is trying to steal top secret information.\"     ] }) <p>To execute the batch inference job, we'll take the following steps:</p> <ol> <li>Create the batch job file - we'll generate a JSON lines file with the desired requests to be processed by the model</li> <li>Upload the batch job file - once it is ready, we'll upload it to the kluster.ai platform using the API, where it will be processed. We'll receive a unique ID associated with our file</li> <li>Start the batch job - after the file is uploaded, we'll initiate the job to process the uploaded data, using the file ID obtained before</li> <li>Monitor job progress - (optional) track the status of the batch job to ensure it has been successfully completed</li> <li>Retrieve results - once the job has completed execution, we can access and process the resultant data</li> </ol> <p>This notebook is prepared for you to follow along. Run the cells below to watch it all come together.</p> <p>This example selects the <code>deepseek-ai/DeepSeek-V3</code> model. If you'd like to use a different model, feel free to change it by modifying the <code>model</code> field. In this notebook, you can also comment DeepSeek V3, and uncomment whatever model you want to try out.</p> <p>Please refer to the Supported models section for a list of the models we support.</p> <p>The following snippets prepare the JSONL file, where each line represents a different request. Note that each separate batch request can have its own model. Also, we are using a temperature of <code>0.5</code> but feel free to change it and play around with the different outcomes (but we are only asking to respond with a single word, the genre).</p> In\u00a0[6]: Copied! <pre># Prompt\nSYSTEM_PROMPT = '''\n    Classify the main genre of the given movie description based on the following genres (Respond with only the genre):\n    \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.\n    '''\n\n# Models\n#model=\"deepseek-ai/DeepSeek-R1\"\nmodel=\"deepseek-ai/DeepSeek-V3\"\n#model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\"\n#model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\"\n#model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\"\n#model=\"Qwen/Qwen2.5-VL-7B-Instruct\"\n\n# Ensure the directory exists\nos.makedirs(\"text_clasification\", exist_ok=True)\n\n# Create the batch job file with the prompt and content\ndef create_batch_file(df):\n    batch_list = []\n    for index, row in df.iterrows():\n        content = row['text']\n\n        request = {\n            \"custom_id\": f\"movie_classification-{index}\",\n            \"method\": \"POST\",\n            \"url\": \"/v1/chat/completions\",\n            \"body\": {\n                \"model\": model,\n                \"temperature\": 0.5,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": content}\n                ],\n            }\n        }\n        batch_list.append(request)\n    return batch_list\n\n# Save file\ndef save_batch_file(batch_list):\n    filename = f\"text_clasification/batch_job_request.jsonl\"\n    with open(filename, 'w') as file:\n        for request in batch_list:\n            file.write(json.dumps(request) + '\\n')\n    return filename\n</pre> # Prompt SYSTEM_PROMPT = '''     Classify the main genre of the given movie description based on the following genres (Respond with only the genre):     \u201cAction\u201d, \u201cAdventure\u201d, \u201cComedy\u201d, \u201cCrime\u201d, \u201cDocumentary\u201d, \u201cDrama\u201d, \u201cFantasy\u201d, \u201cHorror\u201d, \u201cRomance\u201d, \u201cSci-Fi\u201d.     '''  # Models #model=\"deepseek-ai/DeepSeek-R1\" model=\"deepseek-ai/DeepSeek-V3\" #model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\" #model=\"klusterai/Meta-Llama-3.3-70B-Instruct-Turbo\" #model=\"Qwen/Qwen2.5-VL-7B-Instruct\"  # Ensure the directory exists os.makedirs(\"text_clasification\", exist_ok=True)  # Create the batch job file with the prompt and content def create_batch_file(df):     batch_list = []     for index, row in df.iterrows():         content = row['text']          request = {             \"custom_id\": f\"movie_classification-{index}\",             \"method\": \"POST\",             \"url\": \"/v1/chat/completions\",             \"body\": {                 \"model\": model,                 \"temperature\": 0.5,                 \"messages\": [                     {\"role\": \"system\", \"content\": SYSTEM_PROMPT},                     {\"role\": \"user\", \"content\": content}                 ],             }         }         batch_list.append(request)     return batch_list  # Save file def save_batch_file(batch_list):     filename = f\"text_clasification/batch_job_request.jsonl\"     with open(filename, 'w') as file:         for request in batch_list:             file.write(json.dumps(request) + '\\n')     return filename <p>Let's run the functions we've defined before:</p> In\u00a0[7]: Copied! <pre>batch_list = create_batch_file(df)\ndata_dir = save_batch_file(batch_list)\nprint(data_dir)\n</pre> batch_list = create_batch_file(df) data_dir = save_batch_file(batch_list) print(data_dir) <pre>text_clasification/batch_job_request.jsonl\n</pre> <p>Next, we can preview what that batch job file looks like:</p> In\u00a0[8]: Copied! <pre>!head -n 1 text_clasification/batch_job_request.jsonl\n</pre> !head -n 1 text_clasification/batch_job_request.jsonl <pre>{\"custom_id\": \"movie_classification-0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"deepseek-ai/DeepSeek-V3\", \"temperature\": 0.5, \"messages\": [{\"role\": \"system\", \"content\": \"\\n    Classify the main genre of the given movie description based on the following genres (Respond with only the genre):\\n    \\u201cAction\\u201d, \\u201cAdventure\\u201d, \\u201cComedy\\u201d, \\u201cCrime\\u201d, \\u201cDocumentary\\u201d, \\u201cDrama\\u201d, \\u201cFantasy\\u201d, \\u201cHorror\\u201d, \\u201cRomance\\u201d, \\u201cSci-Fi\\u201d.\\n    \"}, {\"role\": \"user\", \"content\": \"Breakfast at Tiffany's: A young New York socialite becomes interested in a young man who has moved into her apartment building, but her past threatens to get in the way.\"}]}}\n</pre> <p>Now that we\u2019ve prepared our input file, it\u2019s time to upload it to the kluster.ai platform. To do so, you can use the <code>files.create</code> endpoint of the client, where the purpose is set to <code>batch</code>. This will return the file ID, which we need to log for the next steps.</p> In\u00a0[9]: Copied! <pre># Upload batch job request file\nwith open(data_dir, 'rb') as file:\n    upload_response = client.files.create(\n        file=file,\n        purpose=\"batch\"\n    )\n\n    # Print job ID\n    file_id = upload_response.id\n    print(f\"File uploaded successfully. File ID: {file_id}\")\n</pre> # Upload batch job request file with open(data_dir, 'rb') as file:     upload_response = client.files.create(         file=file,         purpose=\"batch\"     )      # Print job ID     file_id = upload_response.id     print(f\"File uploaded successfully. File ID: {file_id}\")  <pre>File uploaded successfully. File ID: 67e57d8f8573d5180a551f9e\n</pre> <p>Once the file has been successfully uploaded, we're ready to start (create) the batch job by providing the file ID we got in the previous step. To do so, we use the <code>batches.create</code> method, for which we need to set the endpoint to <code>/v1/chat/completions</code>. This will return the batch job details, with the ID.</p> In\u00a0[10]: Copied! <pre># Create batch job with completions endpoint\nbatch_job = client.batches.create(\n    input_file_id=file_id,\n    endpoint=\"/v1/chat/completions\",\n    completion_window=\"24h\"\n)\n\nprint(\"\\nBatch job created:\")\nbatch_dict = batch_job.model_dump()\nprint(json.dumps(batch_dict, indent=2))\n</pre> # Create batch job with completions endpoint batch_job = client.batches.create(     input_file_id=file_id,     endpoint=\"/v1/chat/completions\",     completion_window=\"24h\" )  print(\"\\nBatch job created:\") batch_dict = batch_job.model_dump() print(json.dumps(batch_dict, indent=2)) <pre>\nBatch job created:\n{\n  \"id\": \"67e57d94562f33dce8762faf\",\n  \"completion_window\": \"24h\",\n  \"created_at\": 1743093140,\n  \"endpoint\": \"/v1/chat/completions\",\n  \"input_file_id\": \"67e57d8f8573d5180a551f9e\",\n  \"object\": \"batch\",\n  \"status\": \"pre_schedule\",\n  \"cancelled_at\": null,\n  \"cancelling_at\": null,\n  \"completed_at\": null,\n  \"error_file_id\": null,\n  \"errors\": [],\n  \"expired_at\": null,\n  \"expires_at\": 1743179540,\n  \"failed_at\": null,\n  \"finalizing_at\": null,\n  \"in_progress_at\": null,\n  \"metadata\": {},\n  \"output_file_id\": null,\n  \"request_counts\": {\n    \"completed\": 0,\n    \"failed\": 0,\n    \"total\": 0\n  }\n}\n</pre> <p>Now that your batch job has been created, you can track its progress.</p> <p>To monitor the job's progress, we can use the <code>batches.retrieve</code> method and pass the batch job ID. The response contains an <code>status</code> field that tells us if it is completed or not, and the subsequent status of each job separately.</p> <p>The following snippet checks the status every 10 seconds until the entire batch is completed:</p> In\u00a0[11]: Copied! <pre>all_completed = False\n\n# Loop to check status every 10 seconds\nwhile not all_completed:\n    all_completed = True\n    output_lines = []\n\n    updated_job = client.batches.retrieve(batch_job.id)\n\n    if updated_job.status != \"completed\":\n        all_completed = False\n        completed = updated_job.request_counts.completed\n        total = updated_job.request_counts.total\n        output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\")\n    else:\n        output_lines.append(f\"Job completed!\")\n\n    # Clear the output and display updated status\n    clear_output(wait=True)\n    for line in output_lines:\n        display(line)\n\n    if not all_completed:\n        time.sleep(10)\n</pre> all_completed = False  # Loop to check status every 10 seconds while not all_completed:     all_completed = True     output_lines = []      updated_job = client.batches.retrieve(batch_job.id)      if updated_job.status != \"completed\":         all_completed = False         completed = updated_job.request_counts.completed         total = updated_job.request_counts.total         output_lines.append(f\"Job status: {updated_job.status} - Progress: {completed}/{total}\")     else:         output_lines.append(f\"Job completed!\")      # Clear the output and display updated status     clear_output(wait=True)     for line in output_lines:         display(line)      if not all_completed:         time.sleep(10) <pre>'Job completed!'</pre> <p>With the job completed, we'll retrieve the results and review the responses generated for each request. The results are parsed. To fetch the results from the platform, you need to retrieve the <code>output_file_id</code> from the batch job, and then use the <code>files.content</code> endpoint, providing that specific file ID. Note that the job status must be <code>completed</code> for you to retrieve the results!</p> In\u00a0[12]: Copied! <pre>#Parse results as a JSON object\ndef parse_json_objects(data_string):\n    if isinstance(data_string, bytes):\n        data_string = data_string.decode('utf-8')\n\n    json_strings = data_string.strip().split('\\n')\n    json_objects = []\n\n    for json_str in json_strings:\n        try:\n            json_obj = json.loads(json_str)\n            json_objects.append(json_obj)\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing JSON: {e}\")\n\n    return json_objects\n\n# Retrieve results with job ID\njob = client.batches.retrieve(batch_job.id)\nresult_file_id = job.output_file_id\nresult = client.files.content(result_file_id).content\n\n# Parse JSON results\nparsed_result = parse_json_objects(result)\n\n# Extract and print only the content of each response\nprint(\"\\nExtracted Responses:\")\nfor item in parsed_result:\n    try:\n        content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n        print(content)\n    except KeyError as e:\n        print(f\"Missing key in response: {e}\")\n</pre> #Parse results as a JSON object def parse_json_objects(data_string):     if isinstance(data_string, bytes):         data_string = data_string.decode('utf-8')      json_strings = data_string.strip().split('\\n')     json_objects = []      for json_str in json_strings:         try:             json_obj = json.loads(json_str)             json_objects.append(json_obj)         except json.JSONDecodeError as e:             print(f\"Error parsing JSON: {e}\")      return json_objects  # Retrieve results with job ID job = client.batches.retrieve(batch_job.id) result_file_id = job.output_file_id result = client.files.content(result_file_id).content  # Parse JSON results parsed_result = parse_json_objects(result)  # Extract and print only the content of each response print(\"\\nExtracted Responses:\") for item in parsed_result:     try:         content = item[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]         print(content)     except KeyError as e:         print(f\"Missing key in response: {e}\") <pre>\nExtracted Responses:\nRomance\nDrama\nDrama\nDrama\nAction\n</pre> <p>This tutorial used the chat completion endpoint to perform a simple text classification task with batch inference. This particular example clasified a series of movies based on their description.</p> <p>To submit a batch job we've:</p> <ol> <li>Created the JSONL file, where each line of the file represented a separate request</li> <li>Submitted the file to the platform</li> <li>Started the batch job, and monitored its progress</li> <li>Once completed, we fetched the results</li> </ol> <p>All of this using the OpenAI Python library and API, no changes needed!</p> <p>Kluster.ai's batch API empowers you to scale your workflows seamlessly, making it an invaluable tool for processing extensive datasets. As next steps, feel free to create your own dataset, or expand on top of this existing example. Good luck!</p>"},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#text-classification-with-klusterai-api","title":"Text classification with kluster.ai API\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#prerequisites","title":"Prerequisites\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#setup","title":"Setup\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#get-the-data","title":"Get the data\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#perform-batch-inference","title":"Perform batch inference\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#create-the-batch-job-file","title":"Create the batch job file\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#upload-batch-job-file-to-klusterai","title":"Upload batch job file to kluster.ai\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#start-the-batch-job","title":"Start the batch job\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#check-job-progress","title":"Check job progress\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#get-the-results","title":"Get the results\u00b6","text":""},{"location":"tutorials/klusterai-api/text-classification/text-classification-openai-api/#summary","title":"Summary\u00b6","text":""}]}